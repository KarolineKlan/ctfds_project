{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ea257d",
   "metadata": {},
   "source": [
    "# Frequent items analysis of reddit communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4c2c536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/livdreyerjohansen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import networkx as nx\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "import html\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import math\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88d1fd7",
   "metadata": {},
   "source": [
    "## Dataload and cleaning\n",
    "\n",
    "We analysize what frequent items we may see in the reddit clusters found in 05_SemanticClustering.ipynb. The output of the analysis performed in aforementioned notebook is organized in a csv file with columns: id (user), text, and community.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d6bbe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('post_clusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cf8dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df_raw.rename(columns={\"label\": \"community\"})\n",
    "df_raw = df_raw.rename(columns={\"user\": \"id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de689bc",
   "metadata": {},
   "source": [
    "To ensure we do not include posts that are either deleted (\"\\[deleted\\]\") or removed (\"\\[removed\\]\"), both basic reddit features that happen independently of what forum you are in, we remove both. Furthermore, we remove each post that was removed by a bot, which is clear in the text which the bot uses to explain why a post or comment is deleted. We then construct a dataframe with all posts and their community (and original poster (OP in reddit linguistics))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03a8126f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             id                                               text  community\n",
      "0  9.290107e+08  great now you live with a hole in your head fo...         44\n",
      "1  9.290107e+08  what? i mean i get where you're comming from b...          9\n",
      "2  9.290107e+08  chaggot? more like that fucking weirdo who sta...          9\n",
      "3  9.290107e+08  but then i'd have to leave too and you wouldnt...        179\n",
      "4  9.290107e+08  wait what's the deal with that guy? he's prett...        114\n",
      "\n",
      "Number of original posts:\n",
      "1,102,030\n",
      "Number of removed posts:\n",
      "0\n",
      "Number of posts in dataframe:\n",
      "1,102,030\n"
     ]
    }
   ],
   "source": [
    "# Expected columns in CSV:\n",
    "# id, text, community\n",
    "# (tell me if they differ)\n",
    "df_raw.drop(df_raw[df_raw[\"community\"] == -1].index, inplace=True)\n",
    "\n",
    "rows = []\n",
    "rows_count = 0\n",
    "allowed_rows = 0\n",
    "\n",
    "for _, r in df_raw.iterrows():\n",
    "    rows_count += 1\n",
    "    post = r[\"text\"]\n",
    "\n",
    "    # Skip empty, deleted/removed posts, or posts containing the bot line\n",
    "    if post and post not in ['[deleted]', '[removed]'] and \\\n",
    "       \"*I am a bot, and this action was performed automatically.\" not in post:\n",
    "\n",
    "        allowed_rows += 1\n",
    "\n",
    "        rows.append({\n",
    "            \"id\": r[\"id\"],\n",
    "            \"text\": post,\n",
    "            \"community\": r[\"community\"]\n",
    "        })\n",
    "\n",
    "# Build the dataframe exactly like before\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nNumber of original posts:\")\n",
    "print(f\"{rows_count:,}\")\n",
    "print(\"Number of removed posts:\")\n",
    "print(f\"{rows_count-allowed_rows:,}\")\n",
    "print(\"Number of posts in dataframe:\")\n",
    "print(f\"{allowed_rows:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dbf9c4",
   "metadata": {},
   "source": [
    "We identify the top-6 largest communities in terms of posts to continue working with only them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f4063e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = df['community'].unique().tolist()\n",
    "communities_dict = dict.fromkeys(communities, 0)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    communities_dict[row['community']] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f165e4",
   "metadata": {},
   "source": [
    "Filter the dataframe to only contain posts from top-6 communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c521f85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 6 communities by number of posts (posts, community): [(95532, 9), (93385, 130), (54467, 81), (51607, 94), (32530, 204), (21818, 198)]\n",
      "Number of posts in top 6 communities: 349,339\n",
      "Number of posts removed based on non identity in top 6: 752,691\n"
     ]
    }
   ],
   "source": [
    "comms_list = list(sorted( ((v,k) for k,v in communities_dict.items()), reverse=True))\n",
    "comms_list = comms_list[:6]\n",
    "top_6_communities = [item[1] for item in comms_list]\n",
    "\n",
    "df_filtered = df[df['community'].isin(top_6_communities)].copy()\n",
    "\n",
    "print(\"\\nTop 6 communities by number of posts (posts, community):\", comms_list)\n",
    "\n",
    "print(f\"Number of posts in top 6 communities: {len(df_filtered):,}\")\n",
    "\n",
    "print(f\"Number of posts removed based on non identity in top 6: {(len(df) - len(df_filtered)):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1289c493",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "We filter out parts of post that we deem have little semantic value. We aim to find frequent items and frequent itemsets (item pairs), and would assume that stop words regularly occur in more than 1% of baskets. As we are working with online fora, we chose to add certain slang-terms as stop words. We furthermore remove:\n",
    "\n",
    "- html entities\n",
    "- URL's\n",
    "- non-text artifacts (such as \"/\", \"?\", \"!\" etc.)\n",
    "- remaining \"removed\" and \"deleted\" artifacts that were not removed in the previous code block due to the way the post was loaded\n",
    "- short words (length of 2 or less)\n",
    "\n",
    "Additionally, we make all words lowercase to steamline and tokenize by word (meaning each word will be its own token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "422cbe8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "community",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tokens",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "n_tokens",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "0144ad5c-5428-4367-a8ec-15d903e88e00",
       "rows": [
        [
         "1",
         "929010695.928077",
         "what? i mean i get where you're comming from but he doesn't really inspire respect like chads do",
         "9",
         "['mean', 'comming', 'really', 'inspire', 'respect', 'chads']",
         "6"
        ],
        [
         "2",
         "929010695.928077",
         "chaggot? more like that fucking weirdo who stalks you on grindr. gay chads are basically normal chad. thats why you shouldnt have a crush kn str8 guys. it doesnt matter if he's gay, he's still way out of your leaguenn",
         "9",
         "['chaggot', 'fucking', 'weirdo', 'stalks', 'grindr', 'gay', 'chads', 'basically', 'normal', 'chad', 'thats', 'shouldnt', 'crush', 'str', 'guys', 'doesnt', 'matter', 'gay', 'still', 'way', 'leaguenn']",
         "21"
        ],
        [
         "5",
         "929010695.928077",
         "isn't that the opposite of incels? she has so manny options she doesn't settle with the creepy ones?",
         "130",
         "['opposite', 'incels', 'manny', 'options', 'settle', 'creepy', 'ones']",
         "7"
        ],
        [
         "7",
         "929010695.928077",
         "what's so bad about hiring incels anyways? wouldn't they be more productive on account of having no life? as long as they don't reveal their power levels i don't see an issue here",
         "130",
         "['bad', 'hiring', 'incels', 'anyways', 'productive', 'account', 'life', 'long', 'reveal', 'power', 'levels', 'see', 'issue']",
         "13"
        ],
        [
         "8",
         "929010695.928077",
         "hey she deserved what's coming for her. can't feel too bad about that",
         "198",
         "['hey', 'deserved', 'coming', 'feel', 'bad']",
         "5"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>community</th>\n",
       "      <th>tokens</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.290107e+08</td>\n",
       "      <td>what? i mean i get where you're comming from b...</td>\n",
       "      <td>9</td>\n",
       "      <td>[mean, comming, really, inspire, respect, chads]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.290107e+08</td>\n",
       "      <td>chaggot? more like that fucking weirdo who sta...</td>\n",
       "      <td>9</td>\n",
       "      <td>[chaggot, fucking, weirdo, stalks, grindr, gay...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.290107e+08</td>\n",
       "      <td>isn't that the opposite of incels? she has so ...</td>\n",
       "      <td>130</td>\n",
       "      <td>[opposite, incels, manny, options, settle, cre...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9.290107e+08</td>\n",
       "      <td>what's so bad about hiring incels anyways? wou...</td>\n",
       "      <td>130</td>\n",
       "      <td>[bad, hiring, incels, anyways, productive, acc...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.290107e+08</td>\n",
       "      <td>hey she deserved what's coming for her. can't ...</td>\n",
       "      <td>198</td>\n",
       "      <td>[hey, deserved, coming, feel, bad]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  community  \\\n",
       "1  9.290107e+08  what? i mean i get where you're comming from b...          9   \n",
       "2  9.290107e+08  chaggot? more like that fucking weirdo who sta...          9   \n",
       "5  9.290107e+08  isn't that the opposite of incels? she has so ...        130   \n",
       "7  9.290107e+08  what's so bad about hiring incels anyways? wou...        130   \n",
       "8  9.290107e+08  hey she deserved what's coming for her. can't ...        198   \n",
       "\n",
       "                                              tokens  n_tokens  \n",
       "1   [mean, comming, really, inspire, respect, chads]         6  \n",
       "2  [chaggot, fucking, weirdo, stalks, grindr, gay...        21  \n",
       "5  [opposite, incels, manny, options, settle, cre...         7  \n",
       "7  [bad, hiring, incels, anyways, productive, acc...        13  \n",
       "8                 [hey, deserved, coming, feel, bad]         5  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize and clean text data\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# extend basic english stopwords with slang terms\n",
    "extra_stops = {\n",
    "    'lol', 'xd', 'haha', 'hahaah', 'omg', 'u', 'ur', 'im', 'ive', 'idk', \n",
    "    'dont', 'cant', 'wont', 'aint', 'ya', 'tho', 'tho', 'nah', 'btw', \n",
    "    'like', 'yeah', 'yep', 'ok', 'okay', 'pls', 'please', 'get'\n",
    "}\n",
    "stop_words.update(extra_stops)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # decode HTML entities: &amp; → &, &#x200B; → zero-width space, etc.\n",
    "    text = html.unescape(text)\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
    "    # keep only letters and spaces\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    # tokenize by whitespace\n",
    "    tokens = text.split()\n",
    "    # remove stopwords and very short tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    if len(tokens) == 1 and tokens[0] in {\"removed\", \"deleted\"}:\n",
    "        return []\n",
    "    return tokens\n",
    "\n",
    "df_filtered[\"tokens\"] = df_filtered[\"text\"].apply(preprocess_text)\n",
    "df_filtered[\"n_tokens\"] = df_filtered[\"tokens\"].apply(len)\n",
    "\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4075dcf8",
   "metadata": {},
   "source": [
    "# Frequent items and the A-priori algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27146132",
   "metadata": {},
   "source": [
    "We process the tokenized posts by identifying the amount of unique tokens for each community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f9c689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community 9:\n",
      "  Total tokens: 1,342,981\n",
      "  Unique tokens: 49,183\n",
      "Community 130:\n",
      "  Total tokens: 1,868,461\n",
      "  Unique tokens: 52,898\n",
      "Community 81:\n",
      "  Total tokens: 868,501\n",
      "  Unique tokens: 38,103\n",
      "Community 94:\n",
      "  Total tokens: 762,842\n",
      "  Unique tokens: 31,028\n",
      "Community 204:\n",
      "  Total tokens: 681,874\n",
      "  Unique tokens: 27,089\n",
      "Community 198:\n",
      "  Total tokens: 253,499\n",
      "  Unique tokens: 19,440\n"
     ]
    }
   ],
   "source": [
    "# build token statistics for each of the top 6 communities\n",
    "\n",
    "community_token_stats = {}\n",
    "\n",
    "for community_id in top_6_communities:\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == community_id]\n",
    "\n",
    "    # flatten all tokens for this community\n",
    "    all_tokens = []\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    unique_tokens = set(all_tokens)\n",
    "\n",
    "    community_token_stats[community_id] = {\n",
    "        \"n_tokens\": len(all_tokens),\n",
    "        \"n_unique_tokens\": len(unique_tokens),       \n",
    "        \"unique_tokens\": unique_tokens \n",
    "    }\n",
    "\n",
    "\n",
    "for cid in top_6_communities:\n",
    "    print(f\"Community {cid}:\")\n",
    "    print(f\"  Total tokens: {community_token_stats[cid]['n_tokens']:,}\")\n",
    "    print(f\"  Unique tokens: {community_token_stats[cid]['n_unique_tokens']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eca728",
   "metadata": {},
   "source": [
    "## First pass of the A-priori algorithm\n",
    "\n",
    "In the first pass of the A-priori algoritm, we initialize a dataframe for each of the communities. In this dataframe, we will store each of the unique tokens found previously, assign them each an integer from 0 to n-1 (number of unique tokens), and count how many baskets (posts) the item (token) appears in. It is important to note that we do not count the total occurrence of the token but only the amount of posts it appears in. In Mining of Massive Datasets, Section 6.2.2, the first pass is described as labeling integers 1 to n, but to keep it within the python framework, we label 0 to n-1 and mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f323c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori_tables = {}\n",
    "\n",
    "for cid in top_6_communities:\n",
    "    # get df for this community\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "    unique_tokens = list(community_token_stats[cid][\"unique_tokens\"])\n",
    "\n",
    "    # apriori table\n",
    "    df_apriori = pd.DataFrame({\n",
    "        \"word\": unique_tokens,\n",
    "        \"integer\": range(len(unique_tokens))\n",
    "    })\n",
    "\n",
    "    # give each word an integer from 0 to n-1\n",
    "    word_to_int = dict(zip(df_apriori[\"word\"], df_apriori[\"integer\"]))\n",
    "\n",
    "    # count posts that contain each token\n",
    "    array_of_counts = np.zeros(len(unique_tokens), dtype=int)\n",
    "\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        for token in set(tokens):             \n",
    "            array_of_counts[word_to_int[token]] += 1\n",
    "\n",
    "    df_apriori[\"count\"] = array_of_counts\n",
    "\n",
    "    # save in dict\n",
    "    apriori_tables[cid] = df_apriori\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "692f4753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 9\n",
      "        word  integer  count\n",
      "43652   chad    43652  70535\n",
      "44159  chads    44159  18492\n",
      "3403   women     3403  12601\n",
      "28993  would    28993   9553\n",
      "42656   even    42656   8937\n",
      "\n",
      "Community 130\n",
      "         word  integer  count\n",
      "35014   incel    35014  52750\n",
      "5465   incels     5465  45845\n",
      "3707    women     3707  13515\n",
      "9308   people     9308  13499\n",
      "31114   would    31114  10744\n",
      "\n",
      "Community 81\n",
      "            word  integer  count\n",
      "9257       white     9257  19078\n",
      "2599       women     2599  10160\n",
      "34732      black    34732   9942\n",
      "29224      asian    29224   8863\n",
      "10580  blackpill    10580   7791\n",
      "\n",
      "Community 94\n",
      "         word  integer  count\n",
      "28412  height    28412  16448\n",
      "25648   short    25648  11274\n",
      "4627     tall     4627   8733\n",
      "2125    women     2125   7709\n",
      "29841     men    29841   5939\n",
      "\n",
      "Community 204\n",
      "             word  integer  count\n",
      "19739        ugly    19739  16418\n",
      "13225       looks    13225   8750\n",
      "4778       people     4778   7296\n",
      "1857        women     1857   6437\n",
      "26975  attractive    26975   5160\n",
      "\n",
      "Community 198\n",
      "        word  integer  count\n",
      "6706   would     6706   2320\n",
      "8963    even     8963   1628\n",
      "3467    know     3467   1537\n",
      "17841   fuck    17841   1343\n",
      "17602  think    17602   1264\n"
     ]
    }
   ],
   "source": [
    "# print top 5 tokens by count for each community\n",
    "for cid, df_apriori in apriori_tables.items():\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "    print(df_apriori.sort_values(by=\"count\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5646a40",
   "metadata": {},
   "source": [
    "## Between the passes of A-priori\n",
    "\n",
    "We create frequency tables where we assign each word an integer from 1-m, where m = number of frequent singletons (words), if the support of the word => 1%. In other words, it must appear in 1% or more of the baskets. If the word is not frequent, we assign it 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2d554ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 9 frequent items:\n",
      "             word  integer  count  freq_integer\n",
      "309         thing      309   2255             1\n",
      "505          ever      505   1957             2\n",
      "782           see      782   3338             3\n",
      "1005         date     1005   1658             4\n",
      "1037        wants     1037   1843             5\n",
      "...           ...      ...    ...           ...\n",
      "47840         way    47840   3022           195\n",
      "47989         try    47989   1408           196\n",
      "48074        come    48074   1391           197\n",
      "48672      dating    48672   1342           198\n",
      "48954  attractive    48954   2478           199\n",
      "\n",
      "[199 rows x 4 columns]\n",
      "\n",
      "Community 130 frequent items:\n",
      "             word  integer  count  freq_integer\n",
      "348         thing      348   3964             1\n",
      "552          ever      552   2964             2\n",
      "873           see      873   5461             3\n",
      "1103         date     1103   1656             4\n",
      "1141        wants     1141   1215             5\n",
      "...           ...      ...    ...           ...\n",
      "51963      thread    51963    934           294\n",
      "52306    comments    52306   1078           295\n",
      "52366      dating    52366   1488           296\n",
      "52615        part    52615   1715           297\n",
      "52655  attractive    52655   2013           298\n",
      "\n",
      "[298 rows x 4 columns]\n",
      "\n",
      "Community 81 frequent items:\n",
      "             word  integer  count  freq_integer\n",
      "234         thing      234   1499             1\n",
      "267     countries      267    702             2\n",
      "350          east      350    612             3\n",
      "370          ever      370   1161             4\n",
      "581           see      581   2753             5\n",
      "...           ...      ...    ...           ...\n",
      "37263        come    37263    853           220\n",
      "37721      dating    37721   1894           221\n",
      "37904        part    37904    740           222\n",
      "37935       dudes    37935    770           223\n",
      "37936  attractive    37936   1861           224\n",
      "\n",
      "[224 rows x 4 columns]\n",
      "\n",
      "Community 94 frequent items:\n",
      "             word  integer  count  freq_integer\n",
      "188         thing      188   1555             1\n",
      "318          ever      318   1091             2\n",
      "465           see      465   2295             3\n",
      "599          date      599   1688             4\n",
      "1197        years     1197    910             5\n",
      "...           ...      ...    ...           ...\n",
      "30361        come    30361    552           201\n",
      "30483      plenty    30483    538           202\n",
      "30723      dating    30723   1517           203\n",
      "30889       dudes    30889    572           204\n",
      "30890  attractive    30890   1504           205\n",
      "\n",
      "[205 rows x 4 columns]\n",
      "\n",
      "Community 204 frequent items:\n",
      "             word  integer  count  freq_integer\n",
      "162         thing      162   1664             1\n",
      "263          ever      263   1140             2\n",
      "407           see      407   2216             3\n",
      "530          date      530   1284             4\n",
      "548         wants      548    463             5\n",
      "...           ...      ...    ...           ...\n",
      "26591      plenty    26591    493           314\n",
      "26825      dating    26825   1148           315\n",
      "26954        part    26954    648           316\n",
      "26974       dudes    26974    459           317\n",
      "26975  attractive    26975   5160           318\n",
      "\n",
      "[318 rows x 4 columns]\n",
      "\n",
      "Community 198 frequent items:\n",
      "            word  integer  count  freq_integer\n",
      "77        around       77    424             1\n",
      "222         gets      222    357             2\n",
      "497        thing      497    577             3\n",
      "635        whore      635    247             4\n",
      "783    literally      783    380             5\n",
      "...          ...      ...    ...           ...\n",
      "18634     dating    18634    269           186\n",
      "18659       went    18659    352           187\n",
      "18801        nni    18801    272           188\n",
      "18825      least    18825    381           189\n",
      "19005       part    19005    221           190\n",
      "\n",
      "[190 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "freq_tables = {}\n",
    "\n",
    "threshold_factor = 0.01  \n",
    "\n",
    "for cid, df_apriori in apriori_tables.items():\n",
    "\n",
    "    # threshold is 1% of posts in that community\n",
    "    threshold = threshold_factor * len(df_filtered[df_filtered[\"community\"] == cid])\n",
    "\n",
    "    frequent_map = np.zeros(len(df_apriori), dtype=int)\n",
    "    new_id = 1\n",
    "\n",
    "    for old_id, count in enumerate(df_apriori['count']):\n",
    "        if count >= threshold:\n",
    "            frequent_map[old_id] = new_id\n",
    "            new_id += 1\n",
    "        else:\n",
    "            frequent_map[old_id] = 0\n",
    "\n",
    "    # add freq_integer column\n",
    "    df_apriori['freq_integer'] = frequent_map\n",
    "\n",
    "    # store only frequent items in new dictionary\n",
    "    df_freq = df_apriori[df_apriori['freq_integer'] != 0].copy()\n",
    "    freq_tables[cid] = df_freq\n",
    "\n",
    "    print(f\"\\nCommunity {cid} frequent items:\")\n",
    "    print(df_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d38f5ea",
   "metadata": {},
   "source": [
    "## Second pass of the A-priori algorithm\n",
    "\n",
    "For the second pass, we first find all pairs of frequent words from the previous dataframes. We then create pairs of those, making sure to remove duplicates. We apply the support threshold of 1% here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cab3263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 9\n",
      "Top pairs:\n",
      "          item_set  count\n",
      "64   (chad, women)   9652\n",
      "337  (chad, would)   7643\n",
      "59    (chad, even)   6866\n",
      "292   (chad, fuck)   5460\n",
      "890   (chad, want)   5359\n",
      "Total pairs where Support(I) => s of 0.01: 211\n",
      "\n",
      "Community 130\n",
      "Top pairs:\n",
      "              item_set  count\n",
      "696   (incels, people)   8725\n",
      "132    (incels, women)   8261\n",
      "1003   (incel, incels)   8194\n",
      "1266    (incel, women)   7332\n",
      "529    (incel, people)   7214\n",
      "Total pairs where Support(I) => s of 0.01: 584\n",
      "\n",
      "Community 81\n",
      "Top pairs:\n",
      "            item_set  count\n",
      "718   (white, women)   5521\n",
      "723   (asian, white)   3973\n",
      "1284  (black, white)   3854\n",
      "800     (men, white)   3565\n",
      "782     (men, women)   3446\n",
      "Total pairs where Support(I) => s of 0.01: 226\n",
      "\n",
      "Community 94\n",
      "Top pairs:\n",
      "            item_set  count\n",
      "11    (face, height)   3373\n",
      "227  (height, women)   3338\n",
      "164   (short, women)   3123\n",
      "32      (men, women)   3052\n",
      "182     (men, short)   3052\n",
      "Total pairs where Support(I) => s of 0.01: 326\n",
      "\n",
      "Community 204\n",
      "Top pairs:\n",
      "             item_set  count\n",
      "336    (people, ugly)   3647\n",
      "1       (ugly, women)   3318\n",
      "2473      (men, ugly)   2581\n",
      "474      (men, women)   2540\n",
      "1876  (looks, people)   2304\n",
      "Total pairs where Support(I) => s of 0.01: 889\n",
      "\n",
      "Community 198\n",
      "Top pairs:\n",
      "            item_set  count\n",
      "89     (even, would)    306\n",
      "1103    (even, know)    276\n",
      "110    (fuck, would)    275\n",
      "1068   (know, would)    273\n",
      "170   (think, would)    265\n",
      "Total pairs where Support(I) => s of 0.01: 10\n"
     ]
    }
   ],
   "source": [
    "pair_tables = {}   # store results for each community\n",
    "\n",
    "for cid in top_6_communities:\n",
    "\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "\n",
    "    # pull posts for this community\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "    N = len(df_comm)\n",
    "\n",
    "    # fetch frequent 1-itemset for this community\n",
    "    df_freq = freq_tables[cid]\n",
    "    frequent_words_set = set(df_freq[\"word\"])\n",
    "\n",
    "    # counter for all frequent pairs\n",
    "    pair_counter = Counter()\n",
    "\n",
    "    # iterate over all posts\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        # keep only frequent tokens\n",
    "        frequent_tokens = [t for t in tokens if t in frequent_words_set]\n",
    "\n",
    "        # deduplicate within a post\n",
    "        unique_tokens = set(frequent_tokens)\n",
    "\n",
    "        # count each 2-item combination in this post\n",
    "        for pair in combinations(unique_tokens, 2):\n",
    "            pair_counter[tuple(sorted(pair))] += 1\n",
    "\n",
    "    # convert counter → dataframe\n",
    "    df_pairs = pd.DataFrame(pair_counter.items(), columns=[\"item_set\", \"count\"])\n",
    "\n",
    "    # threshold for frequent 2-itemsets (1% of posts)\n",
    "    threshold = math.ceil(0.01 * N)\n",
    "    df_pairs = df_pairs[df_pairs[\"count\"] >= threshold]\n",
    "\n",
    "    # store\n",
    "    pair_tables[cid] = df_pairs\n",
    "\n",
    "    # print summary\n",
    "    print(\"Top pairs:\")\n",
    "    print(df_pairs.sort_values(by=\"count\", ascending=False).head())\n",
    "    print(\"Total pairs where Support(I) => s of 0.01:\", len(df_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b44fc2",
   "metadata": {},
   "source": [
    "# A-priori using library\n",
    "\n",
    "To further validate the above results, we also implemented the A-priori algorithm using mlxtend. We find that the results of using the mlxtend framwork are congruent with the results found by implementing the A-priori algorithm as described in Mining of Massive Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e87124af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 9\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 199\n",
      "\n",
      "Community 130\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 298\n",
      "\n",
      "Community 81\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 224\n",
      "\n",
      "Community 94\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 205\n",
      "\n",
      "Community 204\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 318\n",
      "\n",
      "Community 198\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 190\n"
     ]
    }
   ],
   "source": [
    "te = TransactionEncoder()\n",
    "encoded_tables = {} \n",
    "\n",
    "for cid in top_6_communities:\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "    \n",
    "    freq_words = set(freq_tables[cid][\"word\"])\n",
    "    \n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "    transactions = [\n",
    "        [t for t in tokens if t in freq_words]\n",
    "        for tokens in df_comm[\"tokens\"]\n",
    "    ]\n",
    "\n",
    "    te_array = te.fit(transactions).transform(transactions)\n",
    "    df_encoded = pd.DataFrame(te_array, columns=te.columns_)\n",
    "    \n",
    "    encoded_tables[cid] = df_encoded\n",
    "    \n",
    "    print(f\"Number of frequent items (singletons) with Support(I) => s of 0.01: {df_encoded.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ac9b3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 9\n",
      "Number of frequent 2-itemsets: 211\n",
      "Top 5 pairs:\n",
      "      support       itemsets\n",
      "344  0.101034  (women, chad)\n",
      "347  0.080005  (chad, would)\n",
      "231  0.071871   (even, chad)\n",
      "247  0.057154   (fuck, chad)\n",
      "338  0.056096   (want, chad)\n",
      "\n",
      "Community 130\n",
      "Number of frequent 2-itemsets: 584\n",
      "Top 5 pairs:\n",
      "      support          itemsets\n",
      "647  0.093430  (people, incels)\n",
      "708  0.088462   (women, incels)\n",
      "498  0.087744   (incel, incels)\n",
      "599  0.078514    (incel, women)\n",
      "538  0.077250   (incel, people)\n",
      "\n",
      "Community 81\n",
      "Number of frequent 2-itemsets: 226\n",
      "Top 5 pairs:\n",
      "      support        itemsets\n",
      "445  0.101364  (women, white)\n",
      "251  0.072943  (asian, white)\n",
      "286  0.070758  (black, white)\n",
      "378  0.065452    (white, men)\n",
      "379  0.063268    (women, men)\n",
      "\n",
      "Community 94\n",
      "Number of frequent 2-itemsets: 326\n",
      "Top 5 pairs:\n",
      "      support         itemsets\n",
      "270  0.065359   (height, face)\n",
      "395  0.064681  (height, women)\n",
      "500  0.060515   (women, short)\n",
      "445  0.059139     (women, men)\n",
      "436  0.059139     (short, men)\n",
      "\n",
      "Community 204\n",
      "Number of frequent 2-itemsets: 889\n",
      "Top 5 pairs:\n",
      "       support         itemsets\n",
      "1057  0.112112   (ugly, people)\n",
      "1186  0.101998    (ugly, women)\n",
      "967   0.079342      (ugly, men)\n",
      "973   0.078082     (women, men)\n",
      "862   0.070827  (looks, people)\n",
      "\n",
      "Community 198\n",
      "Number of frequent 2-itemsets: 10\n",
      "Top 5 pairs:\n",
      "      support        itemsets\n",
      "192  0.014025   (even, would)\n",
      "190  0.012650    (even, know)\n",
      "193  0.012604   (fuck, would)\n",
      "196  0.012513   (know, would)\n",
      "198  0.012146  (think, would)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "frequent_2_itemsets_by_community = {}\n",
    "\n",
    "for cid in top_6_communities:\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "    \n",
    "    df_encoded = encoded_tables[cid] \n",
    "    \n",
    "    frequent_itemsets = apriori(df_encoded, min_support=0.01, use_colnames=True)\n",
    "    \n",
    "    # filter to only 2-itemsets\n",
    "    frequent_2_itemsets = frequent_itemsets[\n",
    "        frequent_itemsets['itemsets'].apply(lambda x: len(x) == 2)\n",
    "    ].copy()\n",
    "    \n",
    "    frequent_2_itemsets_by_community[cid] = frequent_2_itemsets\n",
    "    \n",
    "    print(\"Number of frequent 2-itemsets:\", len(frequent_2_itemsets))\n",
    "    print(\"Top 5 pairs:\")\n",
    "    print(frequent_2_itemsets.sort_values(by=\"support\", ascending=False).head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
