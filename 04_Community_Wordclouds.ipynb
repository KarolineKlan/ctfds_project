{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc92042",
   "metadata": {},
   "source": [
    "# Investigating community language using wordclouds\n",
    "We move on to analyse the communities based on the words that are used in their posts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5078f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "import json\n",
    "import networkx as nx\n",
    "from netwulf import visualize, draw_netwulf\n",
    "import netwulf as nw\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from itertools import product\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "import random\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import warnings  \n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from itertools import product\n",
    "from itertools import product\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5987be68",
   "metadata": {},
   "source": [
    "## Textual Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed839048",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_All_communities = nx.read_gml(\"data/graphs/reddit_graph_with_communities.gml\", label=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c417edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert G_All_communities to pandas dataframe\n",
    "G_text_df = pd.DataFrame.from_dict(dict(G_All_communities.nodes(data=True)), orient='index')\n",
    "G_text_df.reset_index(inplace=True)\n",
    "G_text_df.rename(columns={'index': 'node_id'}, inplace=True)    \n",
    "G_text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabc3011",
   "metadata": {},
   "source": [
    "We are doing the following cleaning:\n",
    "- Empty or placeholder entries – Many posts contain only markers like \"[removed]\", \"[deleted]\", or empty strings, which are excluded to avoid meaningless text in analysis.\n",
    "- Non-text content – Posts that originally included only media, links, or formatting tags are stripped out during cleaning.\n",
    "- Data consistency issues – The n_posts field in the graph sometimes includes deleted or missing posts that were never stored under the posts attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66fa494",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DROP_TOKENS = {\"\", \"[removed]\", \"[deleted]\"}  # extend if needed\n",
    "\n",
    "def flatten_posts(posts_dict):\n",
    "    # posts_dict looks like: {\"Incel\": [p1, p2, ...], \"Braincels\": [p1, ...]}\n",
    "    raw = []\n",
    "    per_sub_counts = {}\n",
    "    if isinstance(posts_dict, dict):\n",
    "        for sub, lst in posts_dict.items():\n",
    "            lst = lst if isinstance(lst, list) else []\n",
    "            per_sub_counts[sub] = len(lst)\n",
    "            raw.extend(lst)\n",
    "    # counts before/after cleaning\n",
    "    raw_count = len(raw)\n",
    "    cleaned = [str(x).strip() for x in raw if str(x).strip() not in DROP_TOKENS]\n",
    "    kept_count = len(cleaned)\n",
    "    # final joined text\n",
    "    text = \" \".join(cleaned)\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"raw_count\": raw_count,\n",
    "        \"kept_count\": kept_count,\n",
    "        \"per_sub_counts\": per_sub_counts\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "for node, attrs in G_All_communities.nodes(data=True):\n",
    "    info = flatten_posts(attrs.get(\"posts\", {}))\n",
    "    rows.append({\n",
    "        \"node_id\": node,\n",
    "        \"label\": attrs.get(\"label\"),\n",
    "        \"community\": attrs.get(\"community\"),\n",
    "        \"subreddit_origin\": attrs.get(\"subreddit_origin\"),\n",
    "        \"n_posts_declared\": attrs.get(\"n_posts\"),\n",
    "        \"text\": info[\"text\"],\n",
    "        \"raw_count\": info[\"raw_count\"],\n",
    "        \"kept_count\": info[\"kept_count\"],\n",
    "        \"per_sub_counts\": info[\"per_sub_counts\"],\n",
    "        \"text_charlen\": len(info[\"text\"])\n",
    "    })\n",
    "\n",
    "df_nodes = pd.DataFrame(rows)\n",
    "df_nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c7cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mismatch diagnostics\n",
    "df_nodes[\"raw_vs_declared_diff\"] = df_nodes[\"raw_count\"] - df_nodes[\"n_posts_declared\"]\n",
    "df_nodes[\"kept_vs_declared_ratio\"] = df_nodes[\"kept_count\"] / df_nodes[\"n_posts_declared\"]\n",
    "\n",
    "# Flag suspicious cases\n",
    "suspect = df_nodes[\n",
    "    (df_nodes[\"n_posts_declared\"].notna()) & (\n",
    "        (df_nodes[\"raw_vs_declared_diff\"] != 0) |\n",
    "        (df_nodes[\"kept_vs_declared_ratio\"] < 0.90)  # tune threshold\n",
    "    )\n",
    "][[\"node_id\",\"n_posts_declared\",\"raw_count\",\"kept_count\",\"raw_vs_declared_diff\",\"kept_vs_declared_ratio\"]]\n",
    "\n",
    "suspect.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodes[\"n_chars\"] = df_nodes[\"text\"].str.len()\n",
    "df_nodes[\"n_words\"] = df_nodes[\"text\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "min_words = int(df_nodes[\"n_words\"].min())\n",
    "max_words = int(df_nodes[\"n_words\"].max())\n",
    "\n",
    "summary = {\n",
    "    \"Total users\": int(len(df_nodes)),\n",
    "    \"Total words\": int(df_nodes[\"n_words\"].sum()),\n",
    "    \"Average words per user\": float(df_nodes[\"n_words\"].mean()),\n",
    "    \"Median words per user\": float(df_nodes[\"n_words\"].median()),\n",
    "    \"Average characters per user\": float(df_nodes[\"n_chars\"].mean()),\n",
    "    \"Median characters per user\": float(df_nodes[\"n_chars\"].median()),\n",
    "    \"Min/Max post length (words)\": (min_words, max_words),\n",
    "}\n",
    "\n",
    "pd.Series(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ce4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vals = df_nodes[\"n_words\"].astype(float).to_numpy()\n",
    "vals = vals[np.isfinite(vals)]\n",
    "\n",
    "positive = vals[vals > 0]\n",
    "bins = np.logspace(0, np.log10(positive.max()), 60)\n",
    "\n",
    "plt.figure(figsize=(5,3))  # smaller figure\n",
    "plt.hist(positive, bins=bins, color=\"#4c72b0\", alpha=0.8, edgecolor=\"none\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Words per user (log scale)\", fontsize=9)\n",
    "plt.ylabel(\"User count\", fontsize=9)\n",
    "plt.title(\"Distribution of words per user\", fontsize=10)\n",
    "plt.tight_layout(pad=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6df4de",
   "metadata": {},
   "source": [
    "The dataset shows realistic Reddit-like activity: around 100k users and 65M words.\n",
    "Most users post very little (median = 31 words), while a few contribute heavily (avg ≈ 650 words, max ≈ 710k).\n",
    "The histogram confirms a strong right-skew, typical of online forums where a small group generates most content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37d632",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Download once (if not already done)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Base English stopwords from NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Optionally extend with custom slang / internet terms\n",
    "extra_stops = {\n",
    "    'lol', 'xd', 'haha', 'hahaah', 'omg', 'u', 'ur', 'im', 'ive', 'idk', \n",
    "    'dont', 'cant', 'wont', 'aint', 'ya', 'tho', 'tho', 'nah', 'btw', \n",
    "    'like', 'yeah', 'yep', 'ok', 'okay', 'pls', 'please'\n",
    "}\n",
    "stop_words.update(extra_stops)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
    "    # keep only letters and spaces\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    # tokenize by whitespace\n",
    "    tokens = text.split()\n",
    "    # remove stopwords and very short tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "df_nodes[\"tokens\"] = df_nodes[\"text\"].apply(preprocess_text)\n",
    "df_nodes[\"n_tokens\"] = df_nodes[\"tokens\"].apply(len)\n",
    "\n",
    "# Preview\n",
    "df_nodes.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92279f4d",
   "metadata": {},
   "source": [
    "## Frequency-Rank-Plot\n",
    "\n",
    "We will start by analysing our corpuses of product-descriptions for Zipf's law of abbreviation. This linguisic law states that the value of the n'th entry is inversly proportinal to n when token frequency is sorted in a list of decreasing order. This essentially means that the most common token in our corpus should occur twice as often as the next common one, three times as often as the third most common one and so on. We will check if this is the case in our corpuses by plotting the frequency of each token with the ideal zipf's law to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dac941",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Flatten all tokens into one big list\n",
    "all_tokens = [tok for tokens in df_nodes[\"tokens\"] for tok in tokens]\n",
    "\n",
    "# Count word frequencies\n",
    "word_counts = Counter(all_tokens)\n",
    "\n",
    "# Sort by frequency\n",
    "sorted_counts = np.array(sorted(word_counts.values(), reverse=True))\n",
    "ranks = np.arange(1, len(sorted_counts) + 1)\n",
    "\n",
    "print(f\"Total unique tokens: {len(word_counts):,}\")\n",
    "print(f\"Most common words:\\n{word_counts.most_common(10)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddb45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_frequency_rank(tokens, top_n=5000):\n",
    "    \"\"\"\n",
    "    Plot a Frequency–Rank (Zipf's Law) curve for a list of tokens.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens : list of str\n",
    "        All tokens from your corpus.\n",
    "    top_n : int\n",
    "        Number of most frequent words to include in the plot.\n",
    "    \"\"\"\n",
    "    # Count words\n",
    "    word_counts = Counter(tokens)\n",
    "    most_common = word_counts.most_common(top_n)\n",
    "    freqs = np.array([f for _, f in most_common])\n",
    "    ranks = np.arange(1, len(freqs) + 1)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.plot(ranks, freqs, marker='o', markersize=3, linestyle='-', label='Observed frequencies')\n",
    "\n",
    "    # Add Zipf’s Law reference line (theoretical expectation)\n",
    "    constant = freqs[0]  # most frequent word\n",
    "    zipf_line = constant / ranks\n",
    "    plt.plot(ranks, zipf_line, linestyle='--', color='red', label=\"Zipf's Law (1/r)\")\n",
    "\n",
    "    # Log–log axes\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Rank (log scale)')\n",
    "    plt.ylabel('Frequency (log scale)')\n",
    "    plt.title(f'Frequency–Rank Plot (Top {top_n:,} words)')\n",
    "    plt.grid(True, which='both', alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Run it on your corpus ---\n",
    "all_tokens = [tok for tokens in df_nodes[\"tokens\"] for tok in tokens]\n",
    "plot_frequency_rank(all_tokens, top_n=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6901d8a9",
   "metadata": {},
   "source": [
    "The curve mostly follows Zipf’s law, but it bends away from the ideal red line, which is normal for real online text.\n",
    "\n",
    "This happens because:\n",
    "- People on Reddit migth often repeat slang, memes, or usernames, making some words more frequent.\n",
    "- The text might focuses on a few main topics, so certain words appear much more than in general language.\n",
    "- Cleaning and removing stopwords change how rare words appear at the tail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f7a667",
   "metadata": {},
   "source": [
    "## TF-IDF scores and Wordclouds\n",
    "\n",
    "Term Frequency–Inverse Document Frequency (TF-IDF) is a measure that highlights how important a word is within one document (or community) compared to the entire corpus.\n",
    "- Term Frequency (TF): how often a word appears in a document.\n",
    "- Inverse Document Frequency (IDF): how rare that word is across all documents.\n",
    "- TF-IDF = TF × IDF: high when a word is frequent in one document but rare overall.\n",
    "\n",
    "In this context:\n",
    "- Each community is treated as a “document.”\n",
    "- TF-IDF identifies keywords that are characteristic for each community, i.e. terms that distinguish that community’s language use from others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e1325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all cleaned text within each community\n",
    "community_texts = (\n",
    "    df_nodes.groupby(\"community\")[\"tokens\"]\n",
    "    .apply(lambda x: \" \".join([\" \".join(tokens) for tokens in x]))\n",
    "    .reset_index()\n",
    ")\n",
    "community_texts.columns = [\"community\", \"clean_text\"]\n",
    "community_texts.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a00f4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique communities:\", df_nodes['community'].nunique())\n",
    "print(\"Total users:\", len(df_nodes))\n",
    "print(df_nodes['community'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467e3084",
   "metadata": {},
   "outputs": [],
   "source": [
    "community_tokens = (\n",
    "    df_nodes\n",
    "    .dropna(subset=['tokens', 'community'])\n",
    "    .groupby('community', observed=True)['tokens']\n",
    "    .apply(lambda groups: [tok for sublist in groups for tok in sublist])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "community_tokens['n_tokens'] = community_tokens['tokens'].apply(len)\n",
    "community_tokens['n_docs'] = df_nodes.groupby('community')['node_id'].count().values\n",
    "\n",
    "community_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee2051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print amount of tokens and documents per community for the 10 largest communities\n",
    "for idx, row in community_tokens.sort_values(by='n_tokens', ascending=False).head(10).iterrows():\n",
    "    print(f\"Community {row['community']} has {row['n_tokens']} tokens across {row['n_docs']} users\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e8dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Top_9_communities = list(df_nodes['community'].value_counts().nlargest(9).index)\n",
    "\n",
    "TF_IDF = {}\n",
    "TF = {}\n",
    "top_users = {}   # top 3 users by token count\n",
    "\n",
    "for c in Top_9_communities:\n",
    "    # Documents in this community (each doc = one user's token list)\n",
    "    docs_c = df_nodes.loc[df_nodes[\"community\"] == c, [\"node_id\", \"tokens\"]].dropna(subset=[\"tokens\"])\n",
    "    N_docs = len(docs_c)\n",
    "    if N_docs == 0:\n",
    "        continue\n",
    "\n",
    "    # --- 1) Compute per-community term and document frequencies ---\n",
    "    token_doc_frequency = Counter()\n",
    "    token_term_frequency = Counter()\n",
    "\n",
    "    for _, row in docs_c.iterrows():\n",
    "        toks = list(row[\"tokens\"])\n",
    "        token_term_frequency.update(toks)\n",
    "        token_doc_frequency.update(set(toks))\n",
    "\n",
    "    # --- 2) Compute TF, IDF, and TF-IDF ---\n",
    "    total_tokens = token_term_frequency.total()\n",
    "    TF[c] = {tok: freq / total_tokens for tok, freq in token_term_frequency.items()}\n",
    "\n",
    "    TF_IDF[c] = {}\n",
    "    for tok, df in token_doc_frequency.items():\n",
    "        idf = np.log((1 + N_docs) / (1 + df)) + 1.0\n",
    "        TF_IDF[c][tok] = TF[c].get(tok, 0.0) * idf\n",
    "\n",
    "    # --- 3) Print top TF-IDF terms ---\n",
    "    top_TF_IDF = sorted(TF_IDF[c].items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "    print(\"\\n-----------------------------------------------------------------------\")\n",
    "    print(f\"Community {c} — top 20 TF-IDF terms\")\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    for token, score in top_TF_IDF:\n",
    "        print(f\"{token}\\t{score:.4f}\")\n",
    "\n",
    "    # --- 4) Top 3 users by token count ---\n",
    "    users_sorted = (\n",
    "        docs_c.assign(n_tokens=lambda d: d[\"tokens\"].apply(len))\n",
    "              .sort_values(\"n_tokens\", ascending=False)\n",
    "              .loc[:, [\"node_id\", \"n_tokens\"]]\n",
    "              .head(3)\n",
    "    )\n",
    "\n",
    "    # Convert node_id to native int for clean printing\n",
    "    top_users[c] = [str(uid) for uid in users_sorted[\"node_id\"].values]\n",
    "\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    print(f\"Top 3 high-volume users (by token count) in community {c}:\")\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    for uid, tokcount in zip(top_users[c], users_sorted[\"n_tokens\"]):\n",
    "        print(f\"User ID: {uid} — tokens: {tokcount}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc18b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Build top TF-IDF word lists and corresponding top users for display\n",
    "top_tfidf_words = {}\n",
    "Top_com_users = {}\n",
    "\n",
    "for c in Top_9_communities:\n",
    "    # top TF-IDF words for this community\n",
    "    top_tfidf_words[c] = [\n",
    "        word for word, score in sorted(TF_IDF[c].items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    # top 3 users (already computed earlier)\n",
    "    Top_com_users[c] = top_users.get(c, [])\n",
    "\n",
    "# --- 3x3 grid of subplots ---\n",
    "fig, axs = plt.subplots(3, 3, figsize=(16, 16))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for ax, c in zip(axs, Top_9_communities):\n",
    "    # generate word cloud from TF-IDF frequencies\n",
    "    wordcloud = WordCloud(\n",
    "        width=400,\n",
    "        height=400,\n",
    "        background_color='white',\n",
    "        colormap='viridis'\n",
    "    ).generate_from_frequencies(TF_IDF[c])\n",
    "\n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    # format top 3 users nicely for the title\n",
    "    user_lines = \"\\n\".join([f\"User {uid}\" for uid in Top_com_users[c]])\n",
    "    ax.set_title(f\"Community {c}\\nTop 3 users:\\n{user_lines}\", fontsize=12)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Remove empty subplots if fewer than 9 communities\n",
    "for i in range(len(Top_9_communities), len(axs)):\n",
    "    axs[i].axis('off')\n",
    "\n",
    "plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05, wspace=0.2, hspace=0.3)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bedc86d",
   "metadata": {},
   "source": [
    "## Compare to sklearn prebuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be04c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "# Aggregate all tokens per community into one long string\n",
    "community_texts = (\n",
    "    df_nodes.groupby(\"community\")[\"tokens\"]\n",
    "            .apply(lambda token_lists: \" \".join(chain.from_iterable(token_lists)))\n",
    "            .reset_index()\n",
    ")\n",
    "\n",
    "community_texts.columns = [\"community\", \"clean_text\"]\n",
    "\n",
    "# Take the top 9 most populated communities\n",
    "Top_9_communities = list(\n",
    "    df_nodes[\"community\"].value_counts().nlargest(9).index\n",
    ")\n",
    "community_texts = community_texts[community_texts[\"community\"].isin(Top_9_communities)]\n",
    "\n",
    "community_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9efe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=None,          # already cleaned\n",
    "    max_features=3000,        # optional cap\n",
    "    sublinear_tf=True,\n",
    "    min_df= 4,#5,                 # ignore words appearing in <5 communities\n",
    "    max_df=0.8                # ignore very common words\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(community_texts[\"clean_text\"])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f\"TF-IDF matrix shape: {X.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d9ab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_terms_sklearn = {}\n",
    "\n",
    "for i, comm in enumerate(community_texts[\"community\"]):\n",
    "    row = X[i].toarray().flatten()\n",
    "    top_idx = row.argsort()[-100:][::-1]\n",
    "    top_terms_sklearn[comm] = dict(zip(feature_names[top_idx], row[top_idx]))\n",
    "\n",
    "# --- Pretty print top words per community ---\n",
    "for comm in community_texts[\"community\"]:\n",
    "    top_items = sorted(top_terms_sklearn[comm].items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(f\"Community {comm} — Top 20 TF-IDF terms (scikit-learn)\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Rank':<5}{'Word':<20}{'TF-IDF Score':>15}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for rank, (word, score) in enumerate(top_items, start=1):\n",
    "        print(f\"{rank:<5}{word:<20}{score:>15.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4927441",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(16, 16))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for ax, c in zip(axs, Top_9_communities):\n",
    "    words = top_terms_sklearn.get(c, {})\n",
    "    \n",
    "    # --- Fix 1: Replace NaNs/Infs and remove zero values ---\n",
    "    clean_words = {k: float(v) for k, v in words.items() if np.isfinite(v) and v > 0}\n",
    "    \n",
    "    if not clean_words:\n",
    "        ax.axis(\"off\")\n",
    "        continue\n",
    "\n",
    "    # --- Fix 2: Normalize frequencies to avoid large-scale differences ---\n",
    "    max_val = max(clean_words.values())\n",
    "    clean_words = {k: v / max_val for k, v in clean_words.items()}\n",
    "\n",
    "    wc = WordCloud(\n",
    "        width=400,\n",
    "        height=400,\n",
    "        background_color=\"white\",\n",
    "        colormap=\"plasma\"\n",
    "    ).generate_from_frequencies(clean_words)\n",
    "    \n",
    "    ax.imshow(wc, interpolation=\"bilinear\")\n",
    "    ax.set_title(f\"Community {c} — sklearn TF-IDF\", fontsize=12)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Turn off unused subplots\n",
    "for i in range(len(Top_9_communities), len(axs)):\n",
    "    axs[i].axis(\"off\")\n",
    "\n",
    "plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05, wspace=0.2, hspace=0.3)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctfds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
