{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ea257d",
   "metadata": {},
   "source": [
    "# Frequent items analysis of reddit communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4c2c536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/livdreyerjohansen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import networkx as nx\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "import html\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import math\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88d1fd7",
   "metadata": {},
   "source": [
    "## Dataload and cleaning\n",
    "\n",
    "To analysize the what frequent items we may see in the reddit communities found in 03_NetworkAnalysis.ipynb, we must first load the graph with the added attributes, that tell what community each node belongs in and the posts created by each node. We have chosen to only look at the 6 largest communities, by number of nodes, as the distribution of nodes / community is very heavly right skewed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cee6b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load graph\n",
    "\n",
    "G = nx.read_gml('FINAL_reddit_graph_with_louvain_communities.gml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de689bc",
   "metadata": {},
   "source": [
    "To ensure we do not include posts that are either deleted (\"\\[deleted\\]\") or removed (\"\\[removed\\]\"), both basic reddit features that happen independently of what forum you are in, we remove both. Furthermore, we remove each post that was removed by a bot, which is clear in the text which the bot uses to explain why a post or comment is deleted. We then construct a dataframe with all posts and their community (and original poster (OP in reddit linguistics))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03a8126f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id                                               text  community\n",
      "0  1  \"Huh it's still not legalized yet. America is ...        130\n",
      "1  1  \"Hey charisma helps. Everybody wants to sleep ...        130\n",
      "2  1  Aren't the jedis not really good guys though? ...        130\n",
      "3  1  Wait but ferb is the better looking one with a...        130\n",
      "4  1  Great now you live with a hole in your head fo...        130\n",
      "\n",
      "Number of original posts:\n",
      "2,664,156\n",
      "Number of removed posts:\n",
      "158,251\n",
      "Number of posts in dataframe:\n",
      "2,505,905\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "rows_count = 0\n",
    "allowed_rows = 0\n",
    "\n",
    "for node, data in G.nodes(data=True):\n",
    "    community = data.get(\"community\")\n",
    "    posts_dict = data.get(\"posts\", {})\n",
    "\n",
    "    # Ensure it's a dictionary\n",
    "    if not isinstance(posts_dict, dict):\n",
    "        posts_dict = {\"default\": posts_dict}\n",
    "\n",
    "    # Loop through each list of posts in the dictionary\n",
    "    for key, posts in posts_dict.items():\n",
    "        if not isinstance(posts, list):\n",
    "            posts = [posts]\n",
    "\n",
    "        for post in posts:\n",
    "            rows_count += 1\n",
    "            # Skip empty, deleted/removed posts, or posts containing the bot line\n",
    "            if post and post not in ['[deleted]', '[removed]'] and \\\n",
    "               \"*I am a bot, and this action was performed automatically.\" not in post:\n",
    "                allowed_rows += 1\n",
    "                rows.append({\n",
    "                    \"id\": node,\n",
    "                    \"text\": post,\n",
    "                    \"community\": community\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nNumber of original posts:\")\n",
    "print(f\"{rows_count:,}\")\n",
    "print(\"Number of removed posts:\")\n",
    "print(f\"{rows_count-allowed_rows:,}\")\n",
    "print(\"Number of posts in dataframe:\")\n",
    "print(f\"{allowed_rows:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dbf9c4",
   "metadata": {},
   "source": [
    "We identify the top-6 largest communities in terms of nodes to continue working with only them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f4063e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = df['community'].unique().tolist()\n",
    "communities_dict = dict.fromkeys(communities, 0)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    communities_dict[row['community']] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f165e4",
   "metadata": {},
   "source": [
    "Filter the dataframe to only contain posts from top-6 communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c521f85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 6 communities by number of posts (posts, community): [(644383, 130), (635828, 89), (630172, 129), (399843, 191), (154109, 220), (38490, 188)]\n",
      "Number of posts in top 6 communities: 2,502,825\n",
      "Number of posts removed based on non identity in top 6: 3,080\n"
     ]
    }
   ],
   "source": [
    "comms_list = list(sorted( ((v,k) for k,v in communities_dict.items()), reverse=True))\n",
    "comms_list = comms_list[:6]\n",
    "top_6_communities = [item[1] for item in comms_list]\n",
    "\n",
    "df_filtered = df[df['community'].isin(top_6_communities)].copy()\n",
    "\n",
    "print(\"\\nTop 6 communities by number of posts (posts, community):\", comms_list)\n",
    "\n",
    "print(f\"Number of posts in top 6 communities: {len(df_filtered):,}\")\n",
    "\n",
    "print(f\"Number of posts removed based on non identity in top 6: {(len(df) - len(df_filtered)):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1289c493",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "We filter out parts of post that we deem have little semantic value. We aim to find frequent items and frequent itemsets (item pairs), and would assume that stop words regularly occur in more than 1% of baskets. As we are working with online fora, we chose to add certain slang-terms as stop words. We furthermore remove:\n",
    "\n",
    "- html entities\n",
    "- URL's\n",
    "- non-text artifacts (such as \"/\", \"?\", \"!\" etc.)\n",
    "- remaining \"removed\" and \"deleted\" artifacts that were not removed in the previous code block due to the way the post was loaded\n",
    "- short words (length of 2 or less)\n",
    "\n",
    "Additionally, we make all words lowercase to steamline and tokenize by word (meaning each word will be its own token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "422cbe8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "community",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tokens",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "n_tokens",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "d5db6a87-dd53-4a2f-9973-c67e2270e39e",
       "rows": [
        [
         "0",
         "1",
         "\"Huh it's still not legalized yet. America is weirdly antiquated\" ",
         "130",
         "['huh', 'still', 'legalized', 'yet', 'america', 'weirdly', 'antiquated']",
         "7"
        ],
        [
         "1",
         "1",
         "\"Hey charisma helps. Everybody wants to sleep with pretty people. Doesn't mean they enjoy dating them\" ",
         "130",
         "['hey', 'charisma', 'helps', 'everybody', 'wants', 'sleep', 'pretty', 'people', 'mean', 'enjoy', 'dating']",
         "11"
        ],
        [
         "2",
         "1",
         "Aren't the jedis not really good guys though? Like they protect the status quo. That and how they serve the same cosmic deity that doesn't care about anything and can't be bothered by which of its \"\"sides\"\" its pathetic worshippers venerate? Idk much about star wars sorry if I got it wrong ",
         "130",
         "['jedis', 'really', 'good', 'guys', 'though', 'protect', 'status', 'quo', 'serve', 'cosmic', 'deity', 'care', 'anything', 'bothered', 'sides', 'pathetic', 'worshippers', 'venerate', 'much', 'star', 'wars', 'sorry', 'got', 'wrong']",
         "24"
        ],
        [
         "3",
         "1",
         "Wait but ferb is the better looking one with actual game. Phineas is a fucking geometry figure for god sakes ",
         "130",
         "['wait', 'ferb', 'better', 'looking', 'one', 'actual', 'game', 'phineas', 'fucking', 'geometry', 'figure', 'god', 'sakes']",
         "13"
        ],
        [
         "4",
         "1",
         "Great now you live with a hole in your head for eternity ",
         "130",
         "['great', 'live', 'hole', 'head', 'eternity']",
         "5"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>community</th>\n",
       "      <th>tokens</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Huh it's still not legalized yet. America is ...</td>\n",
       "      <td>130</td>\n",
       "      <td>[huh, still, legalized, yet, america, weirdly,...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Hey charisma helps. Everybody wants to sleep ...</td>\n",
       "      <td>130</td>\n",
       "      <td>[hey, charisma, helps, everybody, wants, sleep...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Aren't the jedis not really good guys though? ...</td>\n",
       "      <td>130</td>\n",
       "      <td>[jedis, really, good, guys, though, protect, s...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Wait but ferb is the better looking one with a...</td>\n",
       "      <td>130</td>\n",
       "      <td>[wait, ferb, better, looking, one, actual, gam...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Great now you live with a hole in your head fo...</td>\n",
       "      <td>130</td>\n",
       "      <td>[great, live, hole, head, eternity]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                               text  community  \\\n",
       "0  1  \"Huh it's still not legalized yet. America is ...        130   \n",
       "1  1  \"Hey charisma helps. Everybody wants to sleep ...        130   \n",
       "2  1  Aren't the jedis not really good guys though? ...        130   \n",
       "3  1  Wait but ferb is the better looking one with a...        130   \n",
       "4  1  Great now you live with a hole in your head fo...        130   \n",
       "\n",
       "                                              tokens  n_tokens  \n",
       "0  [huh, still, legalized, yet, america, weirdly,...         7  \n",
       "1  [hey, charisma, helps, everybody, wants, sleep...        11  \n",
       "2  [jedis, really, good, guys, though, protect, s...        24  \n",
       "3  [wait, ferb, better, looking, one, actual, gam...        13  \n",
       "4                [great, live, hole, head, eternity]         5  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize and clean text data\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# extend basic english stopwords with slang terms\n",
    "extra_stops = {\n",
    "    'lol', 'xd', 'haha', 'hahaah', 'omg', 'u', 'ur', 'im', 'ive', 'idk', \n",
    "    'dont', 'cant', 'wont', 'aint', 'ya', 'tho', 'tho', 'nah', 'btw', \n",
    "    'like', 'yeah', 'yep', 'ok', 'okay', 'pls', 'please'\n",
    "}\n",
    "stop_words.update(extra_stops)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # decode HTML entities: &amp; → &, &#x200B; → zero-width space, etc.\n",
    "    text = html.unescape(text)\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
    "    # keep only letters and spaces\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    # tokenize by whitespace\n",
    "    tokens = text.split()\n",
    "    # remove stopwords and very short tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    if len(tokens) == 1 and tokens[0] in {\"removed\", \"deleted\"}:\n",
    "        return []\n",
    "    return tokens\n",
    "\n",
    "df_filtered[\"tokens\"] = df_filtered[\"text\"].apply(preprocess_text)\n",
    "df_filtered[\"n_tokens\"] = df_filtered[\"tokens\"].apply(len)\n",
    "\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4075dcf8",
   "metadata": {},
   "source": [
    "# Frequent items and the A-priori algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27146132",
   "metadata": {},
   "source": [
    "We process the tokenized posts by identifying the amount of unique tokens for each community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4f9c689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community 130:\n",
      "  Total tokens: 8,227,111\n",
      "  Unique tokens: 140,320\n",
      "Community 89:\n",
      "  Total tokens: 7,790,340\n",
      "  Unique tokens: 149,732\n",
      "Community 129:\n",
      "  Total tokens: 6,759,663\n",
      "  Unique tokens: 146,731\n",
      "Community 191:\n",
      "  Total tokens: 4,754,898\n",
      "  Unique tokens: 89,035\n",
      "Community 220:\n",
      "  Total tokens: 1,236,945\n",
      "  Unique tokens: 59,694\n",
      "Community 188:\n",
      "  Total tokens: 1,315,555\n",
      "  Unique tokens: 33,101\n"
     ]
    }
   ],
   "source": [
    "# build token statistics for each of the top 6 communities\n",
    "\n",
    "community_token_stats = {}\n",
    "\n",
    "for community_id in top_6_communities:\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == community_id]\n",
    "\n",
    "    # flatten all tokens for this community\n",
    "    all_tokens = []\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    unique_tokens = set(all_tokens)\n",
    "\n",
    "    community_token_stats[community_id] = {\n",
    "        \"n_tokens\": len(all_tokens),\n",
    "        \"n_unique_tokens\": len(unique_tokens),       \n",
    "        \"unique_tokens\": unique_tokens \n",
    "    }\n",
    "\n",
    "\n",
    "for cid in top_6_communities:\n",
    "    print(f\"Community {cid}:\")\n",
    "    print(f\"  Total tokens: {community_token_stats[cid]['n_tokens']:,}\")\n",
    "    print(f\"  Unique tokens: {community_token_stats[cid]['n_unique_tokens']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eca728",
   "metadata": {},
   "source": [
    "## First pass of the A-priori algorithm\n",
    "\n",
    "In the first pass of the A-priori algoritm, we initialize a dataframe for each of the communities. In this dataframe, we will store each of the unique tokens found previously, assign them each an integer from 0 to n-1 (number of unique tokens), and count how many baskets (posts) the item (token) appears in. It is important to note that we do not count the total occurrence of the token but only the amount of posts it appears in. In Mining of Massive Datasets, Section 6.2.2, the first pass is described as labeling integers 1 to n, but to keep it within the python framework, we label 0 to n-1 and mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f323c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori_tables = {}\n",
    "\n",
    "for cid in top_6_communities:\n",
    "    # get df for this community\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "    unique_tokens = list(community_token_stats[cid][\"unique_tokens\"])\n",
    "\n",
    "    # apriori table\n",
    "    df_apriori = pd.DataFrame({\n",
    "        \"word\": unique_tokens,\n",
    "        \"integer\": range(len(unique_tokens))\n",
    "    })\n",
    "\n",
    "    # give each word an integer from 0 to n-1\n",
    "    word_to_int = dict(zip(df_apriori[\"word\"], df_apriori[\"integer\"]))\n",
    "\n",
    "    # count posts that contain each token\n",
    "    array_of_counts = np.zeros(len(unique_tokens), dtype=int)\n",
    "\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        for token in set(tokens):             \n",
    "            array_of_counts[word_to_int[token]] += 1\n",
    "\n",
    "    df_apriori[\"count\"] = array_of_counts\n",
    "\n",
    "    # save in dict\n",
    "    apriori_tables[cid] = df_apriori\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "692f4753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 130\n",
      "          word  integer  count\n",
      "44224    women    44224  64054\n",
      "30485      get    30485  54969\n",
      "134754  people   134754  54841\n",
      "137670   would   137670  45478\n",
      "98667     even    98667  41610\n",
      "\n",
      "Community 89\n",
      "          word  integer  count\n",
      "46905    women    46905  61740\n",
      "32425      get    32425  55819\n",
      "143859  people   143859  47917\n",
      "146996   would   146996  41218\n",
      "105328    even   105328  39135\n",
      "\n",
      "Community 129\n",
      "          word  integer  count\n",
      "46115    women    46115  49985\n",
      "31846      get    31846  49251\n",
      "102973    even   102973  37953\n",
      "140853  people   140853  36781\n",
      "144013   would   144013  35755\n",
      "\n",
      "Community 191\n",
      "         word  integer  count\n",
      "19336     get    19336  37192\n",
      "28003   women    28003  36120\n",
      "87399   would    87399  28225\n",
      "85470  people    85470  27657\n",
      "62613    even    62613  27309\n",
      "\n",
      "Community 220\n",
      "         word  integer  count\n",
      "26143     get    26143   8165\n",
      "37703   women    37703   7367\n",
      "24451    even    24451   6899\n",
      "57474   would    57474   5976\n",
      "54904  people    54904   5766\n",
      "\n",
      "Community 188\n",
      "         word  integer  count\n",
      "30465  people    30465  10376\n",
      "9871    think     9871   7425\n",
      "20877   women    20877   7331\n",
      "14479     get    14479   7011\n",
      "21284    know    21284   6431\n"
     ]
    }
   ],
   "source": [
    "# print top 5 tokens by count for each community\n",
    "for cid, df_apriori in apriori_tables.items():\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "    print(df_apriori.sort_values(by=\"count\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5646a40",
   "metadata": {},
   "source": [
    "## Between the passes of A-priori\n",
    "\n",
    "We create frequency tables where we assign each word an integer from 1-m, where m = number of frequent singletons (words), if the support of the word => 1%. In other words, it must appear in 1% or more of the baskets. If the word is not frequent, we assign it 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d2d554ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 130 frequent items:\n",
      "              word  integer  count  freq_integer\n",
      "159          place      159   7391             1\n",
      "600           used      600   6884             2\n",
      "1597           end     1597   6460             3\n",
      "3313    attractive     3313  13017             4\n",
      "3731         looks     3731  18848             5\n",
      "...            ...      ...    ...           ...\n",
      "136959        also   136959  19237           173\n",
      "137670       would   137670  45478           174\n",
      "137781         sub   137781  16094           175\n",
      "139255         guy   139255  21330           176\n",
      "139807        shit   139807  24811           177\n",
      "\n",
      "[177 rows x 4 columns]\n",
      "\n",
      "Community 89 frequent items:\n",
      "              word  integer  count  freq_integer\n",
      "167          place      167   6568             1\n",
      "626           used      626   6731             2\n",
      "1762           end     1762   6531             3\n",
      "3531    attractive     3531  11692             4\n",
      "3980         looks     3980  18198             5\n",
      "...            ...      ...    ...           ...\n",
      "146225        also   146225  17903           167\n",
      "146996       would   146996  41218           168\n",
      "147096         sub   147096  13349           169\n",
      "148614         guy   148614  21598           170\n",
      "149194        shit   149194  24519           171\n",
      "\n",
      "[171 rows x 4 columns]\n",
      "\n",
      "Community 129 frequent items:\n",
      "              word  integer  count  freq_integer\n",
      "3418    attractive     3418   9800             1\n",
      "3875         looks     3875  16721             2\n",
      "4221          hate     4221   8793             3\n",
      "4443          best     4443   7127             4\n",
      "4451        pretty     4451  11029             5\n",
      "...            ...      ...    ...           ...\n",
      "143243        also   143243  15788           141\n",
      "144013       would   144013  35755           142\n",
      "144114         sub   144114  12566           143\n",
      "145572         guy   145572  20376           144\n",
      "146167        shit   146167  24806           145\n",
      "\n",
      "[145 rows x 4 columns]\n",
      "\n",
      "Community 191 frequent items:\n",
      "             word  integer  count  freq_integer\n",
      "90          place       90   4313             1\n",
      "384          used      384   4664             2\n",
      "1057          end     1057   4637             3\n",
      "2140   attractive     2140   7509             4\n",
      "2406        looks     2406  11956             5\n",
      "...           ...      ...    ...           ...\n",
      "86916        also    86916  12061           164\n",
      "87399       would    87399  28225           165\n",
      "87462         sub    87462  10322           166\n",
      "88363         guy    88363  14053           167\n",
      "88718        shit    88718  16942           168\n",
      "\n",
      "[168 rows x 4 columns]\n",
      "\n",
      "Community 220 frequent items:\n",
      "          word  integer  count  freq_integer\n",
      "3154     looks     3154   2742             1\n",
      "3431      hate     3431   1771             2\n",
      "3635    pretty     3635   1584             3\n",
      "4208   nothing     4208   1816             4\n",
      "4782      high     4782   1921             5\n",
      "...        ...      ...    ...           ...\n",
      "57554      sub    57554   1946            90\n",
      "58694      bad    58694   1891            91\n",
      "58763      guy    58763   3124            92\n",
      "58791    could    58791   2557            93\n",
      "59258     shit    59258   4674            94\n",
      "\n",
      "[94 rows x 4 columns]\n",
      "\n",
      "Community 188 frequent items:\n",
      "             word  integer  count  freq_integer\n",
      "43         become       43    920             1\n",
      "66         second       66    564             2\n",
      "70          place       70   1221             3\n",
      "132        honest      132    567             4\n",
      "265         asked      265    685             5\n",
      "...           ...      ...    ...           ...\n",
      "32855        shit    32855   1430           539\n",
      "32901        sort    32901    673           540\n",
      "32945   sometimes    32945   1250           541\n",
      "33006  confidence    33006    782           542\n",
      "33065    everyone    33065   2136           543\n",
      "\n",
      "[543 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "freq_tables = {}\n",
    "\n",
    "threshold_factor = 0.01  \n",
    "\n",
    "for cid, df_apriori in apriori_tables.items():\n",
    "\n",
    "    # threshold is 1% of posts in that community\n",
    "    threshold = threshold_factor * len(df_filtered[df_filtered[\"community\"] == cid])\n",
    "\n",
    "    frequent_map = np.zeros(len(df_apriori), dtype=int)\n",
    "    new_id = 1\n",
    "\n",
    "    for old_id, count in enumerate(df_apriori['count']):\n",
    "        if count >= threshold:\n",
    "            frequent_map[old_id] = new_id\n",
    "            new_id += 1\n",
    "        else:\n",
    "            frequent_map[old_id] = 0\n",
    "\n",
    "    # add freq_integer column\n",
    "    df_apriori['freq_integer'] = frequent_map\n",
    "\n",
    "    # store only frequent items in new dictionary\n",
    "    df_freq = df_apriori[df_apriori['freq_integer'] != 0].copy()\n",
    "    freq_tables[cid] = df_freq\n",
    "\n",
    "    print(f\"\\nCommunity {cid} frequent items:\")\n",
    "    print(df_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d38f5ea",
   "metadata": {},
   "source": [
    "## Second pass of the A-priori algorithm\n",
    "\n",
    "For the second pass, we first find all pairs of frequent words from the previous dataframes. We then create pairs of those, making sure to remove duplicates. We apply the support threshold of 1% here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cab3263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 130\n",
      "Top pairs:\n",
      "             item_set  count\n",
      "923      (men, women)  17992\n",
      "409      (get, women)  12466\n",
      "2374  (people, women)  11308\n",
      "96      (get, people)  11257\n",
      "963   (people, think)  10248\n",
      "Total pairs where Support(I) => s of 0.01: 39\n",
      "\n",
      "Community 89\n",
      "Top pairs:\n",
      "             item_set  count\n",
      "35       (men, women)  17505\n",
      "594      (get, women)  12642\n",
      "557     (get, people)  10492\n",
      "127   (people, women)   9955\n",
      "1084    (even, women)   9487\n",
      "Total pairs where Support(I) => s of 0.01: 30\n",
      "\n",
      "Community 129\n",
      "Top pairs:\n",
      "           item_set  count\n",
      "533    (men, women)  14147\n",
      "1889   (get, women)   9308\n",
      "146     (even, get)   8102\n",
      "1187  (even, women)   7761\n",
      "680   (get, people)   7473\n",
      "Total pairs where Support(I) => s of 0.01: 11\n",
      "\n",
      "Community 191\n",
      "Top pairs:\n",
      "           item_set  count\n",
      "2893   (men, women)   9671\n",
      "10     (get, women)   7181\n",
      "2209    (even, get)   6164\n",
      "543   (get, people)   5949\n",
      "4144  (even, women)   5711\n",
      "Total pairs where Support(I) => s of 0.01: 23\n",
      "\n",
      "Community 220\n",
      "Top pairs:\n",
      "        item_set  count\n",
      "11  (men, women)   1949\n",
      "Total pairs where Support(I) => s of 0.01: 1\n",
      "\n",
      "Community 188\n",
      "Top pairs:\n",
      "             item_set  count\n",
      "1035  (people, think)   3588\n",
      "2580    (get, people)   3356\n",
      "7810   (know, people)   2995\n",
      "1736    (one, people)   2876\n",
      "1026   (people, want)   2812\n",
      "Total pairs where Support(I) => s of 0.01: 4018\n"
     ]
    }
   ],
   "source": [
    "pair_tables = {}   # store results for each community\n",
    "\n",
    "for cid in top_6_communities:\n",
    "\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "\n",
    "    # pull posts for this community\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "    N = len(df_comm)\n",
    "\n",
    "    # fetch frequent 1-itemset for this community\n",
    "    df_freq = freq_tables[cid]\n",
    "    frequent_words_set = set(df_freq[\"word\"])\n",
    "\n",
    "    # counter for all frequent pairs\n",
    "    pair_counter = Counter()\n",
    "\n",
    "    # iterate over all posts\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        # keep only frequent tokens\n",
    "        frequent_tokens = [t for t in tokens if t in frequent_words_set]\n",
    "\n",
    "        # deduplicate within a post\n",
    "        unique_tokens = set(frequent_tokens)\n",
    "\n",
    "        # count each 2-item combination in this post\n",
    "        for pair in combinations(unique_tokens, 2):\n",
    "            pair_counter[tuple(sorted(pair))] += 1\n",
    "\n",
    "    # convert counter → dataframe\n",
    "    df_pairs = pd.DataFrame(pair_counter.items(), columns=[\"item_set\", \"count\"])\n",
    "\n",
    "    # threshold for frequent 2-itemsets (1% of posts)\n",
    "    threshold = math.ceil(0.01 * N)\n",
    "    df_pairs = df_pairs[df_pairs[\"count\"] >= threshold]\n",
    "\n",
    "    # store\n",
    "    pair_tables[cid] = df_pairs\n",
    "\n",
    "    # print summary\n",
    "    print(\"Top pairs:\")\n",
    "    print(df_pairs.sort_values(by=\"count\", ascending=False).head())\n",
    "    print(\"Total pairs where Support(I) => s of 0.01:\", len(df_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b44fc2",
   "metadata": {},
   "source": [
    "# A-priori using library\n",
    "\n",
    "To further validate the above results, we also implemented the A-priori algorithm using mlxtend. We find that the results of using the mlxtend framwork are congruent with the results found by implementing the A-priori algorithm as described in Mining of Massive Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e87124af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 130\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 177\n",
      "\n",
      "Community 89\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 171\n",
      "\n",
      "Community 129\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 145\n",
      "\n",
      "Community 191\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 168\n",
      "\n",
      "Community 220\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 94\n",
      "\n",
      "Community 188\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 543\n"
     ]
    }
   ],
   "source": [
    "te = TransactionEncoder()\n",
    "encoded_tables = {} \n",
    "\n",
    "for cid in top_6_communities:\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "    \n",
    "    freq_words = set(freq_tables[cid][\"word\"])\n",
    "    \n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "    transactions = [\n",
    "        [t for t in tokens if t in freq_words]\n",
    "        for tokens in df_comm[\"tokens\"]\n",
    "    ]\n",
    "\n",
    "    te_array = te.fit(transactions).transform(transactions)\n",
    "    df_encoded = pd.DataFrame(te_array, columns=te.columns_)\n",
    "    \n",
    "    encoded_tables[cid] = df_encoded\n",
    "    \n",
    "    print(f\"Number of frequent items (singletons) with Support(I) => s of 0.01: {df_encoded.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ac9b3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 130\n",
      "Number of frequent 2-itemsets: 39\n",
      "Top 5 pairs:\n",
      "      support         itemsets\n",
      "200  0.027921     (women, men)\n",
      "190  0.019346     (women, get)\n",
      "207  0.017549  (people, women)\n",
      "187  0.017469    (people, get)\n",
      "205  0.015904  (people, think)\n",
      "\n",
      "Community 89\n",
      "Number of frequent 2-itemsets: 30\n",
      "Top 5 pairs:\n",
      "      support         itemsets\n",
      "189  0.027531     (women, men)\n",
      "183  0.019883     (women, get)\n",
      "180  0.016501    (people, get)\n",
      "194  0.015657  (people, women)\n",
      "175  0.014921    (women, even)\n",
      "\n",
      "Community 129\n",
      "Number of frequent 2-itemsets: 11\n",
      "Top 5 pairs:\n",
      "      support       itemsets\n",
      "151  0.022449   (women, men)\n",
      "149  0.014771   (women, get)\n",
      "145  0.012857    (even, get)\n",
      "147  0.012316  (even, women)\n",
      "148  0.011859  (people, get)\n",
      "\n",
      "Community 191\n",
      "Number of frequent 2-itemsets: 23\n",
      "Top 5 pairs:\n",
      "      support       itemsets\n",
      "181  0.024187   (women, men)\n",
      "178  0.017960   (women, get)\n",
      "168  0.015416    (even, get)\n",
      "175  0.014878  (people, get)\n",
      "170  0.014283  (even, women)\n",
      "\n",
      "Community 220\n",
      "Number of frequent 2-itemsets: 1\n",
      "Top 5 pairs:\n",
      "     support      itemsets\n",
      "94  0.012647  (women, men)\n",
      "\n",
      "Community 188\n",
      "Number of frequent 2-itemsets: 4018\n",
      "Top 5 pairs:\n",
      "       support         itemsets\n",
      "3841  0.093219  (people, think)\n",
      "2164  0.087191    (people, get)\n",
      "2833  0.077812   (people, know)\n",
      "3673  0.074721    (people, one)\n",
      "3859  0.073058   (people, want)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "frequent_2_itemsets_by_community = {}\n",
    "\n",
    "for cid in top_6_communities:\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "    \n",
    "    df_encoded = encoded_tables[cid] \n",
    "    \n",
    "    frequent_itemsets = apriori(df_encoded, min_support=0.01, use_colnames=True)\n",
    "    \n",
    "    # filter to only 2-itemsets\n",
    "    frequent_2_itemsets = frequent_itemsets[\n",
    "        frequent_itemsets['itemsets'].apply(lambda x: len(x) == 2)\n",
    "    ].copy()\n",
    "    \n",
    "    frequent_2_itemsets_by_community[cid] = frequent_2_itemsets\n",
    "    \n",
    "    print(\"Number of frequent 2-itemsets:\", len(frequent_2_itemsets))\n",
    "    print(\"Top 5 pairs:\")\n",
    "    print(frequent_2_itemsets.sort_values(by=\"support\", ascending=False).head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
