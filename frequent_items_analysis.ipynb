{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ea257d",
   "metadata": {},
   "source": [
    "# Frequent items analysis of reddit communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4c2c536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/livdreyerjohansen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import networkx as nx\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from itertools import combinations\n",
    "import html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88d1fd7",
   "metadata": {},
   "source": [
    "## Dataload and cleaning\n",
    "\n",
    "To analysize the what frequent items we may see in the reddit communities found in 03_NetworkAnalysis.ipynb, we must first load the graph with the added attributes, that tell what community each node belongs in and the posts created by each node. We have chosen to only look at the 6 largest communities, by number of nodes, as the distribution of nodes / community is very heavly right skewed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee6b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load graph\n",
    "\n",
    "G = nx.read_gml('reddit_graph_with_communities.gml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de689bc",
   "metadata": {},
   "source": [
    "To ensure we do not include posts that are either deleted (\"\\[deleted\\]\") or removed (\"\\[removed\\]\"), both basic reddit features that happen independently of what forum you are in, we remove both. Furthermore, we remove each post that was removed by a bot, which is clear in the text which the bot uses to explain why a post or comment is deleted. We then construct a dataframe with all posts and their community (and original poster (OP in reddit linguistics))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03a8126f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id                                               text  community\n",
      "0  1  \"Huh it's still not legalized yet. America is ...       5595\n",
      "1  1  \"Hey charisma helps. Everybody wants to sleep ...       5595\n",
      "2  1  Aren't the jedis not really good guys though? ...       5595\n",
      "3  1  Wait but ferb is the better looking one with a...       5595\n",
      "4  1  Great now you live with a hole in your head fo...       5595\n",
      "\n",
      "Number of original posts:\n",
      "2699919\n",
      "Number of removed posts:\n",
      "163205\n",
      "Number of posts in dataframe:\n",
      "163205\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "rows_count = 0\n",
    "allowed_rows = 0\n",
    "\n",
    "for node, data in G.nodes(data=True):\n",
    "    community = data.get(\"community\")\n",
    "    posts_dict = data.get(\"posts\", {})\n",
    "\n",
    "    # Ensure it's a dictionary\n",
    "    if not isinstance(posts_dict, dict):\n",
    "        posts_dict = {\"default\": posts_dict}\n",
    "\n",
    "    # Loop through each list of posts in the dictionary\n",
    "    for key, posts in posts_dict.items():\n",
    "        if not isinstance(posts, list):\n",
    "            posts = [posts]\n",
    "\n",
    "        for post in posts:\n",
    "            rows_count += 1\n",
    "            # Skip empty, deleted/removed posts, or posts containing the bot line\n",
    "            if post and post not in ['[deleted]', '[removed]'] and \\\n",
    "               \"*I am a bot, and this action was performed automatically.\" not in post:\n",
    "                allowed_rows += 1\n",
    "                rows.append({\n",
    "                    \"id\": node,\n",
    "                    \"text\": post,\n",
    "                    \"community\": community\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "print(\"\\nNumber of original posts:\")\n",
    "print(rows_count)\n",
    "print(\"Number of removed posts:\")\n",
    "print(rows_count-allowed_rows)\n",
    "print(\"Number of posts in dataframe:\")\n",
    "print(rows_count-allowed_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dbf9c4",
   "metadata": {},
   "source": [
    "We identify the top-6 largest communities in terms of nodes to continue working with only them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f4063e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = df['community'].unique().tolist()\n",
    "communities_dict = dict.fromkeys(communities, 0)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    communities_dict[row['community']] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f165e4",
   "metadata": {},
   "source": [
    "Filter the dataframe to only contain posts from top-6 communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c521f85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts in top 6 communities: 2,503,488\n",
      "Number of posts removed based on non identity in top 6: 33,226\n"
     ]
    }
   ],
   "source": [
    "comms_list = list(sorted( ((v,k) for k,v in communities_dict.items()), reverse=True))\n",
    "comms_list = comms_list[:6]\n",
    "top_6_communities = [item[1] for item in comms_list]\n",
    "\n",
    "df_filtered = df[df['community'].isin(top_6_communities)].copy()\n",
    "\n",
    "print(f\"Number of posts in top 6 communities: {len(df_filtered):,}\")\n",
    "\n",
    "print(f\"Number of posts removed based on non identity in top 6: {(len(df) - len(df_filtered)):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "422cbe8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "community",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tokens",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "n_tokens",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "c3c34278-8b2d-42bd-b1f7-b9cbc5f09923",
       "rows": [
        [
         "0",
         "1",
         "\"Huh it's still not legalized yet. America is weirdly antiquated\" ",
         "5595",
         "['huh', 'still', 'legalized', 'yet', 'america', 'weirdly', 'antiquated']",
         "7"
        ],
        [
         "1",
         "1",
         "\"Hey charisma helps. Everybody wants to sleep with pretty people. Doesn't mean they enjoy dating them\" ",
         "5595",
         "['hey', 'charisma', 'helps', 'everybody', 'wants', 'sleep', 'pretty', 'people', 'mean', 'enjoy', 'dating']",
         "11"
        ],
        [
         "2",
         "1",
         "Aren't the jedis not really good guys though? Like they protect the status quo. That and how they serve the same cosmic deity that doesn't care about anything and can't be bothered by which of its \"\"sides\"\" its pathetic worshippers venerate? Idk much about star wars sorry if I got it wrong ",
         "5595",
         "['jedis', 'really', 'good', 'guys', 'though', 'protect', 'status', 'quo', 'serve', 'cosmic', 'deity', 'care', 'anything', 'bothered', 'sides', 'pathetic', 'worshippers', 'venerate', 'much', 'star', 'wars', 'sorry', 'got', 'wrong']",
         "24"
        ],
        [
         "3",
         "1",
         "Wait but ferb is the better looking one with actual game. Phineas is a fucking geometry figure for god sakes ",
         "5595",
         "['wait', 'ferb', 'better', 'looking', 'one', 'actual', 'game', 'phineas', 'fucking', 'geometry', 'figure', 'god', 'sakes']",
         "13"
        ],
        [
         "4",
         "1",
         "Great now you live with a hole in your head for eternity ",
         "5595",
         "['great', 'live', 'hole', 'head', 'eternity']",
         "5"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>community</th>\n",
       "      <th>tokens</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Huh it's still not legalized yet. America is ...</td>\n",
       "      <td>5595</td>\n",
       "      <td>[huh, still, legalized, yet, america, weirdly,...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Hey charisma helps. Everybody wants to sleep ...</td>\n",
       "      <td>5595</td>\n",
       "      <td>[hey, charisma, helps, everybody, wants, sleep...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Aren't the jedis not really good guys though? ...</td>\n",
       "      <td>5595</td>\n",
       "      <td>[jedis, really, good, guys, though, protect, s...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Wait but ferb is the better looking one with a...</td>\n",
       "      <td>5595</td>\n",
       "      <td>[wait, ferb, better, looking, one, actual, gam...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Great now you live with a hole in your head fo...</td>\n",
       "      <td>5595</td>\n",
       "      <td>[great, live, hole, head, eternity]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                               text  community  \\\n",
       "0  1  \"Huh it's still not legalized yet. America is ...       5595   \n",
       "1  1  \"Hey charisma helps. Everybody wants to sleep ...       5595   \n",
       "2  1  Aren't the jedis not really good guys though? ...       5595   \n",
       "3  1  Wait but ferb is the better looking one with a...       5595   \n",
       "4  1  Great now you live with a hole in your head fo...       5595   \n",
       "\n",
       "                                              tokens  n_tokens  \n",
       "0  [huh, still, legalized, yet, america, weirdly,...         7  \n",
       "1  [hey, charisma, helps, everybody, wants, sleep...        11  \n",
       "2  [jedis, really, good, guys, though, protect, s...        24  \n",
       "3  [wait, ferb, better, looking, one, actual, gam...        13  \n",
       "4                [great, live, hole, head, eternity]         5  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize and clean text data\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# extend basic english stopwords with slang terms\n",
    "extra_stops = {\n",
    "    'lol', 'xd', 'haha', 'hahaah', 'omg', 'u', 'ur', 'im', 'ive', 'idk', \n",
    "    'dont', 'cant', 'wont', 'aint', 'ya', 'tho', 'tho', 'nah', 'btw', \n",
    "    'like', 'yeah', 'yep', 'ok', 'okay', 'pls', 'please'\n",
    "}\n",
    "stop_words.update(extra_stops)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # decode HTML entities: &amp; → &, &#x200B; → zero-width space, etc.\n",
    "    text = html.unescape(text)\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
    "    # keep only letters and spaces\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    # tokenize by whitespace\n",
    "    tokens = text.split()\n",
    "    # remove stopwords and very short tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    if len(tokens) == 1 and tokens[0] in {\"removed\", \"deleted\"}:\n",
    "        return []\n",
    "    return tokens\n",
    "\n",
    "df_filtered[\"tokens\"] = df_filtered[\"text\"].apply(preprocess_text)\n",
    "df_filtered[\"n_tokens\"] = df_filtered[\"tokens\"].apply(len)\n",
    "\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4075dcf8",
   "metadata": {},
   "source": [
    "# Apriori algorithm\n",
    "\n",
    "Construct table as described in section 6.2.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f9c689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community 5595:\n",
      "   Total tokens: 8,557,801\n",
      "   Unique tokens: 143,800\n",
      "\n",
      "Community 8821:\n",
      "   Total tokens: 7,075,460\n",
      "   Unique tokens: 151,489\n",
      "\n",
      "Community 6051:\n",
      "   Total tokens: 7,164,056\n",
      "   Unique tokens: 141,186\n",
      "\n",
      "Community 3475:\n",
      "   Total tokens: 4,751,481\n",
      "   Unique tokens: 89,289\n",
      "\n",
      "Community 15981:\n",
      "   Total tokens: 1,253,961\n",
      "   Unique tokens: 60,521\n",
      "\n",
      "Community 530:\n",
      "   Total tokens: 1,292,929\n",
      "   Unique tokens: 32,133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build token statistics for each of the top 6 communities\n",
    "\n",
    "community_token_stats = {}\n",
    "\n",
    "for community_id in top_6_communities:\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == community_id]\n",
    "\n",
    "    # flatten all tokens for this community\n",
    "    all_tokens = []\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    unique_tokens = set(all_tokens)\n",
    "\n",
    "    community_token_stats[community_id] = {\n",
    "        \"n_tokens\": len(all_tokens),\n",
    "        \"n_unique_tokens\": len(unique_tokens),       \n",
    "        \"unique_tokens\": unique_tokens \n",
    "    }\n",
    "\n",
    "# print summary\n",
    "for cid in top_6_communities:\n",
    "    print(f\"Community {cid}:\")\n",
    "    print(f\"   Total tokens: {community_token_stats[cid]['n_tokens']:,}\")\n",
    "    print(f\"   Unique tokens: {community_token_stats[cid]['n_unique_tokens']:,}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f323c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori_tables = {}\n",
    "\n",
    "for cid in top_6_communities:\n",
    "    # get df for this community\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "\n",
    "    # use the unique tokens you already computed\n",
    "    unique_tokens = list(community_token_stats[cid][\"unique_tokens\"])\n",
    "\n",
    "    # apriori base table\n",
    "    df_apriori = pd.DataFrame({\n",
    "        \"word\": unique_tokens,\n",
    "        \"integer\": range(len(unique_tokens))\n",
    "    })\n",
    "\n",
    "    # map word → index\n",
    "    word_to_int = dict(zip(df_apriori[\"word\"], df_apriori[\"integer\"]))\n",
    "\n",
    "    # count posts that contain each token\n",
    "    array_of_counts = np.zeros(len(unique_tokens), dtype=int)\n",
    "\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        for token in set(tokens):              # once per post\n",
    "            array_of_counts[word_to_int[token]] += 1\n",
    "\n",
    "    df_apriori[\"count\"] = array_of_counts\n",
    "\n",
    "    # save in dictionary\n",
    "    apriori_tables[cid] = df_apriori\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "692f4753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Community 5595 ===\n",
      "         word  integer  count  freq_integer\n",
      "12691   women    12691  66748            12\n",
      "82727     get    82727  57474           108\n",
      "29108  people    29108  57189            36\n",
      "30372   would    30372  47288            37\n",
      "38159    even    38159  43302            52\n",
      "143800\n",
      "\n",
      "=== Community 8821 ===\n",
      "         word  integer  count  freq_integer\n",
      "13340   women    13340  52030             8\n",
      "87105     get    87105  51672            88\n",
      "40169    even    40169  39618            42\n",
      "30611  people    30611  38358            28\n",
      "31977   would    31977  37459            30\n",
      "151489\n",
      "\n",
      "=== Community 6051 ===\n",
      "         word  integer  count  freq_integer\n",
      "12293   women    12293  57228            12\n",
      "81125     get    81125  50986           109\n",
      "28442  people    28442  44169            36\n",
      "29738   would    29738  37832            37\n",
      "37419    even    37419  35930            51\n",
      "141186\n",
      "\n",
      "=== Community 3475 ===\n",
      "         word  integer  count  freq_integer\n",
      "51284     get    51284  37196           103\n",
      "7741    women     7741  36081             9\n",
      "18665   would    18665  28191            34\n",
      "17853  people    17853  27673            33\n",
      "23556    even    23556  27238            48\n",
      "89289\n",
      "\n",
      "=== Community 15981 ===\n",
      "         word  integer  count  freq_integer\n",
      "8978      get     8978   8248            10\n",
      "10665   women    10665   7372            11\n",
      "31967    even    31967   6960            52\n",
      "25362   would    25362   6044            39\n",
      "24326  people    24326   5825            35\n",
      "60521\n",
      "\n",
      "=== Community 530 ===\n",
      "         word  integer  count  freq_integer\n",
      "12937  people    12937  10210           206\n",
      "14256   think    14256   7329           228\n",
      "5799    women     5799   7196            81\n",
      "4858      get     4858   6904            73\n",
      "18348    know    18348   6351           298\n",
      "32133\n"
     ]
    }
   ],
   "source": [
    "for cid, df_apriori in apriori_tables.items():\n",
    "    print(f\"\\n=== Community {cid} ===\")\n",
    "    print(df_apriori.sort_values(by=\"count\", ascending=False).head(5))\n",
    "    print(len(df_apriori))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2d554ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 5595 frequent items:\n",
      "         word  integer  count  freq_integer\n",
      "874      long      874   9045             1\n",
      "1915      try     1915  12994             2\n",
      "4129      bad     4129  13740             3\n",
      "4431     read     4431   8070             4\n",
      "7145     laid     7145   7010             5\n",
      "...       ...      ...    ...           ...\n",
      "138421   hard   138421   9365           171\n",
      "139573   look   139573  21564           172\n",
      "141676  makes   141676  11392           173\n",
      "142634  chads   142634   7671           174\n",
      "143537   talk   143537   9282           175\n",
      "\n",
      "[175 rows x 4 columns]\n",
      "\n",
      "Community 8821 frequent items:\n",
      "         word  integer  count  freq_integer\n",
      "936      long      936   7406             1\n",
      "2022      try     2022   9843             2\n",
      "4274      bad     4274  11720             3\n",
      "8721      one     8721  33064             4\n",
      "10348   since    10348   7375             5\n",
      "...       ...      ...    ...           ...\n",
      "145822   hard   145822   8530           141\n",
      "147078   look   147078  21036           142\n",
      "149263  makes   149263   9191           143\n",
      "150292  chads   150292   7810           144\n",
      "151202   talk   151202   6837           145\n",
      "\n",
      "[145 rows x 4 columns]\n",
      "\n",
      "Community 6051 frequent items:\n",
      "         word  integer  count  freq_integer\n",
      "885      long      885   7434             1\n",
      "1854      try     1854  10708             2\n",
      "3891      bad     3891  11492             3\n",
      "4181     read     4181   6160             4\n",
      "6807     laid     6807   5739             5\n",
      "...       ...      ...    ...           ...\n",
      "135910   hard   135910   8016           173\n",
      "137027   look   137027  18518           174\n",
      "139083  makes   139083   9293           175\n",
      "140028  chads   140028   6699           176\n",
      "140929   talk   140929   7522           177\n",
      "\n",
      "[177 rows x 4 columns]\n",
      "\n",
      "Community 3475 frequent items:\n",
      "        word  integer  count  freq_integer\n",
      "539     long      539   5374             1\n",
      "1169     try     1169   7082             2\n",
      "2505     bad     2505   7882             3\n",
      "5084     one     5084  23786             4\n",
      "6012   since     6012   5353             5\n",
      "...      ...      ...    ...           ...\n",
      "85927   hard    85927   5574           164\n",
      "86668   look    86668  13059           165\n",
      "87978  makes    87978   6656           166\n",
      "88563  chads    88563   6054           167\n",
      "89136   talk    89136   5107           168\n",
      "\n",
      "[168 rows x 4 columns]\n",
      "\n",
      "Community 15981 frequent items:\n",
      "         word  integer  count  freq_integer\n",
      "1124     high     1124   1940             1\n",
      "1227      men     1227   4166             2\n",
      "1630      try     1630   1581             3\n",
      "3126      way     3126   2924             4\n",
      "3456      bad     3456   1916             5\n",
      "...       ...      ...    ...           ...\n",
      "56934    look    56934   3257            90\n",
      "57212   going    57212   2302            91\n",
      "58221   thing    58221   2275            92\n",
      "58717   makes    58717   1596            93\n",
      "58763  better    58763   2398            94\n",
      "\n",
      "[94 rows x 4 columns]\n",
      "\n",
      "Community 530 frequent items:\n",
      "              word  integer  count  freq_integer\n",
      "38           young       38    656             1\n",
      "105           used      105    968             2\n",
      "113       question      113    959             3\n",
      "120         family      120    635             4\n",
      "256     physically      256    383             5\n",
      "...            ...      ...    ...           ...\n",
      "31991       coming    31991    480           548\n",
      "32012        seems    32012   1257           549\n",
      "32021        bring    32021    417           550\n",
      "32024         talk    32024   2081           551\n",
      "32116  comfortable    32116    468           552\n",
      "\n",
      "[552 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# dictionary to store frequent-item tables\n",
    "freq_tables = {}\n",
    "\n",
    "threshold_factor = 0.01  # 1% of documents (chapter 6, between passes of A-priori)\n",
    "\n",
    "for cid, df_apriori in apriori_tables.items():\n",
    "\n",
    "    # threshold is 1% of posts in that community\n",
    "    threshold = threshold_factor * len(df_filtered[df_filtered[\"community\"] == cid])\n",
    "\n",
    "    # initialize frequent-item map: old_id -> new_id (0 if not frequent)\n",
    "    frequent_map = np.zeros(len(df_apriori), dtype=int)\n",
    "    new_id = 1\n",
    "\n",
    "    for old_id, count in enumerate(df_apriori['count']):\n",
    "        if count >= threshold:\n",
    "            frequent_map[old_id] = new_id\n",
    "            new_id += 1\n",
    "        else:\n",
    "            frequent_map[old_id] = 0\n",
    "\n",
    "    # add freq_integer column\n",
    "    df_apriori['freq_integer'] = frequent_map\n",
    "\n",
    "    # store only frequent items in new dictionary\n",
    "    df_freq = df_apriori[df_apriori['freq_integer'] != 0].copy()\n",
    "    freq_tables[cid] = df_freq\n",
    "\n",
    "    print(f\"\\nCommunity {cid} frequent items:\")\n",
    "    print(df_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cab3263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 5595 – Frequent Item Pairs:\n",
      "             item_set  count\n",
      "924      (men, women)  18756\n",
      "398      (get, women)  13002\n",
      "2347  (people, women)  11776\n",
      "117     (get, people)  11767\n",
      "964   (people, think)  10718\n",
      "15225\n",
      "\n",
      "Community 8821 – Frequent Item Pairs:\n",
      "           item_set  count\n",
      "456    (men, women)  14642\n",
      "1664   (get, women)   9686\n",
      "155     (even, get)   8414\n",
      "1216  (even, women)   8052\n",
      "651   (get, people)   7747\n",
      "10440\n",
      "\n",
      "Community 6051 – Frequent Item Pairs:\n",
      "             item_set  count\n",
      "23       (men, women)  16316\n",
      "465      (get, women)  11768\n",
      "462     (get, people)   9754\n",
      "91    (people, women)   9333\n",
      "1131    (even, women)   8868\n",
      "15576\n",
      "\n",
      "Community 3475 – Frequent Item Pairs:\n",
      "           item_set  count\n",
      "2920   (men, women)   9654\n",
      "10     (get, women)   7168\n",
      "2006    (even, get)   6147\n",
      "333   (get, people)   5947\n",
      "4152  (even, women)   5676\n",
      "14028\n",
      "\n",
      "Community 15981 – Frequent Item Pairs:\n",
      "          item_set  count\n",
      "6     (men, women)   1952\n",
      "556    (even, get)   1542\n",
      "2     (get, women)   1448\n",
      "706  (even, women)   1242\n",
      "227     (get, one)   1177\n",
      "4371\n",
      "\n",
      "Community 530 – Frequent Item Pairs:\n",
      "             item_set  count\n",
      "1053  (people, think)   3544\n",
      "2250    (get, people)   3329\n",
      "7836   (know, people)   2971\n",
      "1668    (one, people)   2840\n",
      "1062   (people, want)   2784\n",
      "152075\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "pair_tables = {}   # store results for each community\n",
    "\n",
    "for cid in top_6_communities:\n",
    "\n",
    "    # --- get relevant data for the community ---\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "\n",
    "    # table with only frequent tokens\n",
    "    df_freq = freq_tables[cid]\n",
    "\n",
    "    # set of frequent words\n",
    "    frequent_words_set = set(df_freq[\"word\"])\n",
    "\n",
    "    # counter for accumulating all pairs\n",
    "    pair_counter = Counter()\n",
    "\n",
    "    # --- iterate over all posts in that community ---\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        # keep only frequent words in this document\n",
    "        frequent_tokens = [t for t in tokens if t in frequent_words_set]\n",
    "\n",
    "        # remove duplicates\n",
    "        unique_tokens = set(frequent_tokens)\n",
    "\n",
    "        # generate all 2-item sets\n",
    "        for pair in combinations(unique_tokens, 2):\n",
    "            pair_counter[tuple(sorted(pair))] += 1\n",
    "\n",
    "    # convert to dataframe\n",
    "    df_pairs = pd.DataFrame(\n",
    "        pair_counter.items(), columns=[\"item_set\", \"count\"]\n",
    "    )\n",
    "\n",
    "    pair_tables[cid] = df_pairs\n",
    "\n",
    "    print(f\"\\nCommunity {cid} – Frequent Item Pairs:\")\n",
    "    print(df_pairs.sort_values(by=\"count\",ascending = False).head())\n",
    "    print(len(df_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b957c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Community 5595 ---\n",
      "Top pairs:\n",
      "             item_set  count\n",
      "924      (men, women)  18756\n",
      "398      (get, women)  13002\n",
      "2347  (people, women)  11776\n",
      "117     (get, people)  11767\n",
      "964   (people, think)  10718\n",
      "Total surviving pairs: 38\n",
      "\n",
      "--- Community 8821 ---\n",
      "Top pairs:\n",
      "           item_set  count\n",
      "456    (men, women)  14642\n",
      "1664   (get, women)   9686\n",
      "155     (even, get)   8414\n",
      "1216  (even, women)   8052\n",
      "651   (get, people)   7747\n",
      "Total surviving pairs: 10\n",
      "\n",
      "--- Community 6051 ---\n",
      "Top pairs:\n",
      "             item_set  count\n",
      "23       (men, women)  16316\n",
      "465      (get, women)  11768\n",
      "462     (get, people)   9754\n",
      "91    (people, women)   9333\n",
      "1131    (even, women)   8868\n",
      "Total surviving pairs: 33\n",
      "\n",
      "--- Community 3475 ---\n",
      "Top pairs:\n",
      "           item_set  count\n",
      "2920   (men, women)   9654\n",
      "10     (get, women)   7168\n",
      "2006    (even, get)   6147\n",
      "333   (get, people)   5947\n",
      "4152  (even, women)   5676\n",
      "Total surviving pairs: 22\n",
      "\n",
      "--- Community 15981 ---\n",
      "Top pairs:\n",
      "       item_set  count\n",
      "6  (men, women)   1952\n",
      "Total surviving pairs: 1\n",
      "\n",
      "--- Community 530 ---\n",
      "Top pairs:\n",
      "             item_set  count\n",
      "1053  (people, think)   3544\n",
      "2250    (get, people)   3329\n",
      "7836   (know, people)   2971\n",
      "1668    (one, people)   2840\n",
      "1062   (people, want)   2784\n",
      "Total surviving pairs: 4171\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "pair_tables = {}   # store results for each community\n",
    "\n",
    "for cid in top_6_communities:\n",
    "\n",
    "    print(f\"\\n--- Community {cid} ---\")\n",
    "\n",
    "    # --- get relevant data for the community ---\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "    N = len(df_comm)\n",
    "\n",
    "    # table with only frequent tokens\n",
    "    df_freq = freq_tables[cid]\n",
    "\n",
    "    # set of frequent words\n",
    "    frequent_words_set = set(df_freq[\"word\"])\n",
    "\n",
    "    # counter for accumulating all pairs\n",
    "    pair_counter = Counter()\n",
    "\n",
    "    # --- iterate over all posts in that community ---\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        # keep only frequent words in this document\n",
    "        frequent_tokens = [t for t in tokens if t in frequent_words_set]\n",
    "\n",
    "        # remove duplicates\n",
    "        unique_tokens = set(frequent_tokens)\n",
    "\n",
    "        # generate all 2-item sets\n",
    "        for pair in combinations(unique_tokens, 2):\n",
    "            pair_counter[tuple(sorted(pair))] += 1\n",
    "\n",
    "    # convert to dataframe\n",
    "    df_pairs = pd.DataFrame(pair_counter.items(), columns=[\"item_set\", \"count\"])\n",
    "\n",
    "    # --- apply the 1% threshold for pairs ---\n",
    "    threshold = math.ceil(0.01 * N)\n",
    "    df_pairs = df_pairs[df_pairs[\"count\"] >= threshold]\n",
    "\n",
    "    pair_tables[cid] = df_pairs\n",
    "\n",
    "    print(\"Top pairs:\")\n",
    "    print(df_pairs.sort_values(by=\"count\", ascending=False).head())\n",
    "    print(\"Total surviving pairs:\", len(df_pairs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8831e069",
   "metadata": {},
   "source": [
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "pair_tables = {}   # store results for each community\n",
    "\n",
    "for cid in top_6_communities:\n",
    "\n",
    "    # --- get relevant data for the community ---\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "\n",
    "    # table with only frequent tokens\n",
    "    df_freq = freq_tables[cid]\n",
    "\n",
    "    # set of frequent words\n",
    "    frequent_words_set = set(df_freq[\"word\"])\n",
    "\n",
    "    # counter for accumulating all pairs\n",
    "    pair_counter = Counter()\n",
    "\n",
    "    # --- iterate over all posts in that community ---\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        # keep only frequent words in this document\n",
    "        frequent_tokens = [t for t in tokens if t in frequent_words_set]\n",
    "\n",
    "        # remove duplicates\n",
    "        unique_tokens = set(frequent_tokens)\n",
    "\n",
    "        # generate all 2-item sets\n",
    "        for pair in combinations(unique_tokens, 3):\n",
    "            pair_counter[tuple(sorted(pair))] += 1\n",
    "\n",
    "    # convert to dataframe\n",
    "    df_pairs = pd.DataFrame(\n",
    "        pair_counter.items(), columns=[\"item_set\", \"count\"]\n",
    "    )\n",
    "\n",
    "    pair_tables[cid] = df_pairs\n",
    "\n",
    "    print(f\"\\nCommunity {cid} – Frequent Item Pairs:\")\n",
    "    print(df_pairs.sort_values(by=\"count\",ascending = False).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aea86c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46b44fc2",
   "metadata": {},
   "source": [
    "# Apriori using library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d19f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d40c26f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "te = TransactionEncoder()\n",
    "\n",
    "freq_words = set(freq_tables[530][\"word\"])\n",
    "\n",
    "df_530_test = [\n",
    "    [t for t in tokens if t in freq_words]\n",
    "    for tokens in df_filtered[df_filtered[\"community\"] == 530][\"tokens\"]\n",
    "]\n",
    "\n",
    "te_array = te.fit(df_530_test).transform(df_530_test)\n",
    "df_encoded = pd.DataFrame(te_array, columns=te.columns_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c436238b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 37371\n",
      "Unique tokens: 552\n"
     ]
    }
   ],
   "source": [
    "print(\"Documents:\", len(df_530_test))\n",
    "print(\"Unique tokens:\", len(te.columns_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b014381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Frequent Itemsets: 12939\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "frequent_itemsets = apriori(df_encoded, min_support=0.01, use_colnames=True)\n",
    "print(\"Total Frequent Itemsets:\", frequent_itemsets.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "862fe6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "support",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "itemsets",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "bb7e5518-28b7-46f1-a603-2f1ad040b057",
       "rows": [
        [
         "0",
         "0.034384950897754944",
         "frozenset({'able'})"
        ],
        [
         "1",
         "0.01862406678975676",
         "frozenset({'absolutely'})"
        ],
        [
         "2",
         "0.017152337373899547",
         "frozenset({'accept'})"
        ],
        [
         "3",
         "0.012201974793288915",
         "frozenset({'act'})"
        ],
        [
         "4",
         "0.01321880602606299",
         "frozenset({'actual'})"
        ],
        [
         "5",
         "0.06491664659762918",
         "frozenset({'actually'})"
        ],
        [
         "6",
         "0.052821706670948064",
         "frozenset({'advice'})"
        ],
        [
         "7",
         "0.023253324770544006",
         "frozenset({'age'})"
        ],
        [
         "8",
         "0.01404832624227342",
         "frozenset({'ago'})"
        ],
        [
         "9",
         "0.027641754301463702",
         "frozenset({'agree'})"
        ],
        [
         "10",
         "0.021514008188167295",
         "frozenset({'almost'})"
        ],
        [
         "11",
         "0.025046158786224612",
         "frozenset({'alone'})"
        ],
        [
         "12",
         "0.010944315110647293",
         "frozenset({'along'})"
        ],
        [
         "13",
         "0.03500040138074978",
         "frozenset({'already'})"
        ],
        [
         "14",
         "0.11851435605148378",
         "frozenset({'also'})"
        ],
        [
         "15",
         "0.05624682240239758",
         "frozenset({'always'})"
        ],
        [
         "16",
         "0.011827352760161622",
         "frozenset({'amount'})"
        ],
        [
         "17",
         "0.03746220331272912",
         "frozenset({'another'})"
        ],
        [
         "18",
         "0.021567525621471195",
         "frozenset({'answer'})"
        ],
        [
         "19",
         "0.016510128174252764",
         "frozenset({'anxiety'})"
        ],
        [
         "20",
         "0.012870942709587649",
         "frozenset({'anymore'})"
        ],
        [
         "21",
         "0.04848679457333226",
         "frozenset({'anyone'})"
        ],
        [
         "22",
         "0.06502368146423698",
         "frozenset({'anything'})"
        ],
        [
         "23",
         "0.01881137780632041",
         "frozenset({'anyway'})"
        ],
        [
         "24",
         "0.011024591260603142",
         "frozenset({'appearance'})"
        ],
        [
         "25",
         "0.014690535441920205",
         "frozenset({'approach'})"
        ],
        [
         "26",
         "0.010676727944127799",
         "frozenset({'apps'})"
        ],
        [
         "27",
         "0.0533301222873351",
         "frozenset({'around'})"
        ],
        [
         "28",
         "0.038452275828851246",
         "frozenset({'ask'})"
        ],
        [
         "29",
         "0.018195927323325572",
         "frozenset({'asked'})"
        ],
        [
         "30",
         "0.023574429370367397",
         "frozenset({'asking'})"
        ],
        [
         "31",
         "0.012870942709587649",
         "frozenset({'assume'})"
        ],
        [
         "32",
         "0.014904605175135801",
         "frozenset({'attention'})"
        ],
        [
         "33",
         "0.02344063578710765",
         "frozenset({'attracted'})"
        ],
        [
         "34",
         "0.016857991490728103",
         "frozenset({'attraction'})"
        ],
        [
         "35",
         "0.05260763693773247",
         "frozenset({'attractive'})"
        ],
        [
         "36",
         "0.021835112787990687",
         "frozenset({'average'})"
        ],
        [
         "37",
         "0.011425972010382382",
         "frozenset({'avoid'})"
        ],
        [
         "38",
         "0.031655561799256104",
         "frozenset({'away'})"
        ],
        [
         "39",
         "0.010141553611088813",
         "frozenset({'awkward'})"
        ],
        [
         "40",
         "0.04880789917315566",
         "frozenset({'back'})"
        ],
        [
         "41",
         "0.055470819619491046",
         "frozenset({'bad'})"
        ],
        [
         "42",
         "0.021835112787990687",
         "frozenset({'based'})"
        ],
        [
         "43",
         "0.019935243905702282",
         "frozenset({'basically'})"
        ],
        [
         "44",
         "0.02427015600331808",
         "frozenset({'become'})"
        ],
        [
         "45",
         "0.010168312327740762",
         "frozenset({'behavior'})"
        ],
        [
         "46",
         "0.010034518744481014",
         "frozenset({'behind'})"
        ],
        [
         "47",
         "0.04008455754462016",
         "frozenset({'believe'})"
        ],
        [
         "48",
         "0.04701506515747505",
         "frozenset({'best'})"
        ],
        [
         "49",
         "0.07832276364025581",
         "frozenset({'better'})"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 12939
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034385</td>\n",
       "      <td>(able)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.018624</td>\n",
       "      <td>(absolutely)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.017152</td>\n",
       "      <td>(accept)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.012202</td>\n",
       "      <td>(act)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013219</td>\n",
       "      <td>(actual)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12934</th>\n",
       "      <td>0.010811</td>\n",
       "      <td>(think, get, want, people, way)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12935</th>\n",
       "      <td>0.010356</td>\n",
       "      <td>(think, get, want, people, would)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12936</th>\n",
       "      <td>0.010463</td>\n",
       "      <td>(know, think, one, want, people)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12937</th>\n",
       "      <td>0.010623</td>\n",
       "      <td>(know, think, one, people, way)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12938</th>\n",
       "      <td>0.010998</td>\n",
       "      <td>(know, think, one, people, would)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12939 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        support                           itemsets\n",
       "0      0.034385                             (able)\n",
       "1      0.018624                       (absolutely)\n",
       "2      0.017152                           (accept)\n",
       "3      0.012202                              (act)\n",
       "4      0.013219                           (actual)\n",
       "...         ...                                ...\n",
       "12934  0.010811    (think, get, want, people, way)\n",
       "12935  0.010356  (think, get, want, people, would)\n",
       "12936  0.010463   (know, think, one, want, people)\n",
       "12937  0.010623    (know, think, one, people, way)\n",
       "12938  0.010998  (know, think, one, people, would)\n",
       "\n",
       "[12939 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ac9b3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent 2-itemsets: 4171\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "frequent_itemsets = apriori(df_encoded, min_support=0.01, use_colnames=True)\n",
    "frequent_2_itemsets = frequent_itemsets[\n",
    "    frequent_itemsets['itemsets'].apply(lambda x: len(x) == 2)\n",
    "]\n",
    "\n",
    "print(\"Frequent 2-itemsets:\", len(frequent_2_itemsets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf94d54d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
