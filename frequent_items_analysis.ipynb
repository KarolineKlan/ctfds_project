{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ea257d",
   "metadata": {},
   "source": [
    "# Frequent items analysis of reddit communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f4c2c536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/livdreyerjohansen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import networkx as nx\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "import html\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import math\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88d1fd7",
   "metadata": {},
   "source": [
    "## Dataload and cleaning\n",
    "\n",
    "To analysize the what frequent items we may see in the reddit communities found in 03_NetworkAnalysis.ipynb, we must first load the graph with the added attributes, that tell what community each node belongs in and the posts created by each node. We have chosen to only look at the 6 largest communities, by number of nodes, as the distribution of nodes / community is very heavly right skewed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cee6b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load graph\n",
    "\n",
    "G = nx.read_gml('FINAL_reddit_graph_with_louvain_communities.gml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de689bc",
   "metadata": {},
   "source": [
    "To ensure we do not include posts that are either deleted (\"\\[deleted\\]\") or removed (\"\\[removed\\]\"), both basic reddit features that happen independently of what forum you are in, we remove both. Furthermore, we remove each post that was removed by a bot, which is clear in the text which the bot uses to explain why a post or comment is deleted. We then construct a dataframe with all posts and their community (and original poster (OP in reddit linguistics))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "03a8126f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id                                               text  community\n",
      "0  1  \"Huh it's still not legalized yet. America is ...        130\n",
      "1  1  \"Hey charisma helps. Everybody wants to sleep ...        130\n",
      "2  1  Aren't the jedis not really good guys though? ...        130\n",
      "3  1  Wait but ferb is the better looking one with a...        130\n",
      "4  1  Great now you live with a hole in your head fo...        130\n",
      "\n",
      "Number of original posts:\n",
      "2,664,156\n",
      "Number of removed posts:\n",
      "158,251\n",
      "Number of posts in dataframe:\n",
      "2,505,905\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "rows_count = 0\n",
    "allowed_rows = 0\n",
    "\n",
    "for node, data in G.nodes(data=True):\n",
    "    community = data.get(\"community\")\n",
    "    posts_dict = data.get(\"posts\", {})\n",
    "\n",
    "    # Ensure it's a dictionary\n",
    "    if not isinstance(posts_dict, dict):\n",
    "        posts_dict = {\"default\": posts_dict}\n",
    "\n",
    "    # Loop through each list of posts in the dictionary\n",
    "    for key, posts in posts_dict.items():\n",
    "        if not isinstance(posts, list):\n",
    "            posts = [posts]\n",
    "\n",
    "        for post in posts:\n",
    "            rows_count += 1\n",
    "            # Skip empty, deleted/removed posts, or posts containing the bot line\n",
    "            if post and post not in ['[deleted]', '[removed]'] and \\\n",
    "               \"*I am a bot, and this action was performed automatically.\" not in post:\n",
    "                allowed_rows += 1\n",
    "                rows.append({\n",
    "                    \"id\": node,\n",
    "                    \"text\": post,\n",
    "                    \"community\": community\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nNumber of original posts:\")\n",
    "print(f\"{rows_count:,}\")\n",
    "print(\"Number of removed posts:\")\n",
    "print(f\"{rows_count-allowed_rows:,}\")\n",
    "print(\"Number of posts in dataframe:\")\n",
    "print(f\"{allowed_rows:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dbf9c4",
   "metadata": {},
   "source": [
    "We identify the top-6 largest communities in terms of nodes to continue working with only them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8f4063e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = df['community'].unique().tolist()\n",
    "communities_dict = dict.fromkeys(communities, 0)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    communities_dict[row['community']] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f165e4",
   "metadata": {},
   "source": [
    "Filter the dataframe to only contain posts from top-6 communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c521f85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 6 communities by number of posts (posts, community): [(644383, 130), (635828, 89), (630172, 129), (399843, 191), (154109, 220), (38490, 188)]\n",
      "Number of posts in top 6 communities: 2,502,825\n",
      "Number of posts removed based on non identity in top 6: 3,080\n"
     ]
    }
   ],
   "source": [
    "comms_list = list(sorted( ((v,k) for k,v in communities_dict.items()), reverse=True))\n",
    "comms_list = comms_list[:6]\n",
    "top_6_communities = [item[1] for item in comms_list]\n",
    "\n",
    "df_filtered = df[df['community'].isin(top_6_communities)].copy()\n",
    "\n",
    "print(\"\\nTop 6 communities by number of posts (posts, community):\", comms_list)\n",
    "\n",
    "print(f\"Number of posts in top 6 communities: {len(df_filtered):,}\")\n",
    "\n",
    "print(f\"Number of posts removed based on non identity in top 6: {(len(df) - len(df_filtered)):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1289c493",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "We filter out parts of post that we deem have little semantic value. We aim to find frequent items and frequent itemsets (item pairs), and would assume that stop words regularly occur in more than 1% of baskets. As we are working with online fora, we chose to add certain slang-terms as stop words. We furthermore remove:\n",
    "\n",
    "- html entities\n",
    "- URL's\n",
    "- non-text artifacts (such as \"/\", \"?\", \"!\" etc.)\n",
    "- remaining \"removed\" and \"deleted\" artifacts that were not removed in the previous code block due to the way the post was loaded\n",
    "- short words (length of 2 or less)\n",
    "\n",
    "Additionally, we make all words lowercase to steamline and tokenize by word (meaning each word will be its own token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "422cbe8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "community",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tokens",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "n_tokens",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "5115cf6a-9f5e-483d-8d74-40e674e43770",
       "rows": [
        [
         "0",
         "1",
         "\"Huh it's still not legalized yet. America is weirdly antiquated\" ",
         "130",
         "['huh', 'still', 'legalized', 'yet', 'america', 'weirdly', 'antiquated']",
         "7"
        ],
        [
         "1",
         "1",
         "\"Hey charisma helps. Everybody wants to sleep with pretty people. Doesn't mean they enjoy dating them\" ",
         "130",
         "['hey', 'charisma', 'helps', 'everybody', 'wants', 'sleep', 'pretty', 'people', 'mean', 'enjoy', 'dating']",
         "11"
        ],
        [
         "2",
         "1",
         "Aren't the jedis not really good guys though? Like they protect the status quo. That and how they serve the same cosmic deity that doesn't care about anything and can't be bothered by which of its \"\"sides\"\" its pathetic worshippers venerate? Idk much about star wars sorry if I got it wrong ",
         "130",
         "['jedis', 'really', 'good', 'guys', 'though', 'protect', 'status', 'quo', 'serve', 'cosmic', 'deity', 'care', 'anything', 'bothered', 'sides', 'pathetic', 'worshippers', 'venerate', 'much', 'star', 'wars', 'sorry', 'got', 'wrong']",
         "24"
        ],
        [
         "3",
         "1",
         "Wait but ferb is the better looking one with actual game. Phineas is a fucking geometry figure for god sakes ",
         "130",
         "['wait', 'ferb', 'better', 'looking', 'one', 'actual', 'game', 'phineas', 'fucking', 'geometry', 'figure', 'god', 'sakes']",
         "13"
        ],
        [
         "4",
         "1",
         "Great now you live with a hole in your head for eternity ",
         "130",
         "['great', 'live', 'hole', 'head', 'eternity']",
         "5"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>community</th>\n",
       "      <th>tokens</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Huh it's still not legalized yet. America is ...</td>\n",
       "      <td>130</td>\n",
       "      <td>[huh, still, legalized, yet, america, weirdly,...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Hey charisma helps. Everybody wants to sleep ...</td>\n",
       "      <td>130</td>\n",
       "      <td>[hey, charisma, helps, everybody, wants, sleep...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Aren't the jedis not really good guys though? ...</td>\n",
       "      <td>130</td>\n",
       "      <td>[jedis, really, good, guys, though, protect, s...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Wait but ferb is the better looking one with a...</td>\n",
       "      <td>130</td>\n",
       "      <td>[wait, ferb, better, looking, one, actual, gam...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Great now you live with a hole in your head fo...</td>\n",
       "      <td>130</td>\n",
       "      <td>[great, live, hole, head, eternity]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                               text  community  \\\n",
       "0  1  \"Huh it's still not legalized yet. America is ...        130   \n",
       "1  1  \"Hey charisma helps. Everybody wants to sleep ...        130   \n",
       "2  1  Aren't the jedis not really good guys though? ...        130   \n",
       "3  1  Wait but ferb is the better looking one with a...        130   \n",
       "4  1  Great now you live with a hole in your head fo...        130   \n",
       "\n",
       "                                              tokens  n_tokens  \n",
       "0  [huh, still, legalized, yet, america, weirdly,...         7  \n",
       "1  [hey, charisma, helps, everybody, wants, sleep...        11  \n",
       "2  [jedis, really, good, guys, though, protect, s...        24  \n",
       "3  [wait, ferb, better, looking, one, actual, gam...        13  \n",
       "4                [great, live, hole, head, eternity]         5  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize and clean text data\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# extend basic english stopwords with slang terms\n",
    "extra_stops = {\n",
    "    'lol', 'xd', 'haha', 'hahaah', 'omg', 'u', 'ur', 'im', 'ive', 'idk', \n",
    "    'dont', 'cant', 'wont', 'aint', 'ya', 'tho', 'tho', 'nah', 'btw', \n",
    "    'like', 'yeah', 'yep', 'ok', 'okay', 'pls', 'please', 'get'\n",
    "}\n",
    "stop_words.update(extra_stops)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # decode HTML entities: &amp; → &, &#x200B; → zero-width space, etc.\n",
    "    text = html.unescape(text)\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
    "    # keep only letters and spaces\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    # tokenize by whitespace\n",
    "    tokens = text.split()\n",
    "    # remove stopwords and very short tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    if len(tokens) == 1 and tokens[0] in {\"removed\", \"deleted\"}:\n",
    "        return []\n",
    "    return tokens\n",
    "\n",
    "df_filtered[\"tokens\"] = df_filtered[\"text\"].apply(preprocess_text)\n",
    "df_filtered[\"n_tokens\"] = df_filtered[\"tokens\"].apply(len)\n",
    "\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4075dcf8",
   "metadata": {},
   "source": [
    "# Frequent items and the A-priori algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27146132",
   "metadata": {},
   "source": [
    "We process the tokenized posts by identifying the amount of unique tokens for each community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f4f9c689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community 130:\n",
      "  Total tokens: 8,159,749\n",
      "  Unique tokens: 140,319\n",
      "Community 89:\n",
      "  Total tokens: 7,721,869\n",
      "  Unique tokens: 149,731\n",
      "Community 129:\n",
      "  Total tokens: 6,700,663\n",
      "  Unique tokens: 146,730\n",
      "Community 191:\n",
      "  Total tokens: 4,709,952\n",
      "  Unique tokens: 89,034\n",
      "Community 220:\n",
      "  Total tokens: 1,226,959\n",
      "  Unique tokens: 59,693\n",
      "Community 188:\n",
      "  Total tokens: 1,305,336\n",
      "  Unique tokens: 33,100\n"
     ]
    }
   ],
   "source": [
    "# build token statistics for each of the top 6 communities\n",
    "\n",
    "community_token_stats = {}\n",
    "\n",
    "for community_id in top_6_communities:\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == community_id]\n",
    "\n",
    "    # flatten all tokens for this community\n",
    "    all_tokens = []\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    unique_tokens = set(all_tokens)\n",
    "\n",
    "    community_token_stats[community_id] = {\n",
    "        \"n_tokens\": len(all_tokens),\n",
    "        \"n_unique_tokens\": len(unique_tokens),       \n",
    "        \"unique_tokens\": unique_tokens \n",
    "    }\n",
    "\n",
    "\n",
    "for cid in top_6_communities:\n",
    "    print(f\"Community {cid}:\")\n",
    "    print(f\"  Total tokens: {community_token_stats[cid]['n_tokens']:,}\")\n",
    "    print(f\"  Unique tokens: {community_token_stats[cid]['n_unique_tokens']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eca728",
   "metadata": {},
   "source": [
    "## First pass of the A-priori algorithm\n",
    "\n",
    "In the first pass of the A-priori algoritm, we initialize a dataframe for each of the communities. In this dataframe, we will store each of the unique tokens found previously, assign them each an integer from 0 to n-1 (number of unique tokens), and count how many baskets (posts) the item (token) appears in. It is important to note that we do not count the total occurrence of the token but only the amount of posts it appears in. In Mining of Massive Datasets, Section 6.2.2, the first pass is described as labeling integers 1 to n, but to keep it within the python framework, we label 0 to n-1 as mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8f323c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori_tables = {}\n",
    "\n",
    "for cid in top_6_communities:\n",
    "    # get df for this community\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "    unique_tokens = list(community_token_stats[cid][\"unique_tokens\"])\n",
    "\n",
    "    # apriori table\n",
    "    df_apriori = pd.DataFrame({\n",
    "        \"word\": unique_tokens,\n",
    "        \"integer\": range(len(unique_tokens))\n",
    "    })\n",
    "\n",
    "    # give each word an integer from 0 to n-1\n",
    "    word_to_int = dict(zip(df_apriori[\"word\"], df_apriori[\"integer\"]))\n",
    "\n",
    "    # count posts that contain each token\n",
    "    array_of_counts = np.zeros(len(unique_tokens), dtype=int)\n",
    "\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        for token in set(tokens):             \n",
    "            array_of_counts[word_to_int[token]] += 1\n",
    "\n",
    "    df_apriori[\"count\"] = array_of_counts\n",
    "\n",
    "    # save in dict\n",
    "    apriori_tables[cid] = df_apriori\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "692f4753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 130\n",
      "          word  integer  count\n",
      "44223    women    44223  64054\n",
      "134753  people   134753  54841\n",
      "137669   would   137669  45478\n",
      "98666     even    98666  41610\n",
      "108766     one   108766  40846\n",
      "\n",
      "Community 89\n",
      "          word  integer  count\n",
      "46904    women    46904  61740\n",
      "143858  people   143858  47917\n",
      "146995   would   146995  41218\n",
      "105327    even   105327  39135\n",
      "116017     one   116017  38397\n",
      "\n",
      "Community 129\n",
      "          word  integer  count\n",
      "46114    women    46114  49985\n",
      "102972    even   102972  37953\n",
      "140852  people   140852  36781\n",
      "144012   would   144012  35755\n",
      "113479     one   113479  31571\n",
      "\n",
      "Community 191\n",
      "         word  integer  count\n",
      "28002   women    28002  36120\n",
      "87398   would    87398  28225\n",
      "85469  people    85469  27657\n",
      "62612    even    62612  27309\n",
      "68953     one    68953  23836\n",
      "\n",
      "Community 220\n",
      "         word  integer  count\n",
      "37702   women    37702   7367\n",
      "24451    even    24451   6899\n",
      "57473   would    57473   5976\n",
      "54903  people    54903   5766\n",
      "32958     one    32958   5658\n",
      "\n",
      "Community 188\n",
      "         word  integer  count\n",
      "30464  people    30464  10376\n",
      "9871    think     9871   7425\n",
      "20876   women    20876   7331\n",
      "21283    know    21283   6431\n",
      "31891   would    31891   6056\n"
     ]
    }
   ],
   "source": [
    "# print top 5 tokens by count for each community\n",
    "for cid, df_apriori in apriori_tables.items():\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "    print(df_apriori.sort_values(by=\"count\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5646a40",
   "metadata": {},
   "source": [
    "## Between the passes of A-priori\n",
    "\n",
    "We create frequency tables where we assign each word an integer from 1-m, where m = number of frequent singletons (words), if the support of the word => 1%. In other words, it must appear in 1% or more of the baskets. If the word is not frequent, we assign it 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d2d554ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 130 frequent items:\n",
      "              word  integer  count  freq_integer\n",
      "159          place      159   7391             1\n",
      "600           used      600   6884             2\n",
      "1597           end     1597   6460             3\n",
      "3313    attractive     3313  13017             4\n",
      "3731         looks     3731  18848             5\n",
      "...            ...      ...    ...           ...\n",
      "136958        also   136958  19237           172\n",
      "137669       would   137669  45478           173\n",
      "137780         sub   137780  16094           174\n",
      "139254         guy   139254  21330           175\n",
      "139806        shit   139806  24811           176\n",
      "\n",
      "[176 rows x 4 columns]\n",
      "\n",
      "Community 89 frequent items:\n",
      "              word  integer  count  freq_integer\n",
      "167          place      167   6568             1\n",
      "626           used      626   6731             2\n",
      "1762           end     1762   6531             3\n",
      "3531    attractive     3531  11692             4\n",
      "3980         looks     3980  18198             5\n",
      "...            ...      ...    ...           ...\n",
      "146224        also   146224  17903           166\n",
      "146995       would   146995  41218           167\n",
      "147095         sub   147095  13349           168\n",
      "148613         guy   148613  21598           169\n",
      "149193        shit   149193  24519           170\n",
      "\n",
      "[170 rows x 4 columns]\n",
      "\n",
      "Community 129 frequent items:\n",
      "              word  integer  count  freq_integer\n",
      "3418    attractive     3418   9800             1\n",
      "3875         looks     3875  16721             2\n",
      "4221          hate     4221   8793             3\n",
      "4443          best     4443   7127             4\n",
      "4451        pretty     4451  11029             5\n",
      "...            ...      ...    ...           ...\n",
      "143242        also   143242  15788           140\n",
      "144012       would   144012  35755           141\n",
      "144113         sub   144113  12566           142\n",
      "145571         guy   145571  20376           143\n",
      "146166        shit   146166  24806           144\n",
      "\n",
      "[144 rows x 4 columns]\n",
      "\n",
      "Community 191 frequent items:\n",
      "             word  integer  count  freq_integer\n",
      "90          place       90   4313             1\n",
      "384          used      384   4664             2\n",
      "1057          end     1057   4637             3\n",
      "2140   attractive     2140   7509             4\n",
      "2406        looks     2406  11956             5\n",
      "...           ...      ...    ...           ...\n",
      "86915        also    86915  12061           163\n",
      "87398       would    87398  28225           164\n",
      "87461         sub    87461  10322           165\n",
      "88362         guy    88362  14053           166\n",
      "88717        shit    88717  16942           167\n",
      "\n",
      "[167 rows x 4 columns]\n",
      "\n",
      "Community 220 frequent items:\n",
      "          word  integer  count  freq_integer\n",
      "3154     looks     3154   2742             1\n",
      "3431      hate     3431   1771             2\n",
      "3635    pretty     3635   1584             3\n",
      "4208   nothing     4208   1816             4\n",
      "4782      high     4782   1921             5\n",
      "...        ...      ...    ...           ...\n",
      "57553      sub    57553   1946            89\n",
      "58693      bad    58693   1891            90\n",
      "58762      guy    58762   3124            91\n",
      "58790    could    58790   2557            92\n",
      "59257     shit    59257   4674            93\n",
      "\n",
      "[93 rows x 4 columns]\n",
      "\n",
      "Community 188 frequent items:\n",
      "             word  integer  count  freq_integer\n",
      "43         become       43    920             1\n",
      "66         second       66    564             2\n",
      "70          place       70   1221             3\n",
      "132        honest      132    567             4\n",
      "265         asked      265    685             5\n",
      "...           ...      ...    ...           ...\n",
      "32854        shit    32854   1430           538\n",
      "32900        sort    32900    673           539\n",
      "32944   sometimes    32944   1250           540\n",
      "33005  confidence    33005    782           541\n",
      "33064    everyone    33064   2136           542\n",
      "\n",
      "[542 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "freq_tables = {}\n",
    "\n",
    "threshold_factor = 0.01  \n",
    "\n",
    "for cid, df_apriori in apriori_tables.items():\n",
    "\n",
    "    # threshold is 1% of posts in that community\n",
    "    threshold = threshold_factor * len(df_filtered[df_filtered[\"community\"] == cid])\n",
    "\n",
    "    frequent_map = np.zeros(len(df_apriori), dtype=int)\n",
    "    new_id = 1\n",
    "\n",
    "    for old_id, count in enumerate(df_apriori['count']):\n",
    "        if count >= threshold:\n",
    "            frequent_map[old_id] = new_id\n",
    "            new_id += 1\n",
    "        else:\n",
    "            frequent_map[old_id] = 0\n",
    "\n",
    "    # add freq_integer column\n",
    "    df_apriori['freq_integer'] = frequent_map\n",
    "\n",
    "    # store only frequent items in new dictionary\n",
    "    df_freq = df_apriori[df_apriori['freq_integer'] != 0].copy()\n",
    "    freq_tables[cid] = df_freq\n",
    "\n",
    "    print(f\"\\nCommunity {cid} frequent items:\")\n",
    "    print(df_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d38f5ea",
   "metadata": {},
   "source": [
    "## Second pass of the A-priori algorithm\n",
    "\n",
    "For the second pass, we first find all pairs of frequent words from the previous dataframes. We then create pairs of those, making sure to remove duplicates. We apply the support threshold of 1% here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cab3263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 130\n",
      "Top pairs:\n",
      "             item_set  count\n",
      "881      (men, women)  17992\n",
      "2287  (people, women)  11308\n",
      "919   (people, think)  10248\n",
      "713    (women, would)  10004\n",
      "1307   (think, women)   9892\n",
      "Total pairs where Support(I) => s of 0.01: 30\n",
      "\n",
      "Community 89\n",
      "Top pairs:\n",
      "             item_set  count\n",
      "35       (men, women)  17505\n",
      "127   (people, women)   9955\n",
      "1066    (even, women)   9487\n",
      "223     (want, women)   9398\n",
      "771    (women, would)   9290\n",
      "Total pairs where Support(I) => s of 0.01: 21\n",
      "\n",
      "Community 129\n",
      "Top pairs:\n",
      "             item_set  count\n",
      "505      (men, women)  14147\n",
      "1134    (even, women)   7761\n",
      "2885   (women, would)   6814\n",
      "385   (people, women)   6649\n",
      "1208    (want, women)   6643\n",
      "Total pairs where Support(I) => s of 0.01: 7\n",
      "\n",
      "Community 191\n",
      "Top pairs:\n",
      "             item_set  count\n",
      "2783     (men, women)   9671\n",
      "4021    (even, women)   5711\n",
      "6289   (women, would)   5197\n",
      "2827   (think, women)   4878\n",
      "30    (people, think)   4870\n",
      "Total pairs where Support(I) => s of 0.01: 14\n",
      "\n",
      "Community 220\n",
      "Top pairs:\n",
      "        item_set  count\n",
      "10  (men, women)   1949\n",
      "Total pairs where Support(I) => s of 0.01: 1\n",
      "\n",
      "Community 188\n",
      "Top pairs:\n",
      "             item_set  count\n",
      "1000  (people, think)   3588\n",
      "7726   (know, people)   2995\n",
      "1701    (one, people)   2876\n",
      "991    (people, want)   2812\n",
      "1694   (even, people)   2734\n",
      "Total pairs where Support(I) => s of 0.01: 3827\n"
     ]
    }
   ],
   "source": [
    "pair_tables = {}   # store results for each community\n",
    "\n",
    "for cid in top_6_communities:\n",
    "\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "\n",
    "    # pull posts for this community\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "    N = len(df_comm)\n",
    "\n",
    "    # fetch frequent 1-itemset for this community\n",
    "    df_freq = freq_tables[cid]\n",
    "    frequent_words_set = set(df_freq[\"word\"])\n",
    "\n",
    "    # counter for all frequent pairs\n",
    "    pair_counter = Counter()\n",
    "\n",
    "    # iterate over all posts\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        # keep only frequent tokens\n",
    "        frequent_tokens = [t for t in tokens if t in frequent_words_set]\n",
    "\n",
    "        # deduplicate within a post\n",
    "        unique_tokens = set(frequent_tokens)\n",
    "\n",
    "        # count each 2-item combination in this post\n",
    "        for pair in combinations(unique_tokens, 2):\n",
    "            pair_counter[tuple(sorted(pair))] += 1\n",
    "\n",
    "    # convert counter → dataframe\n",
    "    df_pairs = pd.DataFrame(pair_counter.items(), columns=[\"item_set\", \"count\"])\n",
    "\n",
    "    # threshold for frequent 2-itemsets (1% of posts)\n",
    "    threshold = math.ceil(0.01 * N)\n",
    "    df_pairs = df_pairs[df_pairs[\"count\"] >= threshold]\n",
    "\n",
    "    # store\n",
    "    pair_tables[cid] = df_pairs\n",
    "\n",
    "    # print summary\n",
    "    print(\"Top pairs:\")\n",
    "    print(df_pairs.sort_values(by=\"count\", ascending=False).head())\n",
    "    print(\"Total pairs where Support(I) => s of 0.01:\", len(df_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c2a239",
   "metadata": {},
   "source": [
    "To gain knowledge into the context of the frequent item pairs, we count the occurance of words around our top frequent item pair (men, women). We go through each post that contain both items, identify the tokens close (window of 5 on each side if possible) to the items, count the occurance of a token within the window of our item and sort them. We find that there is a difference in words between the communities even though we did not see one when looking only at the frequency pairs. This exercise (or in future work, and extenstion of this exercise) is furthermore important to obtain a sense of the context of the words. It is not possible from the frequent items analysis done previously to find context clues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68995eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Community 130 ===\n",
      "Posts containing both: 17992\n",
      "\n",
      "Top window-context words for 'women':\n",
      "            count\n",
      "want         2614\n",
      "sex          2192\n",
      "think        2088\n",
      "would        1961\n",
      "attractive   1859\n",
      "\n",
      "Top window-context words for 'men':\n",
      "            count\n",
      "want         1871\n",
      "sex          1825\n",
      "attractive   1735\n",
      "think        1518\n",
      "ugly         1518\n",
      "\n",
      "=== Community 89 ===\n",
      "Posts containing both: 17505\n",
      "\n",
      "Top window-context words for 'women':\n",
      "       count\n",
      "want    2448\n",
      "sex     1953\n",
      "think   1942\n",
      "even    1869\n",
      "would   1819\n",
      "\n",
      "Top window-context words for 'men':\n",
      "            count\n",
      "want         1752\n",
      "sex          1606\n",
      "would        1464\n",
      "even         1448\n",
      "attractive   1417\n",
      "\n",
      "=== Community 129 ===\n",
      "Posts containing both: 14147\n",
      "\n",
      "Top window-context words for 'women':\n",
      "       count\n",
      "want    1740\n",
      "even    1456\n",
      "white   1443\n",
      "think   1366\n",
      "would   1346\n",
      "\n",
      "Top window-context words for 'men':\n",
      "       count\n",
      "want    1305\n",
      "white   1293\n",
      "even    1185\n",
      "would   1137\n",
      "sex     1102\n",
      "\n",
      "=== Community 191 ===\n",
      "Posts containing both: 9671\n",
      "\n",
      "Top window-context words for 'women':\n",
      "       count\n",
      "sex     1212\n",
      "want    1169\n",
      "even    1075\n",
      "think    959\n",
      "would    925\n",
      "\n",
      "Top window-context words for 'men':\n",
      "            count\n",
      "sex          1056\n",
      "want          921\n",
      "attractive    907\n",
      "even          846\n",
      "think         780\n",
      "\n",
      "=== Community 220 ===\n",
      "Posts containing both: 1949\n",
      "\n",
      "Top window-context words for 'women':\n",
      "       count\n",
      "want     237\n",
      "even     212\n",
      "white    211\n",
      "man      191\n",
      "sex      183\n",
      "\n",
      "Top window-context words for 'men':\n",
      "            count\n",
      "white         202\n",
      "even          184\n",
      "sex           184\n",
      "want          178\n",
      "attractive    161\n",
      "\n",
      "=== Community 188 ===\n",
      "Posts containing both: 2705\n",
      "\n",
      "Top window-context words for 'women':\n",
      "        count\n",
      "think     560\n",
      "want      539\n",
      "people    504\n",
      "know      449\n",
      "many      396\n",
      "\n",
      "Top window-context words for 'men':\n",
      "        count\n",
      "think     398\n",
      "people    339\n",
      "short     324\n",
      "also      310\n",
      "many      301\n"
     ]
    }
   ],
   "source": [
    "WINDOW = 5\n",
    "\n",
    "context_results = {}\n",
    "\n",
    "for cid in top_6_communities:\n",
    "\n",
    "    print(f\"\\n=== Community {cid} ===\")\n",
    "\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "    N = len(df_comm)\n",
    "\n",
    "    # context counters\n",
    "    context_women = Counter()\n",
    "    context_men = Counter()\n",
    "\n",
    "    valid_posts = 0\n",
    "\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        if \"women\" in tokens and \"men\" in tokens:\n",
    "            valid_posts += 1\n",
    "\n",
    "            # find all positions of each word\n",
    "            women_positions = [i for i, t in enumerate(tokens) if t == \"women\"]\n",
    "            men_positions =    [i for i, t in enumerate(tokens) if t == \"men\"]\n",
    "\n",
    "            # collect window contexts\n",
    "            for pos in women_positions:\n",
    "                start = max(0, pos - WINDOW)\n",
    "                end   = pos + WINDOW + 1\n",
    "                local = set(tokens[start:end]) - {\"women\", \"men\"}\n",
    "                for w in local:\n",
    "                    context_women[w] += 1\n",
    "\n",
    "            for pos in men_positions:\n",
    "                start = max(0, pos - WINDOW)\n",
    "                end   = pos + WINDOW + 1\n",
    "                local = set(tokens[start:end]) - {\"women\", \"men\"}\n",
    "                for w in local:\n",
    "                    context_men[w] += 1\n",
    "\n",
    "    # to DataFrames\n",
    "    df_w = (\n",
    "        pd.DataFrame.from_dict(context_women, orient=\"index\", columns=[\"count\"])\n",
    "        .sort_values(\"count\", ascending=False)\n",
    "    )\n",
    "    df_m = (\n",
    "        pd.DataFrame.from_dict(context_men, orient=\"index\", columns=[\"count\"])\n",
    "        .sort_values(\"count\", ascending=False)\n",
    "    )\n",
    "\n",
    "    print(f\"Posts containing both: {valid_posts}\")\n",
    "    print(\"\\nTop window-context words for 'women':\")\n",
    "    print(df_w.head())\n",
    "\n",
    "    print(\"\\nTop window-context words for 'men':\")\n",
    "    print(df_m.head())\n",
    "\n",
    "    context_results[cid] = {\"women\": df_w, \"men\": df_m}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b44fc2",
   "metadata": {},
   "source": [
    "# A-priori using library\n",
    "\n",
    "To further validate the above results, we also implemented the A-priori algorithm using mlxtend. We find that the results of using the mlxtend framwork are congruent with the results found by implementing the A-priori algorithm as described in Mining of Massive Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e87124af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 130\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 176\n",
      "\n",
      "Community 89\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 170\n",
      "\n",
      "Community 129\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 144\n",
      "\n",
      "Community 191\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 167\n",
      "\n",
      "Community 220\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 93\n",
      "\n",
      "Community 188\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 542\n"
     ]
    }
   ],
   "source": [
    "te = TransactionEncoder()\n",
    "encoded_tables = {} \n",
    "\n",
    "for cid in top_6_communities:\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "    \n",
    "    freq_words = set(freq_tables[cid][\"word\"])\n",
    "    \n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "    transactions = [\n",
    "        [t for t in tokens if t in freq_words]\n",
    "        for tokens in df_comm[\"tokens\"]\n",
    "    ]\n",
    "\n",
    "    te_array = te.fit(transactions).transform(transactions)\n",
    "    df_encoded = pd.DataFrame(te_array, columns=te.columns_)\n",
    "    \n",
    "    encoded_tables[cid] = df_encoded\n",
    "    \n",
    "    print(f\"Number of frequent items (singletons) with Support(I) => s of 0.01: {df_encoded.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0ac9b3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 130\n",
      "Number of frequent 2-itemsets: 30\n",
      "Top 5 pairs:\n",
      "      support         itemsets\n",
      "190  0.027921     (women, men)\n",
      "197  0.017549  (people, women)\n",
      "195  0.015904  (people, think)\n",
      "205  0.015525   (would, women)\n",
      "200  0.015351   (women, think)\n",
      "\n",
      "Community 89\n",
      "Number of frequent 2-itemsets: 21\n",
      "Top 5 pairs:\n",
      "      support         itemsets\n",
      "179  0.027531     (women, men)\n",
      "184  0.015657  (people, women)\n",
      "173  0.014921    (women, even)\n",
      "189  0.014781    (women, want)\n",
      "190  0.014611   (would, women)\n",
      "\n",
      "Community 129\n",
      "Number of frequent 2-itemsets: 7\n",
      "Top 5 pairs:\n",
      "      support         itemsets\n",
      "146  0.022449     (women, men)\n",
      "145  0.012316    (even, women)\n",
      "150  0.010813   (would, women)\n",
      "147  0.010551  (people, women)\n",
      "149  0.010542    (women, want)\n",
      "\n",
      "Community 191\n",
      "Number of frequent 2-itemsets: 14\n",
      "Top 5 pairs:\n",
      "      support         itemsets\n",
      "171  0.024187     (women, men)\n",
      "168  0.014283    (even, women)\n",
      "180  0.012998   (would, women)\n",
      "177  0.012200   (women, think)\n",
      "174  0.012180  (people, think)\n",
      "\n",
      "Community 220\n",
      "Number of frequent 2-itemsets: 1\n",
      "Top 5 pairs:\n",
      "     support      itemsets\n",
      "93  0.012647  (women, men)\n",
      "\n",
      "Community 188\n",
      "Number of frequent 2-itemsets: 3827\n",
      "Top 5 pairs:\n",
      "       support         itemsets\n",
      "3649  0.093219  (people, think)\n",
      "2641  0.077812   (people, know)\n",
      "3481  0.074721    (people, one)\n",
      "3667  0.073058   (people, want)\n",
      "1569  0.071031   (people, even)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "frequent_2_itemsets_by_community = {}\n",
    "\n",
    "for cid in top_6_communities:\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "    \n",
    "    df_encoded = encoded_tables[cid] \n",
    "    \n",
    "    frequent_itemsets = apriori(df_encoded, min_support=0.01, use_colnames=True)\n",
    "    \n",
    "    # filter to only 2-itemsets\n",
    "    frequent_2_itemsets = frequent_itemsets[\n",
    "        frequent_itemsets['itemsets'].apply(lambda x: len(x) == 2)\n",
    "    ].copy()\n",
    "    \n",
    "    frequent_2_itemsets_by_community[cid] = frequent_2_itemsets\n",
    "    \n",
    "    print(\"Number of frequent 2-itemsets:\", len(frequent_2_itemsets))\n",
    "    print(\"Top 5 pairs:\")\n",
    "    print(frequent_2_itemsets.sort_values(by=\"support\", ascending=False).head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
