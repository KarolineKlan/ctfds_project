{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ea257d",
   "metadata": {},
   "source": [
    "# Frequent items analysis of reddit Louvain communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4c2c536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/livdreyerjohansen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import networkx as nx\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "import html\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import math\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88d1fd7",
   "metadata": {},
   "source": [
    "## Dataload and Cleaning\n",
    "\n",
    "To analysize the what frequent items we may see in the reddit communities found in 03_NetworkAnalysis.ipynb, we must first load the graph with the added attributes, that tell what community each node belongs in and the posts created by each node. We have chosen to only look at the 6 largest communities, by number of nodes, as the distribution of nodes / community is very heavly right skewed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cee6b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load graph\n",
    "\n",
    "G = nx.read_gml('FINAL_reddit_graph_with_louvain_communities.gml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de689bc",
   "metadata": {},
   "source": [
    "To ensure we do not include posts that are either deleted (\"\\[deleted\\]\") or removed (\"\\[removed\\]\"), both basic reddit features that happen independently of what forum you are in, we remove both. Furthermore, we remove each post that was removed by a bot, which is clear in the text which the bot uses to explain why a post or comment is deleted. We then construct a dataframe with all posts and their community (and original poster (OP in reddit linguistics))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03a8126f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id                                               text  community\n",
      "0  1  \"Huh it's still not legalized yet. America is ...        130\n",
      "1  1  \"Hey charisma helps. Everybody wants to sleep ...        130\n",
      "2  1  Aren't the jedis not really good guys though? ...        130\n",
      "3  1  Wait but ferb is the better looking one with a...        130\n",
      "4  1  Great now you live with a hole in your head fo...        130\n",
      "\n",
      "Number of original posts:\n",
      "2,664,156\n",
      "Number of removed posts:\n",
      "158,251\n",
      "Number of posts in dataframe:\n",
      "2,505,905\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "rows_count = 0\n",
    "allowed_rows = 0\n",
    "\n",
    "for node, data in G.nodes(data=True):\n",
    "    community = data.get(\"community\")\n",
    "    posts_dict = data.get(\"posts\", {})\n",
    "\n",
    "    # Ensure it's a dictionary\n",
    "    if not isinstance(posts_dict, dict):\n",
    "        posts_dict = {\"default\": posts_dict}\n",
    "\n",
    "    # Loop through each list of posts in the dictionary\n",
    "    for key, posts in posts_dict.items():\n",
    "        if not isinstance(posts, list):\n",
    "            posts = [posts]\n",
    "\n",
    "        for post in posts:\n",
    "            rows_count += 1\n",
    "            # Skip empty, deleted/removed posts, or posts containing the bot line\n",
    "            if post and post not in ['[deleted]', '[removed]'] and \\\n",
    "               \"*I am a bot, and this action was performed automatically.\" not in post:\n",
    "                allowed_rows += 1\n",
    "                rows.append({\n",
    "                    \"id\": node,\n",
    "                    \"text\": post,\n",
    "                    \"community\": community\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nNumber of original posts:\")\n",
    "print(f\"{rows_count:,}\")\n",
    "print(\"Number of removed posts:\")\n",
    "print(f\"{rows_count-allowed_rows:,}\")\n",
    "print(\"Number of posts in dataframe:\")\n",
    "print(f\"{allowed_rows:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dbf9c4",
   "metadata": {},
   "source": [
    "We identify the top-6 largest communities in terms of nodes to continue working with only them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f4063e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = df['community'].unique().tolist()\n",
    "communities_dict = dict.fromkeys(communities, 0)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    communities_dict[row['community']] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f165e4",
   "metadata": {},
   "source": [
    "Filter the dataframe to only contain posts from top-6 communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c521f85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 6 communities by number of posts (posts, community): [(644383, 130), (635828, 89), (630172, 129), (399843, 191), (154109, 220), (38490, 188)]\n",
      "Number of posts in top 6 communities: 2,502,825\n",
      "Number of posts removed based on non identity in top 6: 3,080\n"
     ]
    }
   ],
   "source": [
    "comms_list = list(sorted( ((v,k) for k,v in communities_dict.items()), reverse=True))\n",
    "comms_list = comms_list[:6]\n",
    "top_6_communities = [item[1] for item in comms_list]\n",
    "\n",
    "df_filtered = df[df['community'].isin(top_6_communities)].copy()\n",
    "\n",
    "print(\"\\nTop 6 communities by number of posts (posts, community):\", comms_list)\n",
    "\n",
    "print(f\"Number of posts in top 6 communities: {len(df_filtered):,}\")\n",
    "\n",
    "print(f\"Number of posts removed based on non identity in top 6: {(len(df) - len(df_filtered)):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1289c493",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "We filter out parts of post that we deem have little semantic value. We aim to find frequent items and frequent itemsets (item pairs), and would assume that stop words regularly occur in more than 1% of baskets. As we are working with online fora, we chose to add certain slang-terms as stop words. We furthermore remove:\n",
    "\n",
    "- html entities\n",
    "- URL's (including GIF's and images)\n",
    "- non-text artifacts (such as \"/\", \"?\", \"!\" etc.)\n",
    "- remaining \"removed\" and \"deleted\" artifacts that were not removed in the previous code block due to the way the post was loaded\n",
    "- short words (length of 2 or less)\n",
    "\n",
    "Additionally, we make all words lowercase to steamline and tokenize by word (meaning each word will be its own token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "422cbe8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "community",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tokens",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "n_tokens",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "dc82e224-c6d0-432c-b71f-097f29b332a1",
       "rows": [
        [
         "0",
         "1",
         "\"Huh it's still not legalized yet. America is weirdly antiquated\" ",
         "130",
         "['huh', 'still', 'legalized', 'yet', 'america', 'weirdly', 'antiquated']",
         "7"
        ],
        [
         "1",
         "1",
         "\"Hey charisma helps. Everybody wants to sleep with pretty people. Doesn't mean they enjoy dating them\" ",
         "130",
         "['hey', 'charisma', 'helps', 'everybody', 'wants', 'sleep', 'pretty', 'people', 'mean', 'enjoy', 'dating']",
         "11"
        ],
        [
         "2",
         "1",
         "Aren't the jedis not really good guys though? Like they protect the status quo. That and how they serve the same cosmic deity that doesn't care about anything and can't be bothered by which of its \"\"sides\"\" its pathetic worshippers venerate? Idk much about star wars sorry if I got it wrong ",
         "130",
         "['jedis', 'really', 'good', 'guys', 'though', 'protect', 'status', 'quo', 'serve', 'cosmic', 'deity', 'care', 'anything', 'bothered', 'sides', 'pathetic', 'worshippers', 'venerate', 'much', 'star', 'wars', 'sorry', 'got', 'wrong']",
         "24"
        ],
        [
         "3",
         "1",
         "Wait but ferb is the better looking one with actual game. Phineas is a fucking geometry figure for god sakes ",
         "130",
         "['wait', 'ferb', 'better', 'looking', 'one', 'actual', 'game', 'phineas', 'fucking', 'geometry', 'figure', 'god', 'sakes']",
         "13"
        ],
        [
         "4",
         "1",
         "Great now you live with a hole in your head for eternity ",
         "130",
         "['great', 'live', 'hole', 'head', 'eternity']",
         "5"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>community</th>\n",
       "      <th>tokens</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Huh it's still not legalized yet. America is ...</td>\n",
       "      <td>130</td>\n",
       "      <td>[huh, still, legalized, yet, america, weirdly,...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Hey charisma helps. Everybody wants to sleep ...</td>\n",
       "      <td>130</td>\n",
       "      <td>[hey, charisma, helps, everybody, wants, sleep...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Aren't the jedis not really good guys though? ...</td>\n",
       "      <td>130</td>\n",
       "      <td>[jedis, really, good, guys, though, protect, s...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Wait but ferb is the better looking one with a...</td>\n",
       "      <td>130</td>\n",
       "      <td>[wait, ferb, better, looking, one, actual, gam...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Great now you live with a hole in your head fo...</td>\n",
       "      <td>130</td>\n",
       "      <td>[great, live, hole, head, eternity]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                               text  community  \\\n",
       "0  1  \"Huh it's still not legalized yet. America is ...        130   \n",
       "1  1  \"Hey charisma helps. Everybody wants to sleep ...        130   \n",
       "2  1  Aren't the jedis not really good guys though? ...        130   \n",
       "3  1  Wait but ferb is the better looking one with a...        130   \n",
       "4  1  Great now you live with a hole in your head fo...        130   \n",
       "\n",
       "                                              tokens  n_tokens  \n",
       "0  [huh, still, legalized, yet, america, weirdly,...         7  \n",
       "1  [hey, charisma, helps, everybody, wants, sleep...        11  \n",
       "2  [jedis, really, good, guys, though, protect, s...        24  \n",
       "3  [wait, ferb, better, looking, one, actual, gam...        13  \n",
       "4                [great, live, hole, head, eternity]         5  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize and clean text data\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# extend basic english stopwords with slang terms\n",
    "extra_stops = {\n",
    "    'lol', 'xd', 'haha', 'hahaah', 'omg', 'u', 'ur', 'im', 'ive', 'idk', \n",
    "    'dont', 'cant', 'wont', 'aint', 'ya', 'tho', 'tho', 'nah', 'btw', \n",
    "    'like', 'yeah', 'yep', 'ok', 'okay', 'pls', 'please', 'get'\n",
    "}\n",
    "stop_words.update(extra_stops)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # decode HTML entities: &amp; → &, &#x200B; → zero-width space, etc.\n",
    "    text = html.unescape(text)\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
    "    # keep only letters and spaces\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    # tokenize by whitespace\n",
    "    tokens = text.split()\n",
    "    # remove stopwords and very short tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    if len(tokens) == 1 and tokens[0] in {\"removed\", \"deleted\"}:\n",
    "        return []\n",
    "    return tokens\n",
    "\n",
    "df_filtered[\"tokens\"] = df_filtered[\"text\"].apply(preprocess_text)\n",
    "df_filtered[\"n_tokens\"] = df_filtered[\"tokens\"].apply(len)\n",
    "\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4075dcf8",
   "metadata": {},
   "source": [
    "# Frequent items and the A-priori algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27146132",
   "metadata": {},
   "source": [
    "We process the tokenized posts by identifying the amount of unique tokens for each community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4f9c689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community 130:\n",
      "  Total tokens: 8,159,749\n",
      "  Unique tokens: 140,319\n",
      "Community 89:\n",
      "  Total tokens: 7,721,869\n",
      "  Unique tokens: 149,731\n",
      "Community 129:\n",
      "  Total tokens: 6,700,663\n",
      "  Unique tokens: 146,730\n",
      "Community 191:\n",
      "  Total tokens: 4,709,952\n",
      "  Unique tokens: 89,034\n",
      "Community 220:\n",
      "  Total tokens: 1,226,959\n",
      "  Unique tokens: 59,693\n",
      "Community 188:\n",
      "  Total tokens: 1,305,336\n",
      "  Unique tokens: 33,100\n"
     ]
    }
   ],
   "source": [
    "# build token statistics for each of the top 6 communities\n",
    "\n",
    "community_token_stats = {}\n",
    "\n",
    "for community_id in top_6_communities:\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == community_id]\n",
    "\n",
    "    # flatten all tokens for this community\n",
    "    all_tokens = []\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    unique_tokens = set(all_tokens)\n",
    "\n",
    "    community_token_stats[community_id] = {\n",
    "        \"n_tokens\": len(all_tokens),\n",
    "        \"n_unique_tokens\": len(unique_tokens),       \n",
    "        \"unique_tokens\": unique_tokens \n",
    "    }\n",
    "\n",
    "\n",
    "for cid in top_6_communities:\n",
    "    print(f\"Community {cid}:\")\n",
    "    print(f\"  Total tokens: {community_token_stats[cid]['n_tokens']:,}\")\n",
    "    print(f\"  Unique tokens: {community_token_stats[cid]['n_unique_tokens']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eca728",
   "metadata": {},
   "source": [
    "## First pass of the A-priori algorithm\n",
    "\n",
    "In the first pass of the A-priori algoritm, we initialize a dataframe for each of the communities. In this dataframe, we will store each of the unique tokens found previously, assign them each an integer from 0 to n-1 (number of unique tokens), and count how many baskets (posts) the item (token) appears in. It is important to note that we do not count the total occurrence of the token but only the amount of posts it appears in. In Mining of Massive Datasets, Section 6.2.2, the first pass is described as labeling integers 1 to n, but to keep it within the python framework, we label 0 to n-1 as mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f323c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori_tables = {}\n",
    "\n",
    "for cid in top_6_communities:\n",
    "    # get df for this community\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "    unique_tokens = list(community_token_stats[cid][\"unique_tokens\"])\n",
    "\n",
    "    # apriori table\n",
    "    df_apriori = pd.DataFrame({\n",
    "        \"word\": unique_tokens,\n",
    "        \"integer\": range(len(unique_tokens))\n",
    "    })\n",
    "\n",
    "    # give each word an integer from 0 to n-1\n",
    "    word_to_int = dict(zip(df_apriori[\"word\"], df_apriori[\"integer\"]))\n",
    "\n",
    "    # count posts that contain each token\n",
    "    array_of_counts = np.zeros(len(unique_tokens), dtype=int)\n",
    "\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        for token in set(tokens):             \n",
    "            array_of_counts[word_to_int[token]] += 1\n",
    "\n",
    "    df_apriori[\"count\"] = array_of_counts\n",
    "\n",
    "    # save in dict\n",
    "    apriori_tables[cid] = df_apriori\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "692f4753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 130\n",
      "          word  integer  count\n",
      "14510    women    14510  64054\n",
      "136745  people   136745  54841\n",
      "139172   would   139172  45478\n",
      "132015    even   132015  41610\n",
      "59260      one    59260  40846\n",
      "\n",
      "Community 89\n",
      "          word  integer  count\n",
      "15445    women    15445  61740\n",
      "146121  people   146121  47917\n",
      "148565   would   148565  41218\n",
      "140976    even   140976  39135\n",
      "63045      one    63045  38397\n",
      "\n",
      "Community 129\n",
      "          word  integer  count\n",
      "14950    women    14950  49985\n",
      "138067    even   138067  37953\n",
      "143066  people   143066  36781\n",
      "145571   would   145571  35755\n",
      "61974      one    61974  31571\n",
      "\n",
      "Community 191\n",
      "         word  integer  count\n",
      "9045    women     9045  36120\n",
      "88346   would    88346  28225\n",
      "86816  people    86816  27657\n",
      "83806    even    83806  27309\n",
      "37496     one    37496  23836\n",
      "\n",
      "Community 220\n",
      "         word  integer  count\n",
      "12065   women    12065   7367\n",
      "52612    even    52612   6899\n",
      "58797   would    58797   5976\n",
      "56767  people    56767   5766\n",
      "50275     one    50275   5658\n",
      "\n",
      "Community 188\n",
      "         word  integer  count\n",
      "31479  people    31479  10376\n",
      "6201    think     6201   7425\n",
      "6601    women     6601   7331\n",
      "22096    know    22096   6431\n",
      "32583   would    32583   6056\n"
     ]
    }
   ],
   "source": [
    "# print top 5 tokens by count for each community\n",
    "for cid, df_apriori in apriori_tables.items():\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "    print(df_apriori.sort_values(by=\"count\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5646a40",
   "metadata": {},
   "source": [
    "## Between the passes of A-priori\n",
    "\n",
    "We create frequency tables where we assign each word an integer from 1-m, where m = number of frequent singletons (words), if the support of the word => 1%. In other words, it must appear in 1% or more of the baskets. If the word is not frequent, we assign it 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2d554ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 130 frequent items:\n",
      "                word  integer  count  freq_integer\n",
      "2138            girl     2138  15107             1\n",
      "5119           means     5119   7116             2\n",
      "7688     personality     7688  11510             3\n",
      "8886    relationship     8886   8373             4\n",
      "10621         making    10621   7366             5\n",
      "...              ...      ...    ...           ...\n",
      "136745        people   136745  54841           172\n",
      "138087          chad   138087  24716           173\n",
      "138315          self   138315   8511           174\n",
      "139034        person   139034  13871           175\n",
      "139172         would   139172  45478           176\n",
      "\n",
      "[176 rows x 4 columns]\n",
      "\n",
      "Community 89 frequent items:\n",
      "                word  integer  count  freq_integer\n",
      "2253            girl     2253  16290             1\n",
      "8116     personality     8116   9774             2\n",
      "9387    relationship     9387   7455             3\n",
      "11230         making    11230   6487             4\n",
      "11341           lmao    11341   7882             5\n",
      "...              ...      ...    ...           ...\n",
      "146121        people   146121  47917           166\n",
      "147461          chad   147461  22847           167\n",
      "147692          self   147692   8218           168\n",
      "148422        person   148422  11425           169\n",
      "148565         would   148565  41218           170\n",
      "\n",
      "[170 rows x 4 columns]\n",
      "\n",
      "Community 129 frequent items:\n",
      "               word  integer  count  freq_integer\n",
      "2227           girl     2227  15715             1\n",
      "7898    personality     7898   7811             2\n",
      "11045          lmao    11045   9438             3\n",
      "11176        really    11176  19978             4\n",
      "12795      everyone    12795   8500             5\n",
      "...             ...      ...    ...           ...\n",
      "142452        least   142452   9202           140\n",
      "143066       people   143066  36781           141\n",
      "144445         chad   144445  24688           142\n",
      "145426       person   145426   7900           143\n",
      "145571        would   145571  35755           144\n",
      "\n",
      "[144 rows x 4 columns]\n",
      "\n",
      "Community 191 frequent items:\n",
      "               word  integer  count  freq_integer\n",
      "1349           girl     1349  10583             1\n",
      "4763    personality     4763   5293             2\n",
      "5532   relationship     5532   4107             3\n",
      "6763         really     6763  15672             4\n",
      "7761       everyone     7761   6382             5\n",
      "...             ...      ...    ...           ...\n",
      "86816        people    86816  27657           163\n",
      "87633          chad    87633  19206           164\n",
      "87770          self    87770   4672           165\n",
      "88261        person    88261   6398           166\n",
      "88346         would    88346  28225           167\n",
      "\n",
      "[167 rows x 4 columns]\n",
      "\n",
      "Community 220 frequent items:\n",
      "         word  integer  count  freq_integer\n",
      "2        much        2   3076             1\n",
      "80      looks       80   2742             2\n",
      "1785     girl     1785   2048             3\n",
      "1787   better     1787   2366             4\n",
      "3020    every     3020   2267             5\n",
      "...       ...      ...    ...           ...\n",
      "57688     bro    57688   1923            89\n",
      "57904    chad    57904   4619            90\n",
      "58107  always    58107   1929            91\n",
      "58797   would    58797   5976            92\n",
      "59431   thing    59431   2263            93\n",
      "\n",
      "[93 rows x 4 columns]\n",
      "\n",
      "Community 188 frequent items:\n",
      "             word  integer  count  freq_integer\n",
      "0            much        0   3994             1\n",
      "15          value       15    578             2\n",
      "38        partner       38   1057             3\n",
      "57          looks       57   1668             4\n",
      "86          heard       86    483             5\n",
      "...           ...      ...    ...           ...\n",
      "32474     another    32474   1424           538\n",
      "32511      person    32511   3323           539\n",
      "32583       would    32583   6056           540\n",
      "32861  personally    32861    662           541\n",
      "32950       thing    32950   3574           542\n",
      "\n",
      "[542 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "freq_tables = {}\n",
    "\n",
    "threshold_factor = 0.01  \n",
    "\n",
    "for cid, df_apriori in apriori_tables.items():\n",
    "\n",
    "    # threshold is 1% of posts in that community\n",
    "    threshold = threshold_factor * len(df_filtered[df_filtered[\"community\"] == cid])\n",
    "\n",
    "    frequent_map = np.zeros(len(df_apriori), dtype=int)\n",
    "    new_id = 1\n",
    "\n",
    "    for old_id, count in enumerate(df_apriori['count']):\n",
    "        if count >= threshold:\n",
    "            frequent_map[old_id] = new_id\n",
    "            new_id += 1\n",
    "        else:\n",
    "            frequent_map[old_id] = 0\n",
    "\n",
    "    # add freq_integer column\n",
    "    df_apriori['freq_integer'] = frequent_map\n",
    "\n",
    "    # store only frequent items in new dictionary\n",
    "    df_freq = df_apriori[df_apriori['freq_integer'] != 0].copy()\n",
    "    freq_tables[cid] = df_freq\n",
    "\n",
    "    print(f\"\\nCommunity {cid} frequent items:\")\n",
    "    print(df_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d38f5ea",
   "metadata": {},
   "source": [
    "## Second pass of the A-priori algorithm\n",
    "\n",
    "For the second pass, we first find all pairs of frequent words from the previous dataframes. We then create pairs of those, making sure to remove duplicates. We apply the support threshold of 1% here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cab3263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 130\n",
      "Top pairs:\n",
      "             item_set  count\n",
      "881      (men, women)  17992\n",
      "2296  (people, women)  11308\n",
      "935   (people, think)  10248\n",
      "713    (women, would)  10004\n",
      "1307   (think, women)   9892\n",
      "Total pairs where Support(I) => s of 0.01: 30\n",
      "\n",
      "Community 89\n",
      "Top pairs:\n",
      "             item_set  count\n",
      "36       (men, women)  17505\n",
      "161   (people, women)   9955\n",
      "1071    (even, women)   9487\n",
      "244     (want, women)   9398\n",
      "758    (women, would)   9290\n",
      "Total pairs where Support(I) => s of 0.01: 21\n",
      "\n",
      "Community 129\n",
      "Top pairs:\n",
      "             item_set  count\n",
      "525      (men, women)  14147\n",
      "1090    (even, women)   7761\n",
      "2898   (women, would)   6814\n",
      "391   (people, women)   6649\n",
      "1214    (want, women)   6643\n",
      "Total pairs where Support(I) => s of 0.01: 7\n",
      "\n",
      "Community 191\n",
      "Top pairs:\n",
      "             item_set  count\n",
      "2763     (men, women)   9671\n",
      "4030    (even, women)   5711\n",
      "6289   (women, would)   5197\n",
      "2775   (think, women)   4878\n",
      "57    (people, think)   4870\n",
      "Total pairs where Support(I) => s of 0.01: 14\n",
      "\n",
      "Community 220\n",
      "Top pairs:\n",
      "        item_set  count\n",
      "24  (men, women)   1949\n",
      "Total pairs where Support(I) => s of 0.01: 1\n",
      "\n",
      "Community 188\n",
      "Top pairs:\n",
      "             item_set  count\n",
      "1040  (people, think)   3588\n",
      "9507   (know, people)   2995\n",
      "1351    (one, people)   2876\n",
      "1029   (people, want)   2812\n",
      "1327   (even, people)   2734\n",
      "Total pairs where Support(I) => s of 0.01: 3827\n"
     ]
    }
   ],
   "source": [
    "pair_tables = {}   # store results for each community\n",
    "\n",
    "for cid in top_6_communities:\n",
    "\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "\n",
    "    # pull posts for this community\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "    N = len(df_comm)\n",
    "\n",
    "    # fetch frequent 1-itemset for this community\n",
    "    df_freq = freq_tables[cid]\n",
    "    frequent_words_set = set(df_freq[\"word\"])\n",
    "\n",
    "    # counter for all frequent pairs\n",
    "    pair_counter = Counter()\n",
    "\n",
    "    # iterate over all posts\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        # keep only frequent tokens\n",
    "        frequent_tokens = [t for t in tokens if t in frequent_words_set]\n",
    "\n",
    "        # deduplicate within a post\n",
    "        unique_tokens = set(frequent_tokens)\n",
    "\n",
    "        # count each 2-item combination in this post\n",
    "        for pair in combinations(unique_tokens, 2):\n",
    "            pair_counter[tuple(sorted(pair))] += 1\n",
    "\n",
    "    # convert counter → dataframe\n",
    "    df_pairs = pd.DataFrame(pair_counter.items(), columns=[\"item_set\", \"count\"])\n",
    "\n",
    "    # threshold for frequent 2-itemsets (1% of posts)\n",
    "    threshold = math.ceil(0.01 * N)\n",
    "    df_pairs = df_pairs[df_pairs[\"count\"] >= threshold]\n",
    "\n",
    "    # store\n",
    "    pair_tables[cid] = df_pairs\n",
    "\n",
    "    # print summary\n",
    "    print(\"Top pairs:\")\n",
    "    print(df_pairs.sort_values(by=\"count\", ascending=False).head())\n",
    "    print(\"Total pairs where Support(I) => s of 0.01:\", len(df_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c2a239",
   "metadata": {},
   "source": [
    "To gain knowledge into the context of the frequent item pairs, we count the occurance of words around our top frequent item pair (men, women). We go through each post that contain both items, identify the tokens close (window of 5 on each side if possible) to the items, count the occurance of a token within the window of our item and sort them. We find that there is a difference in words between the communities even though we did not see one when looking only at the frequency pairs. This exercise (or in future work, and extenstion of this exercise) is furthermore important to obtain a sense of the context of the words. It is not possible from the frequent items analysis done previously to find context clues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d68995eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 130\n",
      "Posts containing both: 17992\n",
      "\n",
      "Top window-context words for 'women':\n",
      "            count\n",
      "want         2614\n",
      "sex          2192\n",
      "think        2088\n",
      "would        1961\n",
      "attractive   1859\n",
      "\n",
      "Top window-context words for 'men':\n",
      "            count\n",
      "want         1871\n",
      "sex          1825\n",
      "attractive   1735\n",
      "ugly         1518\n",
      "think        1518\n",
      "\n",
      "Community 89\n",
      "Posts containing both: 17505\n",
      "\n",
      "Top window-context words for 'women':\n",
      "       count\n",
      "want    2448\n",
      "sex     1953\n",
      "think   1942\n",
      "even    1869\n",
      "would   1819\n",
      "\n",
      "Top window-context words for 'men':\n",
      "            count\n",
      "want         1752\n",
      "sex          1606\n",
      "would        1464\n",
      "even         1448\n",
      "attractive   1417\n",
      "\n",
      "Community 129\n",
      "Posts containing both: 14147\n",
      "\n",
      "Top window-context words for 'women':\n",
      "       count\n",
      "want    1740\n",
      "even    1456\n",
      "white   1443\n",
      "think   1366\n",
      "would   1346\n",
      "\n",
      "Top window-context words for 'men':\n",
      "       count\n",
      "want    1305\n",
      "white   1293\n",
      "even    1185\n",
      "would   1137\n",
      "sex     1102\n",
      "\n",
      "Community 191\n",
      "Posts containing both: 9671\n",
      "\n",
      "Top window-context words for 'women':\n",
      "       count\n",
      "sex     1212\n",
      "want    1169\n",
      "even    1075\n",
      "think    959\n",
      "would    925\n",
      "\n",
      "Top window-context words for 'men':\n",
      "            count\n",
      "sex          1056\n",
      "want          921\n",
      "attractive    907\n",
      "even          846\n",
      "think         780\n",
      "\n",
      "Community 220\n",
      "Posts containing both: 1949\n",
      "\n",
      "Top window-context words for 'women':\n",
      "       count\n",
      "want     237\n",
      "even     212\n",
      "white    211\n",
      "man      191\n",
      "sex      183\n",
      "\n",
      "Top window-context words for 'men':\n",
      "            count\n",
      "white         202\n",
      "sex           184\n",
      "even          184\n",
      "want          178\n",
      "attractive    161\n",
      "\n",
      "Community 188\n",
      "Posts containing both: 2705\n",
      "\n",
      "Top window-context words for 'women':\n",
      "        count\n",
      "think     560\n",
      "want      539\n",
      "people    504\n",
      "know      449\n",
      "many      396\n",
      "\n",
      "Top window-context words for 'men':\n",
      "        count\n",
      "think     398\n",
      "people    339\n",
      "short     324\n",
      "also      310\n",
      "many      301\n"
     ]
    }
   ],
   "source": [
    "WINDOW = 5\n",
    "\n",
    "context_results = {}\n",
    "\n",
    "for cid in top_6_communities:\n",
    "\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "    N = len(df_comm)\n",
    "\n",
    "    # context counters\n",
    "    context_women = Counter()\n",
    "    context_men = Counter()\n",
    "\n",
    "    valid_posts = 0\n",
    "\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        if \"women\" in tokens and \"men\" in tokens:\n",
    "            valid_posts += 1\n",
    "\n",
    "            # find all positions of each word\n",
    "            women_positions = [i for i, t in enumerate(tokens) if t == \"women\"]\n",
    "            men_positions =    [i for i, t in enumerate(tokens) if t == \"men\"]\n",
    "\n",
    "            # collect window contexts\n",
    "            for pos in women_positions:\n",
    "                start = max(0, pos - WINDOW)\n",
    "                end   = pos + WINDOW + 1\n",
    "                local = set(tokens[start:end]) - {\"women\", \"men\"}\n",
    "                for w in local:\n",
    "                    context_women[w] += 1\n",
    "\n",
    "            for pos in men_positions:\n",
    "                start = max(0, pos - WINDOW)\n",
    "                end   = pos + WINDOW + 1\n",
    "                local = set(tokens[start:end]) - {\"women\", \"men\"}\n",
    "                for w in local:\n",
    "                    context_men[w] += 1\n",
    "\n",
    "    # to DataFrames\n",
    "    df_w = (\n",
    "        pd.DataFrame.from_dict(context_women, orient=\"index\", columns=[\"count\"])\n",
    "        .sort_values(\"count\", ascending=False)\n",
    "    )\n",
    "    df_m = (\n",
    "        pd.DataFrame.from_dict(context_men, orient=\"index\", columns=[\"count\"])\n",
    "        .sort_values(\"count\", ascending=False)\n",
    "    )\n",
    "\n",
    "    print(f\"Posts containing both: {valid_posts}\")\n",
    "    print(\"\\nTop window-context words for 'women':\")\n",
    "    print(df_w.head())\n",
    "\n",
    "    print(\"\\nTop window-context words for 'men':\")\n",
    "    print(df_m.head())\n",
    "\n",
    "    context_results[cid] = {\"women\": df_w, \"men\": df_m}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b44fc2",
   "metadata": {},
   "source": [
    "# A-priori using library\n",
    "\n",
    "To further validate the above results, we also implemented the A-priori algorithm using mlxtend. We find that the results of using the mlxtend framwork are congruent with the results found by implementing the A-priori algorithm as described in Mining of Massive Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e87124af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 130\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 176\n",
      "\n",
      "Community 89\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 170\n",
      "\n",
      "Community 129\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 144\n",
      "\n",
      "Community 191\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 167\n",
      "\n",
      "Community 220\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 93\n",
      "\n",
      "Community 188\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 542\n"
     ]
    }
   ],
   "source": [
    "te = TransactionEncoder()\n",
    "encoded_tables = {} \n",
    "\n",
    "for cid in top_6_communities:\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "    \n",
    "    freq_words = set(freq_tables[cid][\"word\"])\n",
    "    \n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "    transactions = [\n",
    "        [t for t in tokens if t in freq_words]\n",
    "        for tokens in df_comm[\"tokens\"]\n",
    "    ]\n",
    "\n",
    "    te_array = te.fit(transactions).transform(transactions)\n",
    "    df_encoded = pd.DataFrame(te_array, columns=te.columns_)\n",
    "    \n",
    "    encoded_tables[cid] = df_encoded\n",
    "    \n",
    "    print(f\"Number of frequent items (singletons) with Support(I) => s of 0.01: {df_encoded.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ac9b3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 130\n",
      "Number of frequent 2-itemsets: 30\n",
      "Top 5 pairs:\n",
      "      support         itemsets\n",
      "190  0.027921     (men, women)\n",
      "197  0.017549  (people, women)\n",
      "195  0.015904  (people, think)\n",
      "205  0.015525   (would, women)\n",
      "200  0.015351   (think, women)\n",
      "\n",
      "Community 89\n",
      "Number of frequent 2-itemsets: 21\n",
      "Top 5 pairs:\n",
      "      support         itemsets\n",
      "179  0.027531     (men, women)\n",
      "184  0.015657  (people, women)\n",
      "173  0.014921    (women, even)\n",
      "189  0.014781    (want, women)\n",
      "190  0.014611   (would, women)\n",
      "\n",
      "Community 129\n",
      "Number of frequent 2-itemsets: 7\n",
      "Top 5 pairs:\n",
      "      support         itemsets\n",
      "146  0.022449     (men, women)\n",
      "145  0.012316    (even, women)\n",
      "150  0.010813   (would, women)\n",
      "147  0.010551  (people, women)\n",
      "149  0.010542    (want, women)\n",
      "\n",
      "Community 191\n",
      "Number of frequent 2-itemsets: 14\n",
      "Top 5 pairs:\n",
      "      support         itemsets\n",
      "171  0.024187     (men, women)\n",
      "168  0.014283    (even, women)\n",
      "180  0.012998   (would, women)\n",
      "177  0.012200   (women, think)\n",
      "174  0.012180  (people, think)\n",
      "\n",
      "Community 220\n",
      "Number of frequent 2-itemsets: 1\n",
      "Top 5 pairs:\n",
      "     support      itemsets\n",
      "93  0.012647  (men, women)\n",
      "\n",
      "Community 188\n",
      "Number of frequent 2-itemsets: 3827\n",
      "Top 5 pairs:\n",
      "       support         itemsets\n",
      "3649  0.093219  (people, think)\n",
      "2641  0.077812   (people, know)\n",
      "3481  0.074721    (people, one)\n",
      "3667  0.073058   (people, want)\n",
      "1569  0.071031   (people, even)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "frequent_2_itemsets_by_community = {}\n",
    "\n",
    "for cid in top_6_communities:\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "    \n",
    "    df_encoded = encoded_tables[cid] \n",
    "    \n",
    "    frequent_itemsets = apriori(df_encoded, min_support=0.01, use_colnames=True)\n",
    "    \n",
    "    # filter to only 2-itemsets\n",
    "    frequent_2_itemsets = frequent_itemsets[\n",
    "        frequent_itemsets['itemsets'].apply(lambda x: len(x) == 2)\n",
    "    ].copy()\n",
    "    \n",
    "    frequent_2_itemsets_by_community[cid] = frequent_2_itemsets\n",
    "    \n",
    "    print(\"Number of frequent 2-itemsets:\", len(frequent_2_itemsets))\n",
    "    print(\"Top 5 pairs:\")\n",
    "    print(frequent_2_itemsets.sort_values(by=\"support\", ascending=False).head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
