{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ea257d",
   "metadata": {},
   "source": [
    "# Frequent items analysis of reddit communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4c2c536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/livdreyerjohansen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import networkx as nx\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "import html\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import math\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88d1fd7",
   "metadata": {},
   "source": [
    "## Dataload and cleaning\n",
    "\n",
    "To analysize the what frequent items we may see in the reddit communities found in 03_NetworkAnalysis.ipynb, we must first load the graph with the added attributes, that tell what community each node belongs in and the posts created by each node. We have chosen to only look at the 6 largest communities, by number of nodes, as the distribution of nodes / community is very heavly right skewed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d6bbe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('post_clusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cf8dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df_raw.rename(columns={\"label\": \"community\"})\n",
    "df_raw = df_raw.rename(columns={\"user\": \"id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de689bc",
   "metadata": {},
   "source": [
    "To ensure we do not include posts that are either deleted (\"\\[deleted\\]\") or removed (\"\\[removed\\]\"), both basic reddit features that happen independently of what forum you are in, we remove both. Furthermore, we remove each post that was removed by a bot, which is clear in the text which the bot uses to explain why a post or comment is deleted. We then construct a dataframe with all posts and their community (and original poster (OP in reddit linguistics))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03a8126f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             id                                               text  community\n",
      "0  9.290107e+08  great now you live with a hole in your head fo...         44\n",
      "1  9.290107e+08  what? i mean i get where you're comming from b...          9\n",
      "2  9.290107e+08  chaggot? more like that fucking weirdo who sta...          9\n",
      "3  9.290107e+08  but then i'd have to leave too and you wouldnt...        179\n",
      "4  9.290107e+08  wait what's the deal with that guy? he's prett...        114\n",
      "\n",
      "Number of original posts:\n",
      "1,102,030\n",
      "Number of removed posts:\n",
      "0\n",
      "Number of posts in dataframe:\n",
      "1,102,030\n"
     ]
    }
   ],
   "source": [
    "# Expected columns in CSV:\n",
    "# id, text, community\n",
    "# (tell me if they differ)\n",
    "df_raw.drop(df_raw[df_raw[\"community\"] == -1].index, inplace=True)\n",
    "\n",
    "rows = []\n",
    "rows_count = 0\n",
    "allowed_rows = 0\n",
    "\n",
    "for _, r in df_raw.iterrows():\n",
    "    rows_count += 1\n",
    "    post = r[\"text\"]\n",
    "\n",
    "    # Skip empty, deleted/removed posts, or posts containing the bot line\n",
    "    if post and post not in ['[deleted]', '[removed]'] and \\\n",
    "       \"*I am a bot, and this action was performed automatically.\" not in post:\n",
    "\n",
    "        allowed_rows += 1\n",
    "\n",
    "        rows.append({\n",
    "            \"id\": r[\"id\"],\n",
    "            \"text\": post,\n",
    "            \"community\": r[\"community\"]\n",
    "        })\n",
    "\n",
    "# Build the dataframe exactly like before\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nNumber of original posts:\")\n",
    "print(f\"{rows_count:,}\")\n",
    "print(\"Number of removed posts:\")\n",
    "print(f\"{rows_count-allowed_rows:,}\")\n",
    "print(\"Number of posts in dataframe:\")\n",
    "print(f\"{allowed_rows:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dbf9c4",
   "metadata": {},
   "source": [
    "We identify the top-6 largest communities in terms of nodes to continue working with only them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f4063e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = df['community'].unique().tolist()\n",
    "communities_dict = dict.fromkeys(communities, 0)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    communities_dict[row['community']] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f165e4",
   "metadata": {},
   "source": [
    "Filter the dataframe to only contain posts from top-6 communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c521f85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 6 communities by number of posts (posts, community): [(95532, 9), (93385, 130), (54467, 81), (51607, 94), (32530, 204), (21818, 198)]\n",
      "Number of posts in top 6 communities: 349,339\n",
      "Number of posts removed based on non identity in top 6: 752,691\n"
     ]
    }
   ],
   "source": [
    "comms_list = list(sorted( ((v,k) for k,v in communities_dict.items()), reverse=True))\n",
    "comms_list = comms_list[:6]\n",
    "top_6_communities = [item[1] for item in comms_list]\n",
    "\n",
    "df_filtered = df[df['community'].isin(top_6_communities)].copy()\n",
    "\n",
    "print(\"\\nTop 6 communities by number of posts (posts, community):\", comms_list)\n",
    "\n",
    "print(f\"Number of posts in top 6 communities: {len(df_filtered):,}\")\n",
    "\n",
    "print(f\"Number of posts removed based on non identity in top 6: {(len(df) - len(df_filtered)):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1289c493",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "We filter out parts of post that we deem have little semantic value. We aim to find frequent items and frequent itemsets (item pairs), and would assume that stop words regularly occur in more than 1% of baskets. As we are working with online fora, we chose to add certain slang-terms as stop words. We furthermore remove:\n",
    "\n",
    "- html entities\n",
    "- URL's\n",
    "- non-text artifacts (such as \"/\", \"?\", \"!\" etc.)\n",
    "- remaining \"removed\" and \"deleted\" artifacts that were not removed in the previous code block due to the way the post was loaded\n",
    "- short words (length of 2 or less)\n",
    "\n",
    "Additionally, we make all words lowercase to steamline and tokenize by word (meaning each word will be its own token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "422cbe8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "community",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tokens",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "n_tokens",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "80ee9c34-dfd3-4bfc-a662-000edd256ccd",
       "rows": [
        [
         "1",
         "929010695.928077",
         "what? i mean i get where you're comming from but he doesn't really inspire respect like chads do",
         "9",
         "['mean', 'comming', 'really', 'inspire', 'respect', 'chads']",
         "6"
        ],
        [
         "2",
         "929010695.928077",
         "chaggot? more like that fucking weirdo who stalks you on grindr. gay chads are basically normal chad. thats why you shouldnt have a crush kn str8 guys. it doesnt matter if he's gay, he's still way out of your leaguenn",
         "9",
         "['chaggot', 'fucking', 'weirdo', 'stalks', 'grindr', 'gay', 'chads', 'basically', 'normal', 'chad', 'thats', 'shouldnt', 'crush', 'str', 'guys', 'doesnt', 'matter', 'gay', 'still', 'way', 'leaguenn']",
         "21"
        ],
        [
         "5",
         "929010695.928077",
         "isn't that the opposite of incels? she has so manny options she doesn't settle with the creepy ones?",
         "130",
         "['opposite', 'incels', 'manny', 'options', 'settle', 'creepy', 'ones']",
         "7"
        ],
        [
         "7",
         "929010695.928077",
         "what's so bad about hiring incels anyways? wouldn't they be more productive on account of having no life? as long as they don't reveal their power levels i don't see an issue here",
         "130",
         "['bad', 'hiring', 'incels', 'anyways', 'productive', 'account', 'life', 'long', 'reveal', 'power', 'levels', 'see', 'issue']",
         "13"
        ],
        [
         "8",
         "929010695.928077",
         "hey she deserved what's coming for her. can't feel too bad about that",
         "198",
         "['hey', 'deserved', 'coming', 'feel', 'bad']",
         "5"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>community</th>\n",
       "      <th>tokens</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.290107e+08</td>\n",
       "      <td>what? i mean i get where you're comming from b...</td>\n",
       "      <td>9</td>\n",
       "      <td>[mean, comming, really, inspire, respect, chads]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.290107e+08</td>\n",
       "      <td>chaggot? more like that fucking weirdo who sta...</td>\n",
       "      <td>9</td>\n",
       "      <td>[chaggot, fucking, weirdo, stalks, grindr, gay...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.290107e+08</td>\n",
       "      <td>isn't that the opposite of incels? she has so ...</td>\n",
       "      <td>130</td>\n",
       "      <td>[opposite, incels, manny, options, settle, cre...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9.290107e+08</td>\n",
       "      <td>what's so bad about hiring incels anyways? wou...</td>\n",
       "      <td>130</td>\n",
       "      <td>[bad, hiring, incels, anyways, productive, acc...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.290107e+08</td>\n",
       "      <td>hey she deserved what's coming for her. can't ...</td>\n",
       "      <td>198</td>\n",
       "      <td>[hey, deserved, coming, feel, bad]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  community  \\\n",
       "1  9.290107e+08  what? i mean i get where you're comming from b...          9   \n",
       "2  9.290107e+08  chaggot? more like that fucking weirdo who sta...          9   \n",
       "5  9.290107e+08  isn't that the opposite of incels? she has so ...        130   \n",
       "7  9.290107e+08  what's so bad about hiring incels anyways? wou...        130   \n",
       "8  9.290107e+08  hey she deserved what's coming for her. can't ...        198   \n",
       "\n",
       "                                              tokens  n_tokens  \n",
       "1   [mean, comming, really, inspire, respect, chads]         6  \n",
       "2  [chaggot, fucking, weirdo, stalks, grindr, gay...        21  \n",
       "5  [opposite, incels, manny, options, settle, cre...         7  \n",
       "7  [bad, hiring, incels, anyways, productive, acc...        13  \n",
       "8                 [hey, deserved, coming, feel, bad]         5  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize and clean text data\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# extend basic english stopwords with slang terms\n",
    "extra_stops = {\n",
    "    'lol', 'xd', 'haha', 'hahaah', 'omg', 'u', 'ur', 'im', 'ive', 'idk', \n",
    "    'dont', 'cant', 'wont', 'aint', 'ya', 'tho', 'tho', 'nah', 'btw', \n",
    "    'like', 'yeah', 'yep', 'ok', 'okay', 'pls', 'please', 'get'\n",
    "}\n",
    "stop_words.update(extra_stops)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # decode HTML entities: &amp; → &, &#x200B; → zero-width space, etc.\n",
    "    text = html.unescape(text)\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
    "    # keep only letters and spaces\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    # tokenize by whitespace\n",
    "    tokens = text.split()\n",
    "    # remove stopwords and very short tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    if len(tokens) == 1 and tokens[0] in {\"removed\", \"deleted\"}:\n",
    "        return []\n",
    "    return tokens\n",
    "\n",
    "df_filtered[\"tokens\"] = df_filtered[\"text\"].apply(preprocess_text)\n",
    "df_filtered[\"n_tokens\"] = df_filtered[\"tokens\"].apply(len)\n",
    "\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4075dcf8",
   "metadata": {},
   "source": [
    "# Frequent items and the A-priori algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27146132",
   "metadata": {},
   "source": [
    "We process the tokenized posts by identifying the amount of unique tokens for each community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4f9c689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community 9:\n",
      "  Total tokens: 1,342,981\n",
      "  Unique tokens: 49,183\n",
      "Community 130:\n",
      "  Total tokens: 1,868,461\n",
      "  Unique tokens: 52,898\n",
      "Community 81:\n",
      "  Total tokens: 868,501\n",
      "  Unique tokens: 38,103\n",
      "Community 94:\n",
      "  Total tokens: 762,842\n",
      "  Unique tokens: 31,028\n",
      "Community 204:\n",
      "  Total tokens: 681,874\n",
      "  Unique tokens: 27,089\n",
      "Community 198:\n",
      "  Total tokens: 253,499\n",
      "  Unique tokens: 19,440\n"
     ]
    }
   ],
   "source": [
    "# build token statistics for each of the top 6 communities\n",
    "\n",
    "community_token_stats = {}\n",
    "\n",
    "for community_id in top_6_communities:\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == community_id]\n",
    "\n",
    "    # flatten all tokens for this community\n",
    "    all_tokens = []\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    unique_tokens = set(all_tokens)\n",
    "\n",
    "    community_token_stats[community_id] = {\n",
    "        \"n_tokens\": len(all_tokens),\n",
    "        \"n_unique_tokens\": len(unique_tokens),       \n",
    "        \"unique_tokens\": unique_tokens \n",
    "    }\n",
    "\n",
    "\n",
    "for cid in top_6_communities:\n",
    "    print(f\"Community {cid}:\")\n",
    "    print(f\"  Total tokens: {community_token_stats[cid]['n_tokens']:,}\")\n",
    "    print(f\"  Unique tokens: {community_token_stats[cid]['n_unique_tokens']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eca728",
   "metadata": {},
   "source": [
    "## First pass of the A-priori algorithm\n",
    "\n",
    "In the first pass of the A-priori algoritm, we initialize a dataframe for each of the communities. In this dataframe, we will store each of the unique tokens found previously, assign them each an integer from 0 to n-1 (number of unique tokens), and count how many baskets (posts) the item (token) appears in. It is important to note that we do not count the total occurrence of the token but only the amount of posts it appears in. In Mining of Massive Datasets, Section 6.2.2, the first pass is described as labeling integers 1 to n, but to keep it within the python framework, we label 0 to n-1 and mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f323c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori_tables = {}\n",
    "\n",
    "for cid in top_6_communities:\n",
    "    # get df for this community\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "    unique_tokens = list(community_token_stats[cid][\"unique_tokens\"])\n",
    "\n",
    "    # apriori table\n",
    "    df_apriori = pd.DataFrame({\n",
    "        \"word\": unique_tokens,\n",
    "        \"integer\": range(len(unique_tokens))\n",
    "    })\n",
    "\n",
    "    # give each word an integer from 0 to n-1\n",
    "    word_to_int = dict(zip(df_apriori[\"word\"], df_apriori[\"integer\"]))\n",
    "\n",
    "    # count posts that contain each token\n",
    "    array_of_counts = np.zeros(len(unique_tokens), dtype=int)\n",
    "\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        for token in set(tokens):             \n",
    "            array_of_counts[word_to_int[token]] += 1\n",
    "\n",
    "    df_apriori[\"count\"] = array_of_counts\n",
    "\n",
    "    # save in dict\n",
    "    apriori_tables[cid] = df_apriori\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "692f4753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 9\n",
      "        word  integer  count\n",
      "43221   chad    43221  70535\n",
      "48152  chads    48152  18492\n",
      "14197  women    14197  12601\n",
      "6760   would     6760   9553\n",
      "22080   even    22080   8937\n",
      "\n",
      "Community 130\n",
      "         word  integer  count\n",
      "20385   incel    20385  52750\n",
      "37020  incels    37020  45845\n",
      "15140   women    15140  13515\n",
      "43705  people    43705  13499\n",
      "7180    would     7180  10744\n",
      "\n",
      "Community 81\n",
      "            word  integer  count\n",
      "24123      white    24123  19078\n",
      "10936      women    10936  10160\n",
      "19023      black    19023   9942\n",
      "28132      asian    28132   8863\n",
      "22837  blackpill    22837   7791\n",
      "\n",
      "Community 94\n",
      "         word  integer  count\n",
      "21406  height    21406  16448\n",
      "13047   short    13047  11274\n",
      "22099    tall    22099   8733\n",
      "8946    women     8946   7709\n",
      "19474     men    19474   5939\n",
      "\n",
      "Community 204\n",
      "             word  integer  count\n",
      "7645         ugly     7645  16418\n",
      "8758        looks     8758   8750\n",
      "22466      people    22466   7296\n",
      "7805        women     7805   6437\n",
      "7080   attractive     7080   5160\n",
      "\n",
      "Community 198\n",
      "        word  integer  count\n",
      "10641  would    10641   2320\n",
      "15557   even    15557   1628\n",
      "2059    know     2059   1537\n",
      "9265    fuck     9265   1343\n",
      "8486   think     8486   1264\n"
     ]
    }
   ],
   "source": [
    "# print top 5 tokens by count for each community\n",
    "for cid, df_apriori in apriori_tables.items():\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "    print(df_apriori.sort_values(by=\"count\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5646a40",
   "metadata": {},
   "source": [
    "## Between the passes of A-priori\n",
    "\n",
    "We create frequency tables where we assign each word an integer from 1-m, where m = number of frequent singletons (words), if the support of the word => 1%. In other words, it must appear in 1% or more of the baskets. If the word is not frequent, we assign it 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2d554ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 9 frequent items:\n",
      "            word  integer  count  freq_integer\n",
      "19     attracted       19    979             1\n",
      "139        stacy      139   5953             2\n",
      "166         guys      166   4441             3\n",
      "455        point      455   1557             4\n",
      "473        means      473   1060             5\n",
      "...          ...      ...    ...           ...\n",
      "48298        see    48298   3338           195\n",
      "48466       much    48466   2912           196\n",
      "48640    thought    48640   1075           197\n",
      "48713       call    48713   1179           198\n",
      "48717        way    48717   3022           199\n",
      "\n",
      "[199 rows x 4 columns]\n",
      "\n",
      "Community 130 frequent items:\n",
      "             word  integer  count  freq_integer\n",
      "81        exactly       81   1139             1\n",
      "151          guys      151   5180             2\n",
      "454         point      454   3354             3\n",
      "479         means      479   1832             4\n",
      "645    understand      645   2837             5\n",
      "...           ...      ...    ...           ...\n",
      "52401        call    52401   1808           294\n",
      "52403         way    52403   5282           295\n",
      "52808         two    52808   1227           296\n",
      "52873       posts    52873   2217           297\n",
      "52894       nnyou    52894   1308           298\n",
      "\n",
      "[298 rows x 4 columns]\n",
      "\n",
      "Community 81 frequent items:\n",
      "             word  integer  count  freq_integer\n",
      "18      attracted       18    640             1\n",
      "122          guys      122   4432             2\n",
      "343         point      343    995             3\n",
      "359         means      359    563             4\n",
      "485    understand      485    661             5\n",
      "...           ...      ...    ...           ...\n",
      "37398         see    37398   2753           220\n",
      "37552        much    37552   2152           221\n",
      "37698     thought    37698    673           222\n",
      "37744        call    37744    744           223\n",
      "37747         way    37747   1836           224\n",
      "\n",
      "[224 rows x 4 columns]\n",
      "\n",
      "Community 94 frequent items:\n",
      "             word  integer  count  freq_integer\n",
      "18      attracted       18    652             1\n",
      "111          guys      111   5027             2\n",
      "299         point      299   1252             3\n",
      "314         means      314    649             4\n",
      "404    understand      404    608             5\n",
      "...           ...      ...    ...           ...\n",
      "30473         bit    30473    568           201\n",
      "30575        much    30575   2333           202\n",
      "30702     thought    30702    628           203\n",
      "30748         way    30748   1994           204\n",
      "30974         two    30974    654           205\n",
      "\n",
      "[205 rows x 4 columns]\n",
      "\n",
      "Community 204 frequent items:\n",
      "            word  integer  count  freq_integer\n",
      "15     attracted       15   1049             1\n",
      "25          play       25    386             2\n",
      "49       exactly       49    418             3\n",
      "103         guys      103   3746             4\n",
      "280        point      280   1242             5\n",
      "...          ...      ...    ...           ...\n",
      "26832       call    26832    346           314\n",
      "26833        way    26833   2182           315\n",
      "26956     easier    26956    401           316\n",
      "27039        two    27039    445           317\n",
      "27086      nnyou    27086    474           318\n",
      "\n",
      "[318 rows x 4 columns]\n",
      "\n",
      "Community 198 frequent items:\n",
      "          word  integer  count  freq_integer\n",
      "8      looking        8    358             1\n",
      "13        cunt       13    309             2\n",
      "122    friends      122    594             3\n",
      "246       guys      246    583             4\n",
      "681      point      681    318             5\n",
      "...        ...      ...    ...           ...\n",
      "18985     care    18985    287           186\n",
      "19023   anyone    19023    252           187\n",
      "19241   around    19241    424           188\n",
      "19324      two    19324    260           189\n",
      "19413   things    19413    428           190\n",
      "\n",
      "[190 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "freq_tables = {}\n",
    "\n",
    "threshold_factor = 0.01  \n",
    "\n",
    "for cid, df_apriori in apriori_tables.items():\n",
    "\n",
    "    # threshold is 1% of posts in that community\n",
    "    threshold = threshold_factor * len(df_filtered[df_filtered[\"community\"] == cid])\n",
    "\n",
    "    frequent_map = np.zeros(len(df_apriori), dtype=int)\n",
    "    new_id = 1\n",
    "\n",
    "    for old_id, count in enumerate(df_apriori['count']):\n",
    "        if count >= threshold:\n",
    "            frequent_map[old_id] = new_id\n",
    "            new_id += 1\n",
    "        else:\n",
    "            frequent_map[old_id] = 0\n",
    "\n",
    "    # add freq_integer column\n",
    "    df_apriori['freq_integer'] = frequent_map\n",
    "\n",
    "    # store only frequent items in new dictionary\n",
    "    df_freq = df_apriori[df_apriori['freq_integer'] != 0].copy()\n",
    "    freq_tables[cid] = df_freq\n",
    "\n",
    "    print(f\"\\nCommunity {cid} frequent items:\")\n",
    "    print(df_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d38f5ea",
   "metadata": {},
   "source": [
    "## Second pass of the A-priori algorithm\n",
    "\n",
    "For the second pass, we first find all pairs of frequent words from the previous dataframes. We then create pairs of those, making sure to remove duplicates. We apply the support threshold of 1% here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cab3263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 9\n",
      "Top pairs:\n",
      "          item_set  count\n",
      "64   (chad, women)   9652\n",
      "338  (chad, would)   7643\n",
      "56    (chad, even)   6866\n",
      "339   (chad, fuck)   5460\n",
      "928   (chad, want)   5359\n",
      "Total pairs where Support(I) => s of 0.01: 211\n",
      "\n",
      "Community 130\n",
      "Top pairs:\n",
      "              item_set  count\n",
      "848   (incels, people)   8725\n",
      "131    (incels, women)   8261\n",
      "998    (incel, incels)   8194\n",
      "1269    (incel, women)   7332\n",
      "533    (incel, people)   7214\n",
      "Total pairs where Support(I) => s of 0.01: 584\n",
      "\n",
      "Community 81\n",
      "Top pairs:\n",
      "            item_set  count\n",
      "722   (white, women)   5521\n",
      "724   (asian, white)   3973\n",
      "1280  (black, white)   3854\n",
      "788     (men, white)   3565\n",
      "784     (men, women)   3446\n",
      "Total pairs where Support(I) => s of 0.01: 226\n",
      "\n",
      "Community 94\n",
      "Top pairs:\n",
      "            item_set  count\n",
      "15    (face, height)   3373\n",
      "237  (height, women)   3338\n",
      "146   (short, women)   3123\n",
      "32      (men, women)   3052\n",
      "154     (men, short)   3052\n",
      "Total pairs where Support(I) => s of 0.01: 326\n",
      "\n",
      "Community 204\n",
      "Top pairs:\n",
      "             item_set  count\n",
      "348    (people, ugly)   3647\n",
      "1       (ugly, women)   3318\n",
      "2149      (men, ugly)   2581\n",
      "475      (men, women)   2540\n",
      "2032  (looks, people)   2304\n",
      "Total pairs where Support(I) => s of 0.01: 889\n",
      "\n",
      "Community 198\n",
      "Top pairs:\n",
      "            item_set  count\n",
      "86     (even, would)    306\n",
      "1074    (even, know)    276\n",
      "114    (fuck, would)    275\n",
      "1213   (know, would)    273\n",
      "177   (think, would)    265\n",
      "Total pairs where Support(I) => s of 0.01: 10\n"
     ]
    }
   ],
   "source": [
    "pair_tables = {}   # store results for each community\n",
    "\n",
    "for cid in top_6_communities:\n",
    "\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "\n",
    "    # pull posts for this community\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "    N = len(df_comm)\n",
    "\n",
    "    # fetch frequent 1-itemset for this community\n",
    "    df_freq = freq_tables[cid]\n",
    "    frequent_words_set = set(df_freq[\"word\"])\n",
    "\n",
    "    # counter for all frequent pairs\n",
    "    pair_counter = Counter()\n",
    "\n",
    "    # iterate over all posts\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        # keep only frequent tokens\n",
    "        frequent_tokens = [t for t in tokens if t in frequent_words_set]\n",
    "\n",
    "        # deduplicate within a post\n",
    "        unique_tokens = set(frequent_tokens)\n",
    "\n",
    "        # count each 2-item combination in this post\n",
    "        for pair in combinations(unique_tokens, 2):\n",
    "            pair_counter[tuple(sorted(pair))] += 1\n",
    "\n",
    "    # convert counter → dataframe\n",
    "    df_pairs = pd.DataFrame(pair_counter.items(), columns=[\"item_set\", \"count\"])\n",
    "\n",
    "    # threshold for frequent 2-itemsets (1% of posts)\n",
    "    threshold = math.ceil(0.01 * N)\n",
    "    df_pairs = df_pairs[df_pairs[\"count\"] >= threshold]\n",
    "\n",
    "    # store\n",
    "    pair_tables[cid] = df_pairs\n",
    "\n",
    "    # print summary\n",
    "    print(\"Top pairs:\")\n",
    "    print(df_pairs.sort_values(by=\"count\", ascending=False).head())\n",
    "    print(\"Total pairs where Support(I) => s of 0.01:\", len(df_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d68995eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Community 9 ===\n",
      "Top pair: ('chad', 'women') | Posts containing both: 9652\n",
      "Top context words for chad:\n",
      "       count\n",
      "want    1365\n",
      "would   1067\n",
      "men      962\n",
      "even     959\n",
      "fuck     839\n",
      "Top context words for women:\n",
      "       count\n",
      "men     1979\n",
      "want    1512\n",
      "would   1045\n",
      "even     967\n",
      "chads    796\n",
      "\n",
      "=== Community 130 ===\n",
      "Top pair: ('incels', 'people') | Posts containing both: 8725\n",
      "Top context words for incels:\n",
      "       count\n",
      "think   1210\n",
      "women   1158\n",
      "incel    920\n",
      "even     722\n",
      "want     713\n",
      "Top context words for people:\n",
      "       count\n",
      "think   1182\n",
      "incel    927\n",
      "women    885\n",
      "want     853\n",
      "even     713\n",
      "\n",
      "=== Community 81 ===\n",
      "Top pair: ('white', 'women') | Posts containing both: 5521\n",
      "Top context words for white:\n",
      "       count\n",
      "men     2849\n",
      "asian   1848\n",
      "black   1791\n",
      "guys    1236\n",
      "guy      680\n",
      "Top context words for women:\n",
      "       count\n",
      "men     2534\n",
      "asian   2039\n",
      "black   1949\n",
      "guys     777\n",
      "race     750\n",
      "\n",
      "=== Community 94 ===\n",
      "Top pair: ('face', 'height') | Posts containing both: 3373\n",
      "Top context words for face:\n",
      "           count\n",
      "good         386\n",
      "average      343\n",
      "tall         293\n",
      "frame        293\n",
      "important    291\n",
      "Top context words for height:\n",
      "           count\n",
      "average      380\n",
      "women        355\n",
      "important    305\n",
      "short        278\n",
      "good         276\n",
      "\n",
      "=== Community 204 ===\n",
      "Top pair: ('people', 'ugly') | Posts containing both: 3647\n",
      "Top context words for people:\n",
      "            count\n",
      "attractive    621\n",
      "think         444\n",
      "looks         417\n",
      "good          401\n",
      "know          373\n",
      "Top context words for ugly:\n",
      "       count\n",
      "think    408\n",
      "women    393\n",
      "even     382\n",
      "men      375\n",
      "know     372\n",
      "\n",
      "=== Community 198 ===\n",
      "Top pair: ('even', 'would') | Posts containing both: 306\n",
      "Top context words for even:\n",
      "        count\n",
      "think      30\n",
      "know       25\n",
      "though     24\n",
      "fuck       22\n",
      "never      21\n",
      "Top context words for would:\n",
      "           count\n",
      "think         47\n",
      "never         34\n",
      "fuck          25\n",
      "say           24\n",
      "something     22\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "WINDOW = 5\n",
    "context_results_top_pair = {}  # store results per community\n",
    "\n",
    "for cid in top_6_communities:\n",
    "\n",
    "    print(f\"\\n=== Community {cid} ===\")\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "    N = len(df_comm)\n",
    "\n",
    "    # Get the top frequency pair (highest count)\n",
    "    df_pairs = pair_tables[cid]\n",
    "    if df_pairs.empty:\n",
    "        print(\"No pairs found for this community.\")\n",
    "        continue\n",
    "\n",
    "    top_pair = df_pairs.sort_values(\"count\", ascending=False).iloc[0][\"item_set\"]\n",
    "    word1, word2 = top_pair\n",
    "\n",
    "    context_word1 = Counter()\n",
    "    context_word2 = Counter()\n",
    "    valid_posts = 0\n",
    "\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        if word1 in tokens and word2 in tokens:\n",
    "            valid_posts += 1\n",
    "\n",
    "            # positions of each word in the post\n",
    "            pos1 = [i for i, t in enumerate(tokens) if t == word1]\n",
    "            pos2 = [i for i, t in enumerate(tokens) if t == word2]\n",
    "\n",
    "            # window context for word1\n",
    "            for p in pos1:\n",
    "                start = max(0, p - WINDOW)\n",
    "                end   = p + WINDOW + 1\n",
    "                local = set(tokens[start:end]) - {word1, word2}\n",
    "                for w in local:\n",
    "                    context_word1[w] += 1\n",
    "\n",
    "            # window context for word2\n",
    "            for p in pos2:\n",
    "                start = max(0, p - WINDOW)\n",
    "                end   = p + WINDOW + 1\n",
    "                local = set(tokens[start:end]) - {word1, word2}\n",
    "                for w in local:\n",
    "                    context_word2[w] += 1\n",
    "\n",
    "    # convert counters to DataFrames\n",
    "    df_w1 = pd.DataFrame.from_dict(context_word1, orient=\"index\", columns=[\"count\"]).sort_values(\"count\", ascending=False)\n",
    "    df_w2 = pd.DataFrame.from_dict(context_word2, orient=\"index\", columns=[\"count\"]).sort_values(\"count\", ascending=False)\n",
    "\n",
    "    # store results\n",
    "    context_results_top_pair[cid] = {\n",
    "        \"pair\": top_pair,\n",
    "        word1: df_w1,\n",
    "        word2: df_w2,\n",
    "        \"valid_posts\": valid_posts\n",
    "    }\n",
    "\n",
    "    print(f\"Top pair: {top_pair} | Posts containing both: {valid_posts}\")\n",
    "    print(f\"Top context words for {word1}:\")\n",
    "    print(df_w1.head())\n",
    "    print(f\"Top context words for {word2}:\")\n",
    "    print(df_w2.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b44fc2",
   "metadata": {},
   "source": [
    "# A-priori using library\n",
    "\n",
    "To further validate the above results, we also implemented the A-priori algorithm using mlxtend. We find that the results of using the mlxtend framwork are congruent with the results found by implementing the A-priori algorithm as described in Mining of Massive Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e87124af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 199\n",
      "\n",
      "Community 130\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 298\n",
      "\n",
      "Community 81\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 224\n",
      "\n",
      "Community 94\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 205\n",
      "\n",
      "Community 204\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 318\n",
      "\n",
      "Community 198\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 190\n"
     ]
    }
   ],
   "source": [
    "te = TransactionEncoder()\n",
    "encoded_tables = {} \n",
    "\n",
    "for cid in top_6_communities:\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "    \n",
    "    freq_words = set(freq_tables[cid][\"word\"])\n",
    "    \n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "    transactions = [\n",
    "        [t for t in tokens if t in freq_words]\n",
    "        for tokens in df_comm[\"tokens\"]\n",
    "    ]\n",
    "\n",
    "    te_array = te.fit(transactions).transform(transactions)\n",
    "    df_encoded = pd.DataFrame(te_array, columns=te.columns_)\n",
    "    \n",
    "    encoded_tables[cid] = df_encoded\n",
    "    \n",
    "    print(f\"Number of frequent items (singletons) with Support(I) => s of 0.01: {df_encoded.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ac9b3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 9\n",
      "Number of frequent 2-itemsets: 211\n",
      "Top 5 pairs:\n",
      "      support       itemsets\n",
      "344  0.101034  (chad, women)\n",
      "347  0.080005  (chad, would)\n",
      "231  0.071871   (chad, even)\n",
      "247  0.057154   (chad, fuck)\n",
      "338  0.056096   (chad, want)\n",
      "\n",
      "Community 130\n",
      "Number of frequent 2-itemsets: 584\n",
      "Top 5 pairs:\n",
      "      support          itemsets\n",
      "647  0.093430  (people, incels)\n",
      "708  0.088462   (incels, women)\n",
      "498  0.087744   (incels, incel)\n",
      "599  0.078514    (women, incel)\n",
      "538  0.077250   (people, incel)\n",
      "\n",
      "Community 81\n",
      "Number of frequent 2-itemsets: 226\n",
      "Top 5 pairs:\n",
      "      support        itemsets\n",
      "445  0.101364  (white, women)\n",
      "251  0.072943  (asian, white)\n",
      "286  0.070758  (black, white)\n",
      "378  0.065452    (white, men)\n",
      "379  0.063268    (men, women)\n",
      "\n",
      "Community 94\n",
      "Number of frequent 2-itemsets: 326\n",
      "Top 5 pairs:\n",
      "      support         itemsets\n",
      "270  0.065359   (face, height)\n",
      "395  0.064681  (height, women)\n",
      "500  0.060515   (short, women)\n",
      "445  0.059139     (men, women)\n",
      "436  0.059139     (short, men)\n",
      "\n",
      "Community 204\n",
      "Number of frequent 2-itemsets: 889\n",
      "Top 5 pairs:\n",
      "       support         itemsets\n",
      "1057  0.112112   (people, ugly)\n",
      "1186  0.101998    (ugly, women)\n",
      "967   0.079342      (ugly, men)\n",
      "973   0.078082     (men, women)\n",
      "862   0.070827  (looks, people)\n",
      "\n",
      "Community 198\n",
      "Number of frequent 2-itemsets: 10\n",
      "Top 5 pairs:\n",
      "      support        itemsets\n",
      "192  0.014025   (would, even)\n",
      "190  0.012650    (know, even)\n",
      "193  0.012604   (would, fuck)\n",
      "196  0.012513   (would, know)\n",
      "198  0.012146  (think, would)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "frequent_2_itemsets_by_community = {}\n",
    "\n",
    "for cid in top_6_communities:\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "    \n",
    "    df_encoded = encoded_tables[cid] \n",
    "    \n",
    "    frequent_itemsets = apriori(df_encoded, min_support=0.01, use_colnames=True)\n",
    "    \n",
    "    # filter to only 2-itemsets\n",
    "    frequent_2_itemsets = frequent_itemsets[\n",
    "        frequent_itemsets['itemsets'].apply(lambda x: len(x) == 2)\n",
    "    ].copy()\n",
    "    \n",
    "    frequent_2_itemsets_by_community[cid] = frequent_2_itemsets\n",
    "    \n",
    "    print(\"Number of frequent 2-itemsets:\", len(frequent_2_itemsets))\n",
    "    print(\"Top 5 pairs:\")\n",
    "    print(frequent_2_itemsets.sort_values(by=\"support\", ascending=False).head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
