{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ea257d",
   "metadata": {},
   "source": [
    "# Frequent items analysis of reddit communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4c2c536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/livdreyerjohansen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import networkx as nx\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "import html\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import math\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88d1fd7",
   "metadata": {},
   "source": [
    "## Dataload and Cleaning\n",
    "\n",
    "We analysize what frequent items we may see in the reddit clusters found in 05_SemanticClustering.ipynb. The output of the analysis performed in aforementioned notebook is organized in a csv file with columns: id (user), text, and community.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d6bbe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('post_clusters_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cf8dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df_raw.rename(columns={\"label\": \"community\"})\n",
    "df_raw = df_raw.rename(columns={\"user\": \"id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de689bc",
   "metadata": {},
   "source": [
    "To ensure we do not include posts that are either deleted (\"\\[deleted\\]\") or removed (\"\\[removed\\]\"), both basic reddit features that happen independently of what forum you are in, we remove both. Furthermore, we remove each post that was removed by a bot, which is clear in the text which the bot uses to explain why a post or comment is deleted. We then construct a dataframe with all posts and their community (and original poster (OP in reddit linguistics))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03a8126f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             id                                               text  community\n",
      "0  9.290107e+08  hey charisma helps. everybody wants to sleep w...        214\n",
      "1  9.290107e+08  aren't the jedis not really good guys though? ...        156\n",
      "2  9.290107e+08  wait but ferb is the better looking one with a...         95\n",
      "3  9.290107e+08  what? i mean i get where you're comming from b...         24\n",
      "4  9.290107e+08         hey, i guess time to try out something new        146\n",
      "\n",
      "Number of original posts:\n",
      "1,119,261\n",
      "Number of removed posts:\n",
      "0\n",
      "Number of posts in dataframe:\n",
      "1,119,261\n"
     ]
    }
   ],
   "source": [
    "# Expected columns in CSV:\n",
    "# id, text, community\n",
    "# (tell me if they differ)\n",
    "df_raw.drop(df_raw[df_raw[\"community\"] == -1].index, inplace=True)\n",
    "\n",
    "rows = []\n",
    "rows_count = 0\n",
    "allowed_rows = 0\n",
    "\n",
    "for _, r in df_raw.iterrows():\n",
    "    rows_count += 1\n",
    "    post = r[\"text\"]\n",
    "\n",
    "    # Skip empty, deleted/removed posts, or posts containing the bot line\n",
    "    if post and post not in ['[deleted]', '[removed]'] and \\\n",
    "       \"*I am a bot, and this action was performed automatically.\" not in post:\n",
    "\n",
    "        allowed_rows += 1\n",
    "\n",
    "        rows.append({\n",
    "            \"id\": r[\"id\"],\n",
    "            \"text\": post,\n",
    "            \"community\": r[\"community\"]\n",
    "        })\n",
    "\n",
    "# Build the dataframe exactly like before\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nNumber of original posts:\")\n",
    "print(f\"{rows_count:,}\")\n",
    "print(\"Number of removed posts:\")\n",
    "print(f\"{rows_count-allowed_rows:,}\")\n",
    "print(\"Number of posts in dataframe:\")\n",
    "print(f\"{allowed_rows:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dbf9c4",
   "metadata": {},
   "source": [
    "We identify the top-6 largest communities in terms of posts to continue working with only them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f4063e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = df['community'].unique().tolist()\n",
    "communities_dict = dict.fromkeys(communities, 0)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    communities_dict[row['community']] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f165e4",
   "metadata": {},
   "source": [
    "Filter the dataframe to only contain posts from top-6 communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c521f85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 6 communities by number of posts (posts, community): [(109445, 58), (88639, 192), (79247, 24), (43864, 214), (35248, 100), (30420, 210)]\n",
      "Number of posts in top 6 communities: 386,863\n",
      "Number of posts removed based on non identity in top 6: 732,398\n"
     ]
    }
   ],
   "source": [
    "comms_list = list(sorted( ((v,k) for k,v in communities_dict.items()), reverse=True))\n",
    "comms_list = comms_list[:6]\n",
    "top_6_communities = [item[1] for item in comms_list]\n",
    "\n",
    "df_filtered = df[df['community'].isin(top_6_communities)].copy()\n",
    "\n",
    "print(\"\\nTop 6 communities by number of posts (posts, community):\", comms_list)\n",
    "\n",
    "print(f\"Number of posts in top 6 communities: {len(df_filtered):,}\")\n",
    "\n",
    "print(f\"Number of posts removed based on non identity in top 6: {(len(df) - len(df_filtered)):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1289c493",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "We filter out parts of post that we deem have little semantic value. We aim to find frequent items and frequent itemsets (item pairs), and would assume that stop words regularly occur in more than 1% of baskets. As we are working with online fora, we chose to add certain slang-terms as stop words. We furthermore remove:\n",
    "\n",
    "- html entities\n",
    "- URL's\n",
    "- non-text artifacts (such as \"/\", \"?\", \"!\" etc.)\n",
    "- remaining \"removed\" and \"deleted\" artifacts that were not removed in the previous code block due to the way the post was loaded\n",
    "- short words (length of 2 or less)\n",
    "\n",
    "Additionally, we make all words lowercase to steamline and tokenize by word (meaning each word will be its own token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "422cbe8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "community",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tokens",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "n_tokens",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "8e285ed7-8dc4-4b43-8cde-934c8b0c3a97",
       "rows": [
        [
         "0",
         "929010695.928077",
         "hey charisma helps. everybody wants to sleep with pretty people. doesn't mean they enjoy dating them",
         "214",
         "['hey', 'charisma', 'helps', 'everybody', 'wants', 'sleep', 'pretty', 'people', 'mean', 'enjoy', 'dating']",
         "11"
        ],
        [
         "3",
         "929010695.928077",
         "what? i mean i get where you're comming from but he doesn't really inspire respect like chads do",
         "24",
         "['mean', 'comming', 'really', 'inspire', 'respect', 'chads']",
         "6"
        ],
        [
         "5",
         "929010695.928077",
         "really? i did the same with a girl friend and got a few laughs. and i am one ugly mother fucker",
         "214",
         "['really', 'girl', 'friend', 'got', 'laughs', 'one', 'ugly', 'mother', 'fucker']",
         "9"
        ],
        [
         "7",
         "929010695.928077",
         "chaggot? more like that fucking weirdo who stalks you on grindr. gay chads are basically normal chad. thats why you shouldnt have a crush kn str8 guys. it doesnt matter if he's gay, he's still way out of your leaguenn",
         "24",
         "['chaggot', 'fucking', 'weirdo', 'stalks', 'grindr', 'gay', 'chads', 'basically', 'normal', 'chad', 'thats', 'shouldnt', 'crush', 'str', 'guys', 'doesnt', 'matter', 'gay', 'still', 'way', 'leaguenn']",
         "21"
        ],
        [
         "9",
         "929010695.928077",
         "if it makes you feel any better, a good body can get you sex just fine. relationship is another story, but they can hate fuck you or put a bag over your head or something.",
         "214",
         "['makes', 'feel', 'better', 'good', 'body', 'sex', 'fine', 'relationship', 'another', 'story', 'hate', 'fuck', 'put', 'bag', 'head', 'something']",
         "16"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>community</th>\n",
       "      <th>tokens</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.290107e+08</td>\n",
       "      <td>hey charisma helps. everybody wants to sleep w...</td>\n",
       "      <td>214</td>\n",
       "      <td>[hey, charisma, helps, everybody, wants, sleep...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.290107e+08</td>\n",
       "      <td>what? i mean i get where you're comming from b...</td>\n",
       "      <td>24</td>\n",
       "      <td>[mean, comming, really, inspire, respect, chads]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.290107e+08</td>\n",
       "      <td>really? i did the same with a girl friend and ...</td>\n",
       "      <td>214</td>\n",
       "      <td>[really, girl, friend, got, laughs, one, ugly,...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9.290107e+08</td>\n",
       "      <td>chaggot? more like that fucking weirdo who sta...</td>\n",
       "      <td>24</td>\n",
       "      <td>[chaggot, fucking, weirdo, stalks, grindr, gay...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.290107e+08</td>\n",
       "      <td>if it makes you feel any better, a good body c...</td>\n",
       "      <td>214</td>\n",
       "      <td>[makes, feel, better, good, body, sex, fine, r...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  community  \\\n",
       "0  9.290107e+08  hey charisma helps. everybody wants to sleep w...        214   \n",
       "3  9.290107e+08  what? i mean i get where you're comming from b...         24   \n",
       "5  9.290107e+08  really? i did the same with a girl friend and ...        214   \n",
       "7  9.290107e+08  chaggot? more like that fucking weirdo who sta...         24   \n",
       "9  9.290107e+08  if it makes you feel any better, a good body c...        214   \n",
       "\n",
       "                                              tokens  n_tokens  \n",
       "0  [hey, charisma, helps, everybody, wants, sleep...        11  \n",
       "3   [mean, comming, really, inspire, respect, chads]         6  \n",
       "5  [really, girl, friend, got, laughs, one, ugly,...         9  \n",
       "7  [chaggot, fucking, weirdo, stalks, grindr, gay...        21  \n",
       "9  [makes, feel, better, good, body, sex, fine, r...        16  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize and clean text data\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# extend basic english stopwords with slang terms\n",
    "extra_stops = {\n",
    "    'lol', 'xd', 'haha', 'hahaah', 'omg', 'u', 'ur', 'im', 'ive', 'idk', \n",
    "    'dont', 'cant', 'wont', 'aint', 'ya', 'tho', 'tho', 'nah', 'btw', \n",
    "    'like', 'yeah', 'yep', 'ok', 'okay', 'pls', 'please', 'get'\n",
    "}\n",
    "stop_words.update(extra_stops)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # decode HTML entities: &amp; → &, &#x200B; → zero-width space, etc.\n",
    "    text = html.unescape(text)\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
    "    # keep only letters and spaces\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    # tokenize by whitespace\n",
    "    tokens = text.split()\n",
    "    # remove stopwords and very short tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    if len(tokens) == 1 and tokens[0] in {\"removed\", \"deleted\"}:\n",
    "        return []\n",
    "    return tokens\n",
    "\n",
    "df_filtered[\"tokens\"] = df_filtered[\"text\"].apply(preprocess_text)\n",
    "df_filtered[\"n_tokens\"] = df_filtered[\"tokens\"].apply(len)\n",
    "\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4075dcf8",
   "metadata": {},
   "source": [
    "# Frequent items and the A-priori algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27146132",
   "metadata": {},
   "source": [
    "We process the tokenized posts by identifying the amount of unique tokens for each community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f9c689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community 58:\n",
      "  Total tokens: 1,606,779\n",
      "  Unique tokens: 57,962\n",
      "Community 192:\n",
      "  Total tokens: 1,906,858\n",
      "  Unique tokens: 52,195\n",
      "Community 24:\n",
      "  Total tokens: 1,038,517\n",
      "  Unique tokens: 42,537\n",
      "Community 214:\n",
      "  Total tokens: 1,024,180\n",
      "  Unique tokens: 33,030\n",
      "Community 100:\n",
      "  Total tokens: 533,604\n",
      "  Unique tokens: 24,716\n",
      "Community 210:\n",
      "  Total tokens: 575,470\n",
      "  Unique tokens: 30,720\n"
     ]
    }
   ],
   "source": [
    "# build token statistics for each of the top 6 communities\n",
    "\n",
    "community_token_stats = {}\n",
    "\n",
    "for community_id in top_6_communities:\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == community_id]\n",
    "\n",
    "    # flatten all tokens for this community\n",
    "    all_tokens = []\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    unique_tokens = set(all_tokens)\n",
    "\n",
    "    community_token_stats[community_id] = {\n",
    "        \"n_tokens\": len(all_tokens),\n",
    "        \"n_unique_tokens\": len(unique_tokens),       \n",
    "        \"unique_tokens\": unique_tokens \n",
    "    }\n",
    "\n",
    "\n",
    "for cid in top_6_communities:\n",
    "    print(f\"Community {cid}:\")\n",
    "    print(f\"  Total tokens: {community_token_stats[cid]['n_tokens']:,}\")\n",
    "    print(f\"  Unique tokens: {community_token_stats[cid]['n_unique_tokens']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eca728",
   "metadata": {},
   "source": [
    "## First pass of the A-priori algorithm\n",
    "\n",
    "In the first pass of the A-priori algoritm, we initialize a dataframe for each of the communities. In this dataframe, we will store each of the unique tokens found previously, assign them each an integer from 0 to n-1 (number of unique tokens), and count how many baskets (posts) the item (token) appears in. It is important to note that we do not count the total occurrence of the token but only the amount of posts it appears in. In Mining of Massive Datasets, Section 6.2.2, the first pass is described as labeling integers 1 to n, but to keep it within the python framework, we label 0 to n-1 and mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f323c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori_tables = {}\n",
    "\n",
    "for cid in top_6_communities:\n",
    "    # get df for this community\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "    unique_tokens = list(community_token_stats[cid][\"unique_tokens\"])\n",
    "\n",
    "    # apriori table\n",
    "    df_apriori = pd.DataFrame({\n",
    "        \"word\": unique_tokens,\n",
    "        \"integer\": range(len(unique_tokens))\n",
    "    })\n",
    "\n",
    "    # give each word an integer from 0 to n-1\n",
    "    word_to_int = dict(zip(df_apriori[\"word\"], df_apriori[\"integer\"]))\n",
    "\n",
    "    # count posts that contain each token\n",
    "    array_of_counts = np.zeros(len(unique_tokens), dtype=int)\n",
    "\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        for token in set(tokens):             \n",
    "            array_of_counts[word_to_int[token]] += 1\n",
    "\n",
    "    df_apriori[\"count\"] = array_of_counts\n",
    "\n",
    "    # save in dict\n",
    "    apriori_tables[cid] = df_apriori\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "692f4753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 58\n",
      "         word  integer  count\n",
      "41146   white    41146  27910\n",
      "8584    women     8584  14889\n",
      "7907    black     7907  13629\n",
      "25602   asian    25602  11049\n",
      "43683  people    43683  10564\n",
      "\n",
      "Community 192\n",
      "         word  integer  count\n",
      "48676   incel    48676  49933\n",
      "32458  incels    32458  44285\n",
      "39234  people    39234  13780\n",
      "7857    women     7857  13309\n",
      "5752    would     5752  10499\n",
      "\n",
      "Community 24\n",
      "        word  integer  count\n",
      "32574   chad    32574  62386\n",
      "37441  chads    37441  16060\n",
      "6295   women     6295  10281\n",
      "4580   would     4580   7614\n",
      "37351   even    37351   7053\n",
      "\n",
      "Community 214\n",
      "             word  integer  count\n",
      "27151        ugly    27151  19711\n",
      "21498       looks    21498  11849\n",
      "4972        women     4972  10190\n",
      "24939      people    24939   9785\n",
      "22871  attractive    22871   7602\n",
      "\n",
      "Community 100\n",
      "         word  integer  count\n",
      "3309   height     3309  10812\n",
      "22157   short    22157   8991\n",
      "3626    women     3626   6354\n",
      "10227    tall    10227   6254\n",
      "10383     men    10383   4896\n",
      "\n",
      "Community 210\n",
      "         word  integer  count\n",
      "4567    women     4567  18426\n",
      "13100     men    13100   9088\n",
      "27687   woman    27687   2838\n",
      "19286   think    19286   2726\n",
      "23061  people    23061   2566\n"
     ]
    }
   ],
   "source": [
    "# print top 5 tokens by count for each community\n",
    "for cid, df_apriori in apriori_tables.items():\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "    print(df_apriori.sort_values(by=\"count\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5646a40",
   "metadata": {},
   "source": [
    "## Between the passes of A-priori\n",
    "\n",
    "We create frequency tables where we assign each word an integer from 1-m, where m = number of frequent singletons (words), if the support of the word => 1%. In other words, it must appear in 1% or more of the baskets. If the word is not frequent, we assign it 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2d554ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 58 frequent items:\n",
      "             word  integer  count  freq_integer\n",
      "379          find      379   2312             1\n",
      "428        reason      428   1498             2\n",
      "859       problem      859   1122             3\n",
      "903    especially      903   1236             4\n",
      "1097        every     1097   3059             5\n",
      "...           ...      ...    ...           ...\n",
      "56031      person    56031   1488           209\n",
      "57156     someone    57156   1665           210\n",
      "57225         sex    57225   1433           211\n",
      "57640     getting    57640   2079           212\n",
      "57747       never    57747   4076           213\n",
      "\n",
      "[213 rows x 4 columns]\n",
      "\n",
      "Community 192 frequent items:\n",
      "               word  integer  count  freq_integer\n",
      "340            find      340   3617             1\n",
      "386          reason      386   2547             2\n",
      "465    relationship      465   2008             3\n",
      "680         virgins      680   1247             4\n",
      "776         problem      776   2078             5\n",
      "...             ...      ...    ...           ...\n",
      "51728      problems    51728   1269           316\n",
      "51758      thinking    51758   1232           317\n",
      "51893       getting    51893   2945           318\n",
      "51989         never    51989   5993           319\n",
      "52064        making    52064   1821           320\n",
      "\n",
      "[320 rows x 4 columns]\n",
      "\n",
      "Community 24 frequent items:\n",
      "               word  integer  count  freq_integer\n",
      "259            find      259   1522             1\n",
      "300          reason      300    871             2\n",
      "356    relationship      356    956             3\n",
      "521         femoids      521   1015             4\n",
      "774           every      774   2437             5\n",
      "...             ...      ...    ...           ...\n",
      "41968       someone    41968   1428           178\n",
      "42010           sex    42010   3229           179\n",
      "42261          beta    42261   1139           180\n",
      "42303       getting    42303   2178           181\n",
      "42391         never    42391   3455           182\n",
      "\n",
      "[182 rows x 4 columns]\n",
      "\n",
      "Community 214 frequent items:\n",
      "               word  integer  count  freq_integer\n",
      "211            find      211   3168             1\n",
      "239          reason      239   1140             2\n",
      "287    relationship      287   1599             3\n",
      "395         finding      395    500             4\n",
      "476         problem      476   1128             5\n",
      "...             ...      ...    ...           ...\n",
      "32770      thinking    32770    614           357\n",
      "32788    confidence    32788    928           358\n",
      "32852       getting    32852   1672           359\n",
      "32909         never    32909   3170           360\n",
      "32945        making    32945    668           361\n",
      "\n",
      "[361 rows x 4 columns]\n",
      "\n",
      "Community 100 frequent items:\n",
      "               word  integer  count  freq_integer\n",
      "155            find      155   1060             1\n",
      "173          reason      173    584             2\n",
      "202    relationship      202    361             3\n",
      "343         problem      343    609             4\n",
      "365      especially      365    361             5\n",
      "...             ...      ...    ...           ...\n",
      "24397           sex    24397    472           210\n",
      "24475        manlet    24475   1558           211\n",
      "24578       getting    24578    746           212\n",
      "24622         never    24622   1871           213\n",
      "24650        making    24650    363           214\n",
      "\n",
      "[214 rows x 4 columns]\n",
      "\n",
      "Community 210 frequent items:\n",
      "             word  integer  count  freq_integer\n",
      "195          find      195    509             1\n",
      "220        reason      220    700             2\n",
      "429       problem      429    523             3\n",
      "453    especially      453    314             4\n",
      "559         power      559    846             5\n",
      "...           ...      ...    ...           ...\n",
      "30433        dumb    30433    383           251\n",
      "30454    thinking    30454    352           252\n",
      "30536     getting    30536    666           253\n",
      "30594       never    30594   1463           254\n",
      "30638      making    30638    531           255\n",
      "\n",
      "[255 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "freq_tables = {}\n",
    "\n",
    "threshold_factor = 0.01  \n",
    "\n",
    "for cid, df_apriori in apriori_tables.items():\n",
    "\n",
    "    # threshold is 1% of posts in that community\n",
    "    threshold = threshold_factor * len(df_filtered[df_filtered[\"community\"] == cid])\n",
    "\n",
    "    frequent_map = np.zeros(len(df_apriori), dtype=int)\n",
    "    new_id = 1\n",
    "\n",
    "    for old_id, count in enumerate(df_apriori['count']):\n",
    "        if count >= threshold:\n",
    "            frequent_map[old_id] = new_id\n",
    "            new_id += 1\n",
    "        else:\n",
    "            frequent_map[old_id] = 0\n",
    "\n",
    "    # add freq_integer column\n",
    "    df_apriori['freq_integer'] = frequent_map\n",
    "\n",
    "    # store only frequent items in new dictionary\n",
    "    df_freq = df_apriori[df_apriori['freq_integer'] != 0].copy()\n",
    "    freq_tables[cid] = df_freq\n",
    "\n",
    "    print(f\"\\nCommunity {cid} frequent items:\")\n",
    "    print(df_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d38f5ea",
   "metadata": {},
   "source": [
    "## Second pass of the A-priori algorithm\n",
    "\n",
    "For the second pass, we first find all pairs of frequent words from the previous dataframes. We then create pairs of those, making sure to remove duplicates. We apply the support threshold of 1% here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cab3263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 58\n",
      "Top pairs:\n",
      "            item_set  count\n",
      "834   (white, women)   7297\n",
      "1562  (black, white)   5140\n",
      "840     (men, white)   4779\n",
      "831   (asian, white)   4727\n",
      "893     (men, women)   4696\n",
      "Total pairs where Support(I) => s of 0.01: 116\n",
      "\n",
      "Community 192\n",
      "Top pairs:\n",
      "              item_set  count\n",
      "33    (incels, people)   8951\n",
      "155    (incels, women)   8217\n",
      "891    (incel, incels)   8057\n",
      "599    (incel, people)   7365\n",
      "1765    (incel, women)   7263\n",
      "Total pairs where Support(I) => s of 0.01: 741\n",
      "\n",
      "Community 24\n",
      "Top pairs:\n",
      "          item_set  count\n",
      "87   (chad, women)   8106\n",
      "498  (chad, would)   6506\n",
      "56    (chad, even)   5715\n",
      "984   (chad, fuck)   4664\n",
      "778   (chad, want)   4502\n",
      "Total pairs where Support(I) => s of 0.01: 188\n",
      "\n",
      "Community 214\n",
      "Top pairs:\n",
      "            item_set  count\n",
      "689   (people, ugly)   4619\n",
      "870    (ugly, women)   4511\n",
      "872     (men, women)   3991\n",
      "1042  (looks, women)   3338\n",
      "890      (men, ugly)   3310\n",
      "Total pairs where Support(I) => s of 0.01: 1216\n",
      "\n",
      "Community 100\n",
      "Top pairs:\n",
      "            item_set  count\n",
      "139  (height, women)   2743\n",
      "210   (short, women)   2653\n",
      "75      (men, women)   2610\n",
      "723     (men, short)   2595\n",
      "213  (height, short)   2347\n",
      "Total pairs where Support(I) => s of 0.01: 422\n",
      "\n",
      "Community 210\n",
      "Top pairs:\n",
      "            item_set  count\n",
      "279     (men, women)   6089\n",
      "341   (think, women)   1945\n",
      "324  (people, women)   1829\n",
      "432    (even, women)   1717\n",
      "1     (women, would)   1642\n",
      "Total pairs where Support(I) => s of 0.01: 332\n"
     ]
    }
   ],
   "source": [
    "pair_tables = {}   # store results for each community\n",
    "\n",
    "for cid in top_6_communities:\n",
    "\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "\n",
    "    # pull posts for this community\n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "    N = len(df_comm)\n",
    "\n",
    "    # fetch frequent 1-itemset for this community\n",
    "    df_freq = freq_tables[cid]\n",
    "    frequent_words_set = set(df_freq[\"word\"])\n",
    "\n",
    "    # counter for all frequent pairs\n",
    "    pair_counter = Counter()\n",
    "\n",
    "    # iterate over all posts\n",
    "    for tokens in df_comm[\"tokens\"]:\n",
    "        # keep only frequent tokens\n",
    "        frequent_tokens = [t for t in tokens if t in frequent_words_set]\n",
    "\n",
    "        # deduplicate within a post\n",
    "        unique_tokens = set(frequent_tokens)\n",
    "\n",
    "        # count each 2-item combination in this post\n",
    "        for pair in combinations(unique_tokens, 2):\n",
    "            pair_counter[tuple(sorted(pair))] += 1\n",
    "\n",
    "    # convert counter → dataframe\n",
    "    df_pairs = pd.DataFrame(pair_counter.items(), columns=[\"item_set\", \"count\"])\n",
    "\n",
    "    # threshold for frequent 2-itemsets (1% of posts)\n",
    "    threshold = math.ceil(0.01 * N)\n",
    "    df_pairs = df_pairs[df_pairs[\"count\"] >= threshold]\n",
    "\n",
    "    # store\n",
    "    pair_tables[cid] = df_pairs\n",
    "\n",
    "    # print summary\n",
    "    print(\"Top pairs:\")\n",
    "    print(df_pairs.sort_values(by=\"count\", ascending=False).head())\n",
    "    print(\"Total pairs where Support(I) => s of 0.01:\", len(df_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b44fc2",
   "metadata": {},
   "source": [
    "# A-priori using library\n",
    "\n",
    "To further validate the above results, we also implemented the A-priori algorithm using mlxtend. We find that the results of using the mlxtend framwork are congruent with the results found by implementing the A-priori algorithm as described in Mining of Massive Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e87124af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 58\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 213\n",
      "\n",
      "Community 192\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 320\n",
      "\n",
      "Community 24\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 182\n",
      "\n",
      "Community 214\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 361\n",
      "\n",
      "Community 100\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 214\n",
      "\n",
      "Community 210\n",
      "Number of frequent items (singletons) with Support(I) => s of 0.01: 255\n"
     ]
    }
   ],
   "source": [
    "te = TransactionEncoder()\n",
    "encoded_tables = {} \n",
    "\n",
    "for cid in top_6_communities:\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "    \n",
    "    freq_words = set(freq_tables[cid][\"word\"])\n",
    "    \n",
    "    df_comm = df_filtered[df_filtered[\"community\"] == cid]\n",
    "    transactions = [\n",
    "        [t for t in tokens if t in freq_words]\n",
    "        for tokens in df_comm[\"tokens\"]\n",
    "    ]\n",
    "\n",
    "    te_array = te.fit(transactions).transform(transactions)\n",
    "    df_encoded = pd.DataFrame(te_array, columns=te.columns_)\n",
    "    \n",
    "    encoded_tables[cid] = df_encoded\n",
    "    \n",
    "    print(f\"Number of frequent items (singletons) with Support(I) => s of 0.01: {df_encoded.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ac9b3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Community 58\n",
      "Number of frequent 2-itemsets: 116\n",
      "Top 5 pairs:\n",
      "      support        itemsets\n",
      "326  0.066673  (white, women)\n",
      "241  0.046964  (white, black)\n",
      "290  0.043666    (men, white)\n",
      "224  0.043191  (white, asian)\n",
      "291  0.042907    (men, women)\n",
      "\n",
      "Community 192\n",
      "Number of frequent 2-itemsets: 741\n",
      "Top 5 pairs:\n",
      "      support          itemsets\n",
      "754  0.100983  (people, incels)\n",
      "816  0.092702   (incels, women)\n",
      "593  0.090897   (incel, incels)\n",
      "638  0.083090   (incel, people)\n",
      "701  0.081939    (incel, women)\n",
      "\n",
      "Community 24\n",
      "Number of frequent 2-itemsets: 188\n",
      "Top 5 pairs:\n",
      "      support       itemsets\n",
      "319  0.102288  (chad, women)\n",
      "322  0.082098  (would, chad)\n",
      "213  0.072116   (chad, even)\n",
      "226  0.058854   (chad, fuck)\n",
      "313  0.056810   (want, chad)\n",
      "\n",
      "Community 214\n",
      "Number of frequent 2-itemsets: 1216\n",
      "Top 5 pairs:\n",
      "       support        itemsets\n",
      "1374  0.105303  (people, ugly)\n",
      "1554  0.102841   (ugly, women)\n",
      "1267  0.090986    (men, women)\n",
      "1163  0.076099  (women, looks)\n",
      "1261  0.075461     (men, ugly)\n",
      "\n",
      "Community 100\n",
      "Number of frequent 2-itemsets: 422\n",
      "Top 5 pairs:\n",
      "      support         itemsets\n",
      "453  0.077820  (women, height)\n",
      "599  0.075267   (short, women)\n",
      "528  0.074047     (men, women)\n",
      "518  0.073621     (men, short)\n",
      "434  0.066585  (short, height)\n",
      "\n",
      "Community 210\n",
      "Number of frequent 2-itemsets: 332\n",
      "Top 5 pairs:\n",
      "      support         itemsets\n",
      "480  0.200164     (men, women)\n",
      "559  0.063938   (women, think)\n",
      "512  0.060125  (people, women)\n",
      "320  0.056443    (even, women)\n",
      "582  0.053978   (would, women)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "frequent_2_itemsets_by_community = {}\n",
    "\n",
    "for cid in top_6_communities:\n",
    "    print(f\"\\nCommunity {cid}\")\n",
    "    \n",
    "    df_encoded = encoded_tables[cid] \n",
    "    \n",
    "    frequent_itemsets = apriori(df_encoded, min_support=0.01, use_colnames=True)\n",
    "    \n",
    "    # filter to only 2-itemsets\n",
    "    frequent_2_itemsets = frequent_itemsets[\n",
    "        frequent_itemsets['itemsets'].apply(lambda x: len(x) == 2)\n",
    "    ].copy()\n",
    "    \n",
    "    frequent_2_itemsets_by_community[cid] = frequent_2_itemsets\n",
    "    \n",
    "    print(\"Number of frequent 2-itemsets:\", len(frequent_2_itemsets))\n",
    "    print(\"Top 5 pairs:\")\n",
    "    print(frequent_2_itemsets.sort_values(by=\"support\", ascending=False).head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
